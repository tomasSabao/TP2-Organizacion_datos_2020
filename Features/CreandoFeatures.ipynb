{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "#import xgboost as xgb\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import VotingClassifier, BaggingRegressor, AdaBoostRegressor\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from time import time\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Embedding,LSTM,GRU\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer=SnowballStemmer(language='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemmizar_texto(texto):\n",
    "    texto=' '.join([stemmer.stem(palabra) for palabra in texto.split() ])\n",
    "    return texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/tomas/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "english_stopwords=set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-91091f81257e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train.csv\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test.csv\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 448\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1114\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1115\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1872\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1873\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1874\u001b[0;31m                 \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1875\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1876\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train.csv'"
     ]
    }
   ],
   "source": [
    "train_set=pd.read_csv(\"train.csv\",encoding='utf-8')\n",
    "test_set=pd.read_csv(\"test.csv\",encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quitar_stopwords(texto):\n",
    "    texto=' '.join([word for word in texto.split() if word not in english_stopwords])\n",
    "    return texto\n",
    "\n",
    "def quitar_menciones(texto):\n",
    "    texto=' '.join([palabra for palabra in texto.split() if '@' not in palabra])\n",
    "    return texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "Diccionario_de_lenguaje_de_internet={\n",
    "\"2moro\": \"Tomorrow\",\n",
    "\"2nite\": \"Tonight\",\n",
    "\"4EAE\": \"For Ever And Ever\",\n",
    "\"ABT\":\"About\",\n",
    "\"ADN\": \"Any Day Now\",\n",
    "\"AFAIC\": \"As Far As I’m Concerned\",\n",
    "\"AFAICT\": \"As Far As I Can Tell\",\n",
    "\"AFAIK\": \"As Far As I Know\",\n",
    "\"AFAIR\": \"As Far As I Remember\",\n",
    "\"AKA\": \"Also Known As\",\n",
    "\"AMA\": \"Ask Me Anything\",\n",
    "\"ASAIC\": \"As Soon As I Can\",\n",
    "\"ASAP\": \"As Soon As Possible\",\n",
    "\"ATM\": \"At The Moment\",\n",
    "\"B4\": \"Before\",\n",
    "\"B4N\": \"Bye For Now\",\n",
    "\"Bae\": \"Babe/Before Anyone Else\",\n",
    "\"BBL\": \"Be Back Later\",\n",
    "\"BBT\": \"Be Back Tomorrow\",\n",
    "\"BCNU\": \"Be Seeing You\",\n",
    "\"BD\": \"Big Deal\",\n",
    "\"BF\": \"Boyfriend\",\n",
    "\"BFF\": \"Best Friends Forever\",\n",
    "\"BMT\": \"Before My Time\",\n",
    "\"BOL\": \"Be On Later\",\n",
    "\"BOT\": \"Back On Topic\",\n",
    "\"BRB\": \"Be Right Back\",\n",
    "\"BRO\": \"Brother\",\n",
    "\"BT\": \"But\",\n",
    "\"BTW\": \"By The Way\",\n",
    "\"CFY\": \"Calling For You\",\n",
    "\"CU\": \"See You\",\n",
    "\"CUL\": \"See You Later\",\n",
    "\"Cuz\": \"Because\",\n",
    "\"CYA\": \"Cover Your Ass\",\n",
    "\"DAE\": \"Does Anyone Else\",\n",
    "\"DBA\": \"Doing Business As\",\n",
    "\"DFTBA\": \"Don’t Forget To Be Awesome\",\n",
    "\"DIKU\": \"Do I Know You\",\n",
    "\"DM\": \"Direct Message\",\n",
    "\"DND\": \"Do Not Disturb\",\n",
    "\"DR\": \"Double Rainbow\",\n",
    "\"DWBH\": \"Don’t Worry, Be Happy\",\n",
    "\"ELI5\": \"Explain Like I’m 5\",\n",
    "\"EOM\": \"End Of Message\",\n",
    "\"EOS\": \"End Of Story\",\n",
    "\"F2F\": \"Face To Face\",\n",
    "\"FAQ\": \"Frequently Asked Question\",\n",
    "\"FB\": \"Facebook\",\n",
    "\"FBF\": \"Flash Back Friday\",\n",
    "\"FF\": \"Follow Friday\",\n",
    "\"FIFY\": \"Fixed It For You\",\n",
    "\"FITB\": \"Fill In The Blank\",\n",
    "\"FML\": \"Fuck My Life\",\n",
    "\"FOMO\": \"Fear Of Missing Out\",\n",
    "\"FTFY\": \"Fixed That For You\",\n",
    "\"FTL\": \"For The Loss\",\n",
    "\"FTW\": \"For The Win\",\n",
    "\"FWB\": \"Friends With Benefits\",\n",
    "\"FWIW\": \"For What It’s Worth\",\n",
    "\"FYE\": \"For Your Entertainment\",\n",
    "\"FYEO\": \"For Your Eyes Only\",\n",
    "\"FYI\": \"For Your Information\",\n",
    "\"GA\": \"Go Ahead\",\n",
    "\"GAL\": \"Get A Life\",\n",
    "\"GF\": \"Girlfriend\",\n",
    "\"GM\": \"Good Morning\",\n",
    "\"GN\": \"Good Night\",\n",
    "\"Gr8\": \"Great\",\n",
    "\"GTR\": \"Getting Ready\",\n",
    "\"HAND\": \"Have A Nice Day\",\n",
    "\"HB\": \"Hurry Back\",\n",
    "\"HBD\": \"Happy Birthday\",\n",
    "\"HBU\": \"How About You\",\n",
    "\"HMB\": \"Hit Be Back\",\n",
    "\"HMU\": \"Hit Me Up\",\n",
    "\"HRU\": \"How Are You\",\n",
    "\"HTH\": \"Hope This Helps\",\n",
    "\"IAC\": \"In Any Case\",\n",
    "\"IC\": \"I See\",\n",
    "\"ICYMI\": \"In Case You Missed It\",\n",
    "\"IDC\": \"I Don’t Care\",\n",
    "\"IDK\": \"I Don’t Know\",\n",
    "\"IG\": \"Instagram\",\n",
    "\"IIRC\": \"If I Remember Correctly\",\n",
    "\"IKR\": \"I Know Right\",\n",
    "\"ILY\": \"I Love You\",\n",
    "\"IMHO\": \"In My Humble Opinion\",\n",
    "\"IMMD\": \"It Made My Day\",\n",
    "\"IMY\": \"I Miss You\",\n",
    "\"IRL\": \"In Real Life\",\n",
    "\"IS\": \"I’m Sorry\",\n",
    "\"ISO\": \"In Search Of\",\n",
    "\"IU2U\": \"It’s Up To You\",\n",
    "\"J4F\": \"Just For Fun\",\n",
    "\"JAM\": \"Just A Minute\",\n",
    "\"JFY\": \"Just For You\",\n",
    "\"JIC\": \"Just In Case\",\n",
    "\"JK\": \"Just Kidding\",\n",
    "\"JSYK\": \"Just So You Know\",\n",
    "\"KK\": \"Okay\",\n",
    "\"L8\": \"Late\",\n",
    "\"L8R\": \"Later\",\n",
    "\"LMA\": \"Leave Me Alone\",\n",
    "\"LMAO\": \"Laughing My Ass Off\",\n",
    "\"LMBO\": \"Laughing My Butt Off\",\n",
    "\"LMK\": \"Let Me Know\",\n",
    "\"LOL\": \"Laugh Out Loud\",\n",
    "\"LTNS\": \"Long Time No See\",\n",
    "\"LYLAS\": \"Love You Like A Sister\",\n",
    "\"M/F\": \"Male or Female\",\n",
    "\"M8\": \"Mate\",\n",
    "\"MP\": \"My pleasure\",\n",
    "\"MSM\": \"Mainstream Media\",\n",
    "\"MU\": \"Miss You\",\n",
    "\"MYOB\": \"Mind Your Own Business\",\n",
    "\"NAGI\": \"Not A Good Idea\",\n",
    "\"NBD\": \"No Big Deal\",\n",
    "\"NE1\": \"Anyone\",\n",
    "\"NM\": \"Not Much\",\n",
    "\"NP\": \"No Problem\",\n",
    "\"NSFL\": \"Not Safe For Life\",\n",
    "\"NSFW\": \"Not Safe For Work\",\n",
    "\"NTS\": \"Note To Self\",\n",
    "\"NVM\": \"Never Mind\",\n",
    "\"OC\": \"Original Content\",\n",
    "\"OH\": \"Overheard\",\n",
    "\"OIC\": \"Oh ! I See\",\n",
    "\"OMD\": \"Oh My Damn\",\n",
    "\"OMG\": \"Oh My Goodness\",\n",
    "\"OMW\": \"On My Way\",\n",
    "\"OT\": \"Off Topic\",\n",
    "\"OFC\": \"Of course\",\n",
    "\"PAW\": \"Parents Are Watching\",\n",
    "\"Pls\": \"Please\",\n",
    "\"POTD\": \"Photo Of The Day\",\n",
    "\"POV\": \"Point Of View\",\n",
    "\"PPL\": \"People\",\n",
    "\"PTB\": \"Please Text Back\",\n",
    "\"Q4U\": \"Question For You\",\n",
    "\"QQ\": \"Crying\",\n",
    "\"RBTL\": \"Read Between The Lines\",\n",
    "\"RIP\": \"Rest In Peace\",\n",
    "\"RL\": \"Real Life\",\n",
    "\"ROFL\": \"Rolling On the Floor Laughing\",\n",
    "\"RT\": \"Retweet\",\n",
    "\"RTM\": \"Read The Manual\",\n",
    "\"SIS\": \"Sister\",\n",
    "\"SITD\": \"Still In The Dark\",\n",
    "\"SM\": \"Social Media\",\n",
    "\"SMH\": \"Shaking My Head\",\n",
    "\"SMY\": \"Somebody\",\n",
    "\"SNH\": \"Sarcasm Noted Here\",\n",
    "\"SOL\": \"Sooner Or Later\",\n",
    "\"Some1\": \"Someone\",\n",
    "\"SRSLY\": \"Seriously\",\n",
    "\"STBY\": \"Sucks To Be You\",\n",
    "\"Str8\": \"Straight\",\n",
    "\"SYS\": \"See You Soon\",\n",
    "\"TBA\": \"To Be Announced\",\n",
    "\"TBH\": \"To Be Honest\",\n",
    "\"TBT\": \"Throwback Thursday\",\n",
    "\"TBT\": \"Truth Be Told\",\n",
    "\"TFH\": \"Thread From Hell\",\n",
    "\"TFTI\": \"Thanks For The Invite\",\n",
    "\"TGIF\": \"Thank God It’s Friday\",\n",
    "\"THX\": \"Thanks\",\n",
    "\"TIA\": \"Thanks in Advance\",\n",
    "\"TIL\": \"Today I Learned\",\n",
    "\"TIME\": \"Tears In My Eyes\",\n",
    "\"TL;DR\": \"Too Long; Didn’t Read\",\n",
    "\"TLDR\":\"Too long didn’t read\",\n",
    "\"TL DR\":\"Too long didn’t read\",\n",
    "\"TLC\": \"Tender Loving Care\",\n",
    "\"TMI\": \"Too Much Information\",\n",
    "\"TTYL\": \"Talk To You Later\",\n",
    "\"TTYS\": \"Talk To You Soon\",\n",
    "\"Txt\": \"Text\",\n",
    "\"TYVM\": \"Thank You Very Much\",\n",
    "\"U\": \"You\",\n",
    "\"U4F\": \"You Forever\",\n",
    "\"UR\": \"Your\",\n",
    "\"VBG\": \"Very Big Grin\",\n",
    "\"VSF\": \"Very Sad Face\",\n",
    "\"WB\": \"Welcome Back\",\n",
    "\"WBU\": \"What About You?\",\n",
    "\"WEG\": \"Wicked Evil Grin\",\n",
    "\"WKND\": \"Weekend\",\n",
    "\"WOM\": \"Word of Mouth\",\n",
    "\"WOTD\": \"Word Of The Day\",\n",
    "\"Wru\": \"Who Are You\",\n",
    "\"WTH\": \"What The Heck?\",\n",
    "\"WTPA\": \"Where The Party At?\",\n",
    "\"WU?\": \"What's Up\",\n",
    "\"WU\":\"What's Up\",\n",
    "\"WYCM\": \"Will You Call Me?\",\n",
    "\"WYWH\": \"Wish You Were Here\",\n",
    "\"XOXO\": \"Hugs and Kisses\",\n",
    "\"YGM\": \"You’ve Got Mail\",\n",
    "\"YNK\": \"You Never Know\",\n",
    "\"YOLO\": \"You Only Live Once\",\n",
    "\"YT\": \"YouTube\",\n",
    "\"YW\": \"You’re Welcome\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reemplazar_lenguaje_internet(texto):\n",
    "    texto=texto.upper()\n",
    "    palabras=texto.split()\n",
    "    palabras_procesadas=[]\n",
    "    for palabra in palabras:\n",
    "        traduccion=Diccionario_de_lenguaje_de_internet.get(palabra,'not internet slang')\n",
    "        if(traduccion!='not internet slang'):\n",
    "            lista_aux=traduccion.split()\n",
    "            for x in lista_aux:\n",
    "                palabras_procesadas.append(x.lower())\n",
    "        else:\n",
    "            palabras_procesadas.append(palabra.lower())\n",
    "    texto=' '.join([word for word in palabras_procesadas])\n",
    "    return texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "diccionario_contracciones = {  \"ain't\": \"is not\",\n",
    "                               \"aint\":\"is not\",\n",
    "                               \"aren't\": \"are not\",\n",
    "                               \"arent\":\"are not\",\n",
    "                               \"can't\": \"can not\",\n",
    "                               \"cant\":\"can not\",\n",
    "                               \"cause\": \"because\",\n",
    "                               \"could've\": \"could have\",\n",
    "                               \"couldve\":\"could have\",\n",
    "                               \"couldn't\": \"could not\",\n",
    "                               \"couldnt\":\"could not\",\n",
    "                               \"didn't\": \"did not\", \n",
    "                               \"didnt\":\"did not\",\n",
    "                               \"doesn't\": \"does not\",\n",
    "                               \"doesnt\":\"does not\",\n",
    "                               \"don't\": \"do not\",\n",
    "                               \"dont\":\"do not\",\n",
    "                               \"hadn't\": \"had not\",\n",
    "                               \"hadnt\":\"had not\",\n",
    "                               \"hasn't\": \"has not\",\n",
    "                               \"hasnt\":\"has not\",\n",
    "                               \"haven't\": \"have not\",\n",
    "                               \"havent\":\"have not\",\n",
    "                               \"he'd\": \"he would\",\n",
    "                               \"hed\":\"he would\",\n",
    "                               \"he'll\": \"he will\",\n",
    "                               \"he ll\":\"he will\",\n",
    "                               \"he's\": \"he is\",\n",
    "                               \"hes\":\"he is\",\n",
    "                               \"how'd\": \"how did\",\n",
    "                               \"howd\":\"how did\",\n",
    "                               \"how'd'y\": \"how do you\",\n",
    "                               \"howdy\":\"how do you\",\n",
    "                               \"how'll\": \"how will\",\n",
    "                               \"howll\":\"how will\",\n",
    "                               \"how's\": \"how is\",\n",
    "                               \"hows\":\"how is\",\n",
    "                               \"I'd\": \"I would\",\n",
    "                               \"id\":\"i would\",\n",
    "                               \"I'd've\": \"I would have\",\n",
    "                               \"idve\":\"i would have\",\n",
    "                               \"I'll\": \"I will\", \n",
    "                               \"Ill\":\"i will\", #duda: si algun tweet habla de un enfermo puede traer ruido\n",
    "                               \"I'll've\": \"I will have\",\n",
    "                               \"I'm\": \"I am\", \n",
    "                               \"im\":\"i am\",\n",
    "                               \"I've\": \"I have\",\n",
    "                               \"ive\":\"i have\",\n",
    "                               \"i'd\": \"i would\",\n",
    "                               \"i'd've\": \"i would have\",\n",
    "                               \"i'll\": \"i will\",\n",
    "                               \"i'll've\": \"i will have\",\n",
    "                               \"i'm\": \"i am\", \n",
    "                               \"im\":\"i am\",\n",
    "                               \"i've\": \"i have\",\n",
    "                               \"isn't\": \"is not\",\n",
    "                               \"isnt\":\"is not\",\n",
    "                               \"it'd\": \"it would\",\n",
    "                               \"itd\":\"it would\",\n",
    "                               \"it'd've\": \"it would have\",\n",
    "                               \"it'll\": \"it will\",\n",
    "                               \"itll\":\"it will\",\n",
    "                               \"it'll've\": \"it will have\",\n",
    "                               \"it's\": \"it is\",\n",
    "                               \"its\":\"it is\",\n",
    "                               \"let's\": \"let us\",\n",
    "                               \"lets\":\"let us\",\n",
    "                               \"ma'am\": \"madam\",\n",
    "                               \"maam\":\"madam\",\n",
    "                               \"mayn't\": \"may not\",\n",
    "                               \"maynt\":\"may not\",\n",
    "                               \"might've\": \"might have\",\n",
    "                               \"mightve\":\"might have\",\n",
    "                               \"mightn't\": \"might not\",\n",
    "                               \"mightnt\":\"might not\",\n",
    "                               \"mightn't've\": \"might not have\",\n",
    "                               \"must've\": \"must have\",\n",
    "                               \"mustve\":\"must have\",\n",
    "                               \"mustn't\": \"must not\",\n",
    "                               \"mustnt\":\"must not\",\n",
    "                               \"mustn't've\": \"must not have\",\n",
    "                               \"needn't\": \"need not\",\n",
    "                               \"neednt\":\"need not\",\n",
    "                               \"needn't've\": \"need not have\",\n",
    "                               \"o'clock\": \"of the clock\",\n",
    "                               \"oclock\":\"of the clock\",\n",
    "                               \"oughtn't\": \"ought not\",\n",
    "                               \"oughtnt\":\"ought not\",\n",
    "                               \"oughtn't've\": \"ought not have\",\n",
    "                               \"shan't\": \"shall not\",\n",
    "                               \"shant\":\"shall not\",\n",
    "                               \"sha'n't\": \"shall not\",\n",
    "                               \"shan't've\": \"shall not have\",\n",
    "                               \"she'd\": \"she would\",\n",
    "                               \"shed\":\"she would\",\n",
    "                               \"she'd've\": \"she would have\",\n",
    "                               \"she'll\": \"she will\",\n",
    "                               \"shell\":\"she will\",#nuevamente aca tengo ruido por dos palabras igual escritas\n",
    "                               \"she'll've\": \"she will have\", \n",
    "                               \"she's\": \"she is\",\n",
    "                               \"shes\":\"she is\",\n",
    "                               \"should've\": \"should have\",\n",
    "                               \"shouldve\":\"should have\",\n",
    "                               \"shouldn't\": \"should not\",\n",
    "                               \"shouldnt\": \"should not\",\n",
    "                               \"shouldn't've\": \"should not have\",\n",
    "                               \"so've\": \"so have\",\n",
    "                               \"so's\": \"so as\",\n",
    "                               \"this's\": \"this is\",\n",
    "                               \"that'd\": \"that would\",\n",
    "                               \"that'd've\": \"that would have\",\n",
    "                               \"that's\": \"that is\",\n",
    "                               \"thats\":\"that is\",\n",
    "                               \"there'd\": \"there would\",\n",
    "                               \"thered\":\"there would\",\n",
    "                               \"there'd've\": \"there would have\",\n",
    "                               \"there's\": \"there is\",\n",
    "                               \"theres\":\"there is\",\n",
    "                               \"here's\": \"here is\",\n",
    "                               \"heres\":\"here is\",\n",
    "                               \"they'd\": \"they would\",\n",
    "                               \"theyd\":\"they would\",\n",
    "                               \"they'd've\": \"they would have\",\n",
    "                               \"they'll\": \"they will\",\n",
    "                               \"theyll\":\"they will\",\n",
    "                               \"they'll've\": \"they will have\", \n",
    "                               \"they're\": \"they are\",\n",
    "                               \"theyre\":\"they are\",\n",
    "                               \"they've\": \"they have\", \n",
    "                               \"theyve\":\"they have\",\n",
    "                               \"to've\": \"to have\", \n",
    "                               \"tove\":\"to have\",\n",
    "                               \"wasn't\": \"was not\",\n",
    "                               \"wasnt\":\"was not\",\n",
    "                               \"we'd\": \"we would\",\n",
    "                               \"wed\":\"we would\",#aca nuevamente hay conflicto\n",
    "                               \"we'd've\": \"we would have\",\n",
    "                               \"we'll\": \"we will\",\n",
    "                               \"we'll've\": \"we will have\",\n",
    "                               \"we're\": \"we are\",\n",
    "                               \"we've\": \"we have\",\n",
    "                               \"weren't\": \"were not\",\n",
    "                               \"werent\":\"were not\",\n",
    "                               \"what'll\": \"what will\",\n",
    "                               \"whatll\":\"what will\",\n",
    "                               \"what'll've\": \"what will have\",\n",
    "                               \"what're\": \"what are\", \n",
    "                               \"whatre\":\"what are\",\n",
    "                               \"what's\": \"what is\",\n",
    "                               \"whats\":\"what is\",\n",
    "                               \"what've\": \"what have\",\n",
    "                               \"whatve\":\"what have\",\n",
    "                               \"when's\": \"when is\",\n",
    "                               \"whens\":\"when is\",\n",
    "                               \"when've\": \"when have\",\n",
    "                               \"whenve\":\"when have\",\n",
    "                               \"where'd\": \"where did\",\n",
    "                               \"whered\":\"where did\",\n",
    "                               \"where's\": \"where is\",\n",
    "                               \"wheres\":\"where is\",\n",
    "                               \"where've\": \"where have\",\n",
    "                               \"whereve\":\"where have\",\n",
    "                               \"who'll\": \"who will\", \n",
    "                               \"wholl\":\"who will\",\n",
    "                               \"who'll've\": \"who will have\",\n",
    "                               \"who's\": \"who is\",\n",
    "                               \"whos\":\"who is\",\n",
    "                               \"who've\": \"who have\",\n",
    "                               \"whove\":\"who have\",\n",
    "                               \"why's\": \"why is\",\n",
    "                               \"whys\":\"why is\",\n",
    "                               \"why've\": \"why have\",\n",
    "                               \"whyve\":\"why have\",\n",
    "                               \"will've\": \"will have\",\n",
    "                               \"willve\":\"will have\",\n",
    "                               \"won't\": \"will not\",\n",
    "                               \"wont\":\"will not\",\n",
    "                               \"won't've\": \"will not have\",\n",
    "                               \"would've\": \"would have\",\n",
    "                               \"wouldve\":\"would have\",\n",
    "                               \"wouldn't\": \"would not\",\n",
    "                               \"wouldnt\":\"would not\",\n",
    "                               \"wouldn't've\": \"would not have\",\n",
    "                               \"y'all\": \"you all\",\n",
    "                               \"yall\":\"you all\",\n",
    "                               \"y'all'd\": \"you all would\",\n",
    "                               \"yalld\":\"you all would\",\n",
    "                               \"y'all'd've\": \"you all would have\",\n",
    "                               \"y'all're\": \"you all are\",\n",
    "                               \"yallre\":\"you all are\",\n",
    "                               \"y'all've\": \"you all have\",\n",
    "                               \"you'd\": \"you would\",\n",
    "                               \"youd\":\"you would\",\n",
    "                               \"you'd've\": \"you would have\",\n",
    "                               \"you'll\": \"you will\",\n",
    "                               \"youll\":\"you will\",\n",
    "                               \"you'll've\": \"you will have\",\n",
    "                               \"you're\": \"you are\",\n",
    "                               \"youre\":\"you are\",\n",
    "                               \"you've\": \"you have\",\n",
    "                               \"youve\":\"you have\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reemplazar_contracciones(texto):\n",
    "    texto=texto.lower()\n",
    "    palabras=texto.split()\n",
    "    palabras_procesadas=[]\n",
    "    for palabra in palabras:\n",
    "        traduccion=diccionario_contracciones.get(palabra,'no es contraccion')\n",
    "        if(traduccion!='no es contraccion'):\n",
    "            lista_aux=traduccion.split()\n",
    "            for x in lista_aux:\n",
    "                palabras_procesadas.append(x.lower())\n",
    "        else:\n",
    "            palabras_procesadas.append(palabra.lower())\n",
    "    texto=' '.join([word for word in palabras_procesadas])\n",
    "    return texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eliminar_palabras_de_una_sola_letra(texto):\n",
    "    texto=texto.lower()\n",
    "    lista_palabras=texto.split()\n",
    "    palabras_procesadas=[]\n",
    "    for palabra in lista_palabras:\n",
    "        if(len(palabra)!=1):\n",
    "            palabras_procesadas.append(palabra)\n",
    "    texto=\" \".join([word for word in palabras_procesadas])\n",
    "    return texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#es necesario importar la libreria re (regular expression) para poder usar esta funcion\n",
    "#ya esta bajada en el comienzo del notebook pero si lo que quieren es analizar la documentación\n",
    "#lo pueden revisar en google\n",
    "#falta borrar los arroba\n",
    "def limpiar_texto(texto):\n",
    "    texto = reemplazar_lenguaje_internet(texto)\n",
    "    texto=reemplazar_contracciones(texto)\n",
    "    texto=texto.lower()\n",
    "    texto = re.sub('\\[.*?\\]', '', texto) # remove text in square brackets\n",
    "    texto = re.sub('https?://\\S+|www\\.\\S+', '', texto) # remove URLs\n",
    "    texto = re.sub('<.*?>+', '', texto) # remove html tags\n",
    "    texto = re.sub('[%s]' % re.escape(string.punctuation), '', texto) # remove punctuation\n",
    "    texto = re.sub('\\n', '', texto) # remove words conatinaing numbers\n",
    "    texto = re.sub('\\w*\\d\\w*', '', texto)\n",
    "    texto = re.sub('[‘’“”…]', '', texto)\n",
    "    texto = quitar_stopwords(texto)\n",
    "    texto = quitar_menciones(texto)\n",
    "    texto = eliminar_palabras_de_una_sola_letra(texto)\n",
    "    return texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizar_texto(texto):\n",
    "    lista_palabras=nltk.word_tokenize(texto)\n",
    "    texto=' '.join([lemmatizer.lemmatize(word) for word in lista_palabras])\n",
    "    return texto\n",
    "#nota: la función esta definida aca, pero para ver los resultados que produce hay\n",
    "#que ir al comienzo del notebook y ejecutar la celda de lemmatizar texto luego\n",
    "#de haber limpiado el texto original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(palabra):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([palabra])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizar_con_pos(texto):\n",
    "    lista_palabras=texto.split()\n",
    "    texto=' '.join(lemmatizer.lemmatize(palabra,get_wordnet_pos(palabra)) for palabra in lista_palabras)\n",
    "    return texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>10869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Two giant cranes holding a bridge collapse int...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>10870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@aria_ahrary @TheTawniest The out of control w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>10871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>10872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Police investigating after an e-bike collided ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>10873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         1     NaN      NaN   \n",
       "1         4     NaN      NaN   \n",
       "2         5     NaN      NaN   \n",
       "3         6     NaN      NaN   \n",
       "4         7     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "7608  10869     NaN      NaN   \n",
       "7609  10870     NaN      NaN   \n",
       "7610  10871     NaN      NaN   \n",
       "7611  10872     NaN      NaN   \n",
       "7612  10873     NaN      NaN   \n",
       "\n",
       "                                                   text  target  \n",
       "0     Our Deeds are the Reason of this #earthquake M...       1  \n",
       "1                Forest fire near La Ronge Sask. Canada       1  \n",
       "2     All residents asked to 'shelter in place' are ...       1  \n",
       "3     13,000 people receive #wildfires evacuation or...       1  \n",
       "4     Just got sent this photo from Ruby #Alaska as ...       1  \n",
       "...                                                 ...     ...  \n",
       "7608  Two giant cranes holding a bridge collapse int...       1  \n",
       "7609  @aria_ahrary @TheTawniest The out of control w...       1  \n",
       "7610  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...       1  \n",
       "7611  Police investigating after an e-bike collided ...       1  \n",
       "7612  The Latest: More Homes Razed by Northern Calif...       1  \n",
       "\n",
       "[7613 rows x 5 columns]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature: Cantidad de caracteres usados en tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GyL\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "C:\\Users\\GyL\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "cuenta_caract = train_set[['id','text']]\n",
    "cuenta_caract['caracteres_usados'] = cuenta_caract['text'].str.len()\n",
    "cuenta_caract_t = test_set[['id','text']]\n",
    "cuenta_caract_t['caracteres_usados'] = cuenta_caract_t['text'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuenta_caract = cuenta_caract[['id','caracteres_usados']]\n",
    "cuenta_caract_t = cuenta_caract_t[['id','caracteres_usados']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set=train_set.merge(cuenta_caract,on='id',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set=test_set.merge(cuenta_caract_t,on='id',how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature: cantidad de menciones por tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GyL\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "C:\\Users\\GyL\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "cant_menciones = train_set[['id','text']]\n",
    "cant_menciones['menciones_realizadas']=cant_menciones['text'].str.count('@')\n",
    "cant_menciones_t = test_set[['id','text']]\n",
    "cant_menciones_t['menciones_realizadas']=cant_menciones_t['text'].str.count('@')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "cant_menciones = cant_menciones[['id','menciones_realizadas']]\n",
    "cant_menciones_t=cant_menciones_t[['id','menciones_realizadas']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set=train_set.merge(cant_menciones,on='id',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set=test_set.merge(cant_menciones_t,on='id',how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature: ID que comparten localización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiene_localizacion = train_set[['id','location']]\n",
    "tiene_localizacion_t = test_set[['id','location']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GyL\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n",
      "C:\\Users\\GyL\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\Users\\GyL\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "tiene_localizacion['location'].loc[~(tiene_localizacion['location'].isnull())]=1\n",
    "tiene_localizacion_t['location'].loc[~(tiene_localizacion_t['location'].isnull())]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GyL\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:4153: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  downcast=downcast,\n"
     ]
    }
   ],
   "source": [
    "tiene_localizacion.fillna(0,inplace=True)\n",
    "tiene_localizacion_t.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    5080\n",
       "0    2533\n",
       "Name: location, dtype: int64"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tiene_localizacion['location'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiene_localizacion = tiene_localizacion.rename(columns={'location':'permite_location'})\n",
    "tiene_localizacion_t = tiene_localizacion_t.rename(columns={'location':'permite_location'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set=train_set.merge(tiene_localizacion,on='id',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set=test_set.merge(tiene_localizacion_t,on='id',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature: usa Keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GyL\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "C:\\Users\\GyL\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "usa_keyword = train_set[['id','keyword']]\n",
    "usa_keyword['keyword'].loc[~(usa_keyword['keyword'].isnull())] = 1\n",
    "usa_keyword_t = test_set[['id','keyword']]\n",
    "usa_keyword_t['keyword'].loc[~(usa_keyword_t['keyword'].isnull())] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "usa_keyword.fillna(0,inplace=True)\n",
    "usa_keyword_t.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "usa_keyword = usa_keyword.rename(columns={'keyword':'use_keyword'})\n",
    "usa_keyword_t = usa_keyword_t.rename(columns={'keyword':'use_keyword'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = train_set.merge(usa_keyword,on='id',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set=test_set.merge(usa_keyword_t,on='id',how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature: Cita URL en Tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "usa_url = train_set[['id','text']]\n",
    "usa_url_t = test_set[['id','text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GyL\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\Users\\GyL\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "usa_url['cita_url'] = usa_url['text'].str.count('http')\n",
    "usa_url_t['cita_url'] = usa_url_t['text'].str.count('http')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "usa_url = usa_url[['id','cita_url']]\n",
    "usa_url_t = usa_url_t[['id','cita_url']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = train_set.merge(usa_url,on='id',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set=test_set.merge(usa_url_t,on='id',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature: usa Hashtag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GyL\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "C:\\Users\\GyL\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "usa_hashtag = train_set[['id','text']]\n",
    "usa_hashtag['use_hashtag']=usa_hashtag['text'].str.count('#')\n",
    "usa_hashtag_t = test_set[['id','text']]\n",
    "usa_hashtag_t['use_hashtag']=usa_hashtag_t['text'].str.count('#')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "usa_hashtag = usa_hashtag[['id','use_hashtag']]\n",
    "usa_hashtag_t = usa_hashtag_t[['id','use_hashtag']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 683,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = train_set.merge(usa_hashtag,on='id',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 684,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set=test_set.merge(usa_hashtag_t,on='id',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature: Cantidad de palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 685,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-685-9a582d2d4513>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cant_palabras['cant_palabras'] = cant_palabras['text'].str.count(' ') + 1\n",
      "<ipython-input-685-9a582d2d4513>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cant_palabras_t['cant_palabras'] = cant_palabras_t['text'].str.count(' ') + 1\n"
     ]
    }
   ],
   "source": [
    "cant_palabras = train_set[['id', 'text']]\n",
    "cant_palabras['cant_palabras'] = cant_palabras['text'].str.count(' ') + 1\n",
    "cant_palabras_t = test_set[['id', 'text']]\n",
    "cant_palabras_t['cant_palabras'] = cant_palabras_t['text'].str.count(' ') + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 686,
   "metadata": {},
   "outputs": [],
   "source": [
    "cant_palabras = cant_palabras[['id', 'cant_palabras']]\n",
    "cant_palabras_t = cant_palabras_t[['id', 'cant_palabras']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 687,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = train_set.merge(cant_palabras,on='id',how='left')\n",
    "test_set = test_set.merge(cant_palabras_t,on='id',how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature: Cantidad de abreviaciones de internet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 688,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contar_abreviaciones(data):\n",
    "    lista_cant = []\n",
    "    for tweet in data:\n",
    "        cant = 0\n",
    "        tweet = quitar_stopwords(tweet)\n",
    "        tweet = tweet.upper()\n",
    "        tweet = tweet.split()\n",
    "        for word in tweet:\n",
    "            if (word in Diccionario_de_lenguaje_de_internet):\n",
    "                cant = cant + 1\n",
    "        lista_cant = lista_cant + [cant]\n",
    "    return lista_cant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 689,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-689-1cd6de0a6bdd>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cant_abreviaciones['cant_abreviaciones'] = contar_abreviaciones(cant_abreviaciones['text'])\n",
      "<ipython-input-689-1cd6de0a6bdd>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cant_abreviaciones_t['cant_abreviaciones'] = contar_abreviaciones(cant_abreviaciones_t['text'])\n"
     ]
    }
   ],
   "source": [
    "cant_abreviaciones = train_set[['id', 'text']]\n",
    "cant_abreviaciones['cant_abreviaciones'] = contar_abreviaciones(cant_abreviaciones['text'])\n",
    "cant_abreviaciones_t = test_set[['id', 'text']]\n",
    "cant_abreviaciones_t['cant_abreviaciones'] = contar_abreviaciones(cant_abreviaciones_t['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 690,
   "metadata": {},
   "outputs": [],
   "source": [
    "cant_abreviaciones = cant_abreviaciones[['id', 'cant_abreviaciones']]\n",
    "cant_abreviaciones_t = cant_abreviaciones_t[['id', 'cant_abreviaciones']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 691,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = train_set.merge(cant_abreviaciones,on='id',how='left')\n",
    "test_set = test_set.merge(cant_abreviaciones_t,on='id',how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature: Location en Estados Unidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 692,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_estados_usa = [\"ALABAMA\", \"AL\", \"ALASKA\", \"AK\", \n",
    "                     \"ARIZONA\", \"AZ\", \"ARKANSAS\", \"AR\", \n",
    "                     \"CALIFORNIA\", \"CA\", \"COLORADO\", \"CO\", \n",
    "                     \"CONNECTICUT\", \"CT\", \"DELAWARE\", \"DE\", \n",
    "                     \"FLORIDA\", \"FL\", \"GEORGIA\", \"GA\", \n",
    "                     \"HAWAII\", \"HI\", \"IDAHO\", \"ID\", \n",
    "                     \"ILLINOIS\", \"IL\", \"INDIANA\", \"IN\", \n",
    "                     \"IOWA\", \"IA\", \"KANSAS\", \"KS\", \n",
    "                     \"KENTUCKY\", \"KY\", \"LOUISIANA\", \"LA\", \n",
    "                     \"MAINE\", \"ME\", \"MARYLAND\", \"MD\", \n",
    "                     \"MASSACHUSETTS\", \"MA\", \"MICHIGAN\", \"MI\", \n",
    "                     \"MINNESOTA\", \"MN\", \"MISSISSIPPI\", \"MS\", \n",
    "                     \"MISSOURI\", \"MO\", \"MONTANA\", \"MT\", \n",
    "                     \"NEBRASKA\", \"NE\", \"NEVADA\", \"NV\", \n",
    "                     \"NEW HAMPSHIRE\", \"NH\", \"NEW JERSEY\", \n",
    "                     \"NJ\", \"NEW MEXICO\", \"NM\", \"NEW YORK\", \n",
    "                     \"NY\", \"NORTH CAROLINA\", \"NC\", \"NORTH DAKOTA\", \n",
    "                     \"ND\", \"OHIO\", \"OH\", \"OKLAHOMA\", \"OK\", \n",
    "                     \"OREGON\", \"OR\", \"PENNSYLVANIA\", \"PA\", \n",
    "                     \"RHODE ISLAND\", \"RI\", \"SOUTH CAROLINA\", \"SC\", \"CAROLINA\", \n",
    "                     \"SOUTH DAKOTA\", \"SD\", \"TENNESSEE\", \"TN\", \n",
    "                     \"TEXAS\", \"TX\", \"UTAH\", \"UT\", \n",
    "                     \"VERMONT\", \"VT\", \"VIRGINIA\", \"VA\", \n",
    "                     \"WASHINGTON\", \"WA\", \"WEST VIRGINIA\", \"WV\", \n",
    "                     \"WISCONSIN\", \"WI\", \"WYOMING\", \"WY\",\n",
    "                     \"USA\", \"UNITED STATES\", \"SAN FRANCISCO\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 693,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ver_location_usa(data):\n",
    "    lista_ubicacion_usa = []\n",
    "    for location in data:\n",
    "        if (location is None):\n",
    "            lista_ubicacion_usa += [0]\n",
    "        else:\n",
    "            location_usa = False\n",
    "            location = str(location)\n",
    "            location = location.upper()\n",
    "            if (location in lista_estados_usa):\n",
    "                location_usa = True\n",
    "            location = location.split()\n",
    "            for word in location:\n",
    "                if (word in lista_estados_usa):\n",
    "                    location_usa = True\n",
    "                    break\n",
    "            if (location_usa):\n",
    "                lista_ubicacion_usa += [1]\n",
    "            else:\n",
    "                lista_ubicacion_usa += [0]\n",
    "    return lista_ubicacion_usa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 694,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-694-aa4731089d51>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  location_usa['location_usa'] = ver_location_usa(location_usa['location'])\n",
      "<ipython-input-694-aa4731089d51>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  location_usa_t['location_usa'] = ver_location_usa(location_usa_t['location'])\n"
     ]
    }
   ],
   "source": [
    "location_usa = train_set[['id', 'location']]\n",
    "location_usa['location_usa'] = ver_location_usa(location_usa['location'])\n",
    "location_usa_t = test_set[['id', 'location']]\n",
    "location_usa_t['location_usa'] = ver_location_usa(location_usa_t['location'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 695,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_usa = location_usa[['id', 'location_usa']]\n",
    "location_usa_t = location_usa_t[['id', 'location_usa']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = train_set.merge(location_usa,on='id',how='left')\n",
    "test_set = test_set.merge(location_usa_t,on='id',how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature: Tweet tiene caritas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 697,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_caritas = [\":)\", \";)\", \":(\", \";(\", \"XD\", \"xD\", \"xd\", \":P\", \":p\", \"-_-\", \":O\", \":'(\", \":D\", \":-D\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 698,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buscar_caritas(data):\n",
    "    tiene_carita = []\n",
    "    for tweet in data:\n",
    "        tweet = quitar_menciones(tweet)\n",
    "        tweet = quitar_links(tweet)\n",
    "        hay_carita = False\n",
    "        for carita in lista_caritas:\n",
    "            if (carita in tweet):\n",
    "                hay_carita = True\n",
    "                break\n",
    "        if (hay_carita):\n",
    "            tiene_carita += [1]\n",
    "        else:\n",
    "            tiene_carita += [0]\n",
    "    return tiene_carita"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 699,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-699-31913808ce48>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  has_emoji['has_emoji'] = buscar_caritas(has_emoji['text'])\n",
      "<ipython-input-699-31913808ce48>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  has_emoji_t['has_emoji'] = buscar_caritas(has_emoji_t['text'])\n"
     ]
    }
   ],
   "source": [
    "has_emoji = train_set[['id', 'text']]\n",
    "has_emoji['has_emoji'] = buscar_caritas(has_emoji['text'])\n",
    "has_emoji_t = test_set[['id', 'text']]\n",
    "has_emoji_t['has_emoji'] = buscar_caritas(has_emoji_t['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 700,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_emoji = has_emoji[['id', 'has_emoji']]\n",
    "has_emoji_t = has_emoji_t[['id', 'has_emoji']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 701,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = train_set.merge(has_emoji,on='id',how='left')\n",
    "test_set = test_set.merge(has_emoji_t,on='id',how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature: Palabras con misma letra repetida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_letras_seguidas = [\"aaa\", \"bbb\", \"ccc\", \"ddd\", \"eee\", \"fff\", \"ggg\", \"hhh\", \"iii\", \"jjj\", \"kkk\", \"lll\", \"mmm\", \"nnn\", \"ooo\", \"ppp\", \"qqq\", \"rrr\", \"sss\", \"ttt\", \"uuu\", \"vvv\", \"www\", \"xxx\", \"yyy\", \"zzz\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 703,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buscar_letras_seguidas(data):\n",
    "    tiene_letras_seguidas = []\n",
    "    for tweet in data:\n",
    "        tweet = quitar_menciones(tweet)\n",
    "        tweet = quitar_links(tweet)\n",
    "        hay_letra_seguida = False\n",
    "        for letra in lista_letras_seguidas:\n",
    "            if (letra in tweet):\n",
    "                hay_letra_seguida = True\n",
    "                break\n",
    "        if (hay_letra_seguida):\n",
    "            tiene_letras_seguidas += [1]\n",
    "        else:\n",
    "            tiene_letras_seguidas += [0]\n",
    "    return tiene_letras_seguidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-704-8c809b22b19a>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  has_repeated_letter['has_repeated_letter'] = buscar_letras_seguidas(has_repeated_letter['text'])\n",
      "<ipython-input-704-8c809b22b19a>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  has_repeated_letter_t['has_repeated_letter'] = buscar_letras_seguidas(has_repeated_letter_t['text'])\n"
     ]
    }
   ],
   "source": [
    "has_repeated_letter = train_set[['id', 'text']]\n",
    "has_repeated_letter['has_repeated_letter'] = buscar_letras_seguidas(has_repeated_letter['text'])\n",
    "has_repeated_letter_t = test_set[['id', 'text']]\n",
    "has_repeated_letter_t['has_repeated_letter'] = buscar_letras_seguidas(has_repeated_letter_t['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 705,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_repeated_letter = has_repeated_letter[['id', 'has_repeated_letter']]\n",
    "has_repeated_letter_t = has_repeated_letter_t[['id', 'has_repeated_letter']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 706,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = train_set.merge(has_repeated_letter,on='id',how='left')\n",
    "test_set = test_set.merge(has_repeated_letter_t,on='id',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 707,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.to_csv('train_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 708,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set.to_csv('test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 709,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>caracteres_usados</th>\n",
       "      <th>menciones_realizadas</th>\n",
       "      <th>permite_location</th>\n",
       "      <th>use_keyword</th>\n",
       "      <th>cita_url</th>\n",
       "      <th>use_hashtag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>69</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>133</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>65</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>88</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>10869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Two giant cranes holding a bridge collapse int...</td>\n",
       "      <td>1</td>\n",
       "      <td>83</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>10870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@aria_ahrary @TheTawniest The out of control w...</td>\n",
       "      <td>1</td>\n",
       "      <td>125</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>10871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n",
       "      <td>1</td>\n",
       "      <td>65</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>10872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Police investigating after an e-bike collided ...</td>\n",
       "      <td>1</td>\n",
       "      <td>137</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>10873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
       "      <td>1</td>\n",
       "      <td>94</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         1     NaN      NaN   \n",
       "1         4     NaN      NaN   \n",
       "2         5     NaN      NaN   \n",
       "3         6     NaN      NaN   \n",
       "4         7     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "7608  10869     NaN      NaN   \n",
       "7609  10870     NaN      NaN   \n",
       "7610  10871     NaN      NaN   \n",
       "7611  10872     NaN      NaN   \n",
       "7612  10873     NaN      NaN   \n",
       "\n",
       "                                                   text  target  \\\n",
       "0     Our Deeds are the Reason of this #earthquake M...       1   \n",
       "1                Forest fire near La Ronge Sask. Canada       1   \n",
       "2     All residents asked to 'shelter in place' are ...       1   \n",
       "3     13,000 people receive #wildfires evacuation or...       1   \n",
       "4     Just got sent this photo from Ruby #Alaska as ...       1   \n",
       "...                                                 ...     ...   \n",
       "7608  Two giant cranes holding a bridge collapse int...       1   \n",
       "7609  @aria_ahrary @TheTawniest The out of control w...       1   \n",
       "7610  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...       1   \n",
       "7611  Police investigating after an e-bike collided ...       1   \n",
       "7612  The Latest: More Homes Razed by Northern Calif...       1   \n",
       "\n",
       "      caracteres_usados  menciones_realizadas  permite_location  use_keyword  \\\n",
       "0                    69                     0                 0            0   \n",
       "1                    38                     0                 0            0   \n",
       "2                   133                     0                 0            0   \n",
       "3                    65                     0                 0            0   \n",
       "4                    88                     0                 0            0   \n",
       "...                 ...                   ...               ...          ...   \n",
       "7608                 83                     0                 0            0   \n",
       "7609                125                     2                 0            0   \n",
       "7610                 65                     0                 0            0   \n",
       "7611                137                     0                 0            0   \n",
       "7612                 94                     0                 0            0   \n",
       "\n",
       "      cita_url  use_hashtag  \n",
       "0            0            1  \n",
       "1            0            0  \n",
       "2            0            0  \n",
       "3            0            1  \n",
       "4            0            2  \n",
       "...        ...          ...  \n",
       "7608         1            0  \n",
       "7609         0            0  \n",
       "7610         1            0  \n",
       "7611         0            0  \n",
       "7612         1            0  \n",
       "\n",
       "[7613 rows x 11 columns]"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>caracteres_usados</th>\n",
       "      <th>menciones_realizadas</th>\n",
       "      <th>permite_location</th>\n",
       "      <th>use_keyword</th>\n",
       "      <th>cita_url</th>\n",
       "      <th>use_hashtag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3258</th>\n",
       "      <td>10861</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>EARTHQUAKE SAFETY LOS ANGELES ÛÒ SAFETY FASTE...</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3259</th>\n",
       "      <td>10865</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Storm in RI worse than last hurricane. My city...</td>\n",
       "      <td>139</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3260</th>\n",
       "      <td>10868</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Green Line derailment in Chicago http://t.co/U...</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3261</th>\n",
       "      <td>10874</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MEG issues Hazardous Weather Outlook (HWO) htt...</td>\n",
       "      <td>65</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3262</th>\n",
       "      <td>10875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#CityofCalgary has activated its Municipal Eme...</td>\n",
       "      <td>68</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3263 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         0     NaN      NaN   \n",
       "1         2     NaN      NaN   \n",
       "2         3     NaN      NaN   \n",
       "3         9     NaN      NaN   \n",
       "4        11     NaN      NaN   \n",
       "...     ...     ...      ...   \n",
       "3258  10861     NaN      NaN   \n",
       "3259  10865     NaN      NaN   \n",
       "3260  10868     NaN      NaN   \n",
       "3261  10874     NaN      NaN   \n",
       "3262  10875     NaN      NaN   \n",
       "\n",
       "                                                   text  caracteres_usados  \\\n",
       "0                    Just happened a terrible car crash                 34   \n",
       "1     Heard about #earthquake is different cities, s...                 64   \n",
       "2     there is a forest fire at spot pond, geese are...                 96   \n",
       "3              Apocalypse lighting. #Spokane #wildfires                 40   \n",
       "4         Typhoon Soudelor kills 28 in China and Taiwan                 45   \n",
       "...                                                 ...                ...   \n",
       "3258  EARTHQUAKE SAFETY LOS ANGELES ÛÒ SAFETY FASTE...                 55   \n",
       "3259  Storm in RI worse than last hurricane. My city...                139   \n",
       "3260  Green Line derailment in Chicago http://t.co/U...                 55   \n",
       "3261  MEG issues Hazardous Weather Outlook (HWO) htt...                 65   \n",
       "3262  #CityofCalgary has activated its Municipal Eme...                 68   \n",
       "\n",
       "      menciones_realizadas  permite_location  use_keyword  cita_url  \\\n",
       "0                        0                 0            0         0   \n",
       "1                        0                 0            0         0   \n",
       "2                        0                 0            0         0   \n",
       "3                        0                 0            0         0   \n",
       "4                        0                 0            0         0   \n",
       "...                    ...               ...          ...       ...   \n",
       "3258                     0                 0            0         0   \n",
       "3259                     0                 0            0         0   \n",
       "3260                     0                 0            0         1   \n",
       "3261                     0                 0            0         1   \n",
       "3262                     0                 0            0         0   \n",
       "\n",
       "      use_hashtag  \n",
       "0               0  \n",
       "1               1  \n",
       "2               0  \n",
       "3               2  \n",
       "4               0  \n",
       "...           ...  \n",
       "3258            0  \n",
       "3259            0  \n",
       "3260            0  \n",
       "3261            0  \n",
       "3262            2  \n",
       "\n",
       "[3263 rows x 10 columns]"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature: cantidad de palabras únicas usadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set=pd.read_csv(\"train_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_cantidad_de_palabras_unicas=[]\n",
    "def contar_cantidad_de_palabras_unicas(texto):\n",
    "    diccionario_auxiliar={}\n",
    "    palabras=texto.split()\n",
    "    for palabra in palabras:\n",
    "        esta=diccionario_auxiliar.get(palabra,\"no esta\")\n",
    "        if(esta==\"no esta\"):\n",
    "            diccionario_auxiliar[palabra]=\"esta\"\n",
    "    lista_cantidad_de_palabras_unicas.append(len(diccionario_auxiliar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set['numero_de_palabras_unicas']=train_set['text'].apply"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Feature: Hace mencion a alguna ciudad o país"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ciudades_del_mundo=pd.read_csv('world-cities_csv.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>country</th>\n",
       "      <th>subcountry</th>\n",
       "      <th>geonameid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>les Escaldes</td>\n",
       "      <td>Andorra</td>\n",
       "      <td>Escaldes-Engordany</td>\n",
       "      <td>3040051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Andorra la Vella</td>\n",
       "      <td>Andorra</td>\n",
       "      <td>Andorra la Vella</td>\n",
       "      <td>3041563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Umm al Qaywayn</td>\n",
       "      <td>United Arab Emirates</td>\n",
       "      <td>Umm al Qaywayn</td>\n",
       "      <td>290594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ras al-Khaimah</td>\n",
       "      <td>United Arab Emirates</td>\n",
       "      <td>Raʼs al Khaymah</td>\n",
       "      <td>291074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Khawr Fakkān</td>\n",
       "      <td>United Arab Emirates</td>\n",
       "      <td>Ash Shāriqah</td>\n",
       "      <td>291696</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               name               country          subcountry  geonameid\n",
       "0      les Escaldes               Andorra  Escaldes-Engordany    3040051\n",
       "1  Andorra la Vella               Andorra    Andorra la Vella    3041563\n",
       "2    Umm al Qaywayn  United Arab Emirates      Umm al Qaywayn     290594\n",
       "3    Ras al-Khaimah  United Arab Emirates     Raʼs al Khaymah     291074\n",
       "4      Khawr Fakkān  United Arab Emirates        Ash Shāriqah     291696"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ciudades_del_mundo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "LISTA_CIUDADES=[]\n",
    "def generar_lista_ciudades(texto):\n",
    "    texto=str(texto)\n",
    "    texto=texto.lower()\n",
    "    lista_auxiliar=texto.split()\n",
    "    if len(lista_auxiliar)!=1:\n",
    "        texto=' '.join([word for word in lista_auxiliar])\n",
    "    global LISTA_CIUDADES\n",
    "    LISTA_CIUDADES.append(texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LISTA_CIUDADES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "ciudades=ciudades_del_mundo['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "ciudades=list(ciudades)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23018"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ciudades)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def menciona_ciudad(texto):\n",
    "    texto=texto.lower()\n",
    "    global ciudades\n",
    "    for ciudad in ciudades:\n",
    "        lista_auxiliar=ciudad.split()\n",
    "        if len(lista_auxiliar)!=1:\n",
    "            ciudad=' '.join([word for word in lista_auxiliar])\n",
    "        if ciudad.lower() in texto:\n",
    "            return ciudad\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ciudades_por_longitud(lista_ciudades):\n",
    "    diccionario={}\n",
    "    for ciudad in lista_ciudades:\n",
    "        longitud=len(ciudad)\n",
    "        contador=diccionario.get(longitud,-1)\n",
    "        if contador==-1:\n",
    "            diccionario[longitud]=1\n",
    "        else:\n",
    "            diccionario[longitud]=contador+1\n",
    "    for key in diccionario.keys():\n",
    "        print(\"llave: \" + str(key) + \"   \"+ str(diccionario[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ciudades_por_longitud(ciudades)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set=pd.read_csv('train_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-74-a1fb1febe72d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'menciona_ciudad'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmenciona_ciudad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   3846\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3847\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3848\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3850\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-73-634a0a1fc836>\u001b[0m in \u001b[0;36mmenciona_ciudad\u001b[0;34m(texto)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mciudad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mciudades\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mlista_auxiliar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mciudad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlista_auxiliar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m             \u001b[0mciudad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlista_auxiliar\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mciudad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtexto\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_set['menciona_ciudad']=train_set['text'].apply(menciona_ciudad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>caracteres_usados</th>\n",
       "      <th>menciones_realizadas</th>\n",
       "      <th>permite_location</th>\n",
       "      <th>use_keyword</th>\n",
       "      <th>cita_url</th>\n",
       "      <th>use_hashtag</th>\n",
       "      <th>menciona_ciudadd</th>\n",
       "      <th>menciona_ciudad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>69</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Aso</td>\n",
       "      <td>Aso</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Ron</td>\n",
       "      <td>Ron</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>133</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Ati</td>\n",
       "      <td>Ati</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>65</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Wil</td>\n",
       "      <td>Wil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>88</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Wil</td>\n",
       "      <td>Wil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>7608</td>\n",
       "      <td>10869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Two giant cranes holding a bridge collapse int...</td>\n",
       "      <td>1</td>\n",
       "      <td>83</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Ho</td>\n",
       "      <td>Ho</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>7609</td>\n",
       "      <td>10870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@aria_ahrary @TheTawniest The out of control w...</td>\n",
       "      <td>1</td>\n",
       "      <td>125</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Wil</td>\n",
       "      <td>Wil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>7610</td>\n",
       "      <td>10871</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n",
       "      <td>1</td>\n",
       "      <td>65</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Wa</td>\n",
       "      <td>Wa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>7611</td>\n",
       "      <td>10872</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Police investigating after an e-bike collided ...</td>\n",
       "      <td>1</td>\n",
       "      <td>137</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Teni</td>\n",
       "      <td>Teni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>7612</td>\n",
       "      <td>10873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
       "      <td>1</td>\n",
       "      <td>94</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Wil</td>\n",
       "      <td>Wil</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0     id keyword location  \\\n",
       "0              0      1     NaN      NaN   \n",
       "1              1      4     NaN      NaN   \n",
       "2              2      5     NaN      NaN   \n",
       "3              3      6     NaN      NaN   \n",
       "4              4      7     NaN      NaN   \n",
       "...          ...    ...     ...      ...   \n",
       "7608        7608  10869     NaN      NaN   \n",
       "7609        7609  10870     NaN      NaN   \n",
       "7610        7610  10871     NaN      NaN   \n",
       "7611        7611  10872     NaN      NaN   \n",
       "7612        7612  10873     NaN      NaN   \n",
       "\n",
       "                                                   text  target  \\\n",
       "0     Our Deeds are the Reason of this #earthquake M...       1   \n",
       "1                Forest fire near La Ronge Sask. Canada       1   \n",
       "2     All residents asked to 'shelter in place' are ...       1   \n",
       "3     13,000 people receive #wildfires evacuation or...       1   \n",
       "4     Just got sent this photo from Ruby #Alaska as ...       1   \n",
       "...                                                 ...     ...   \n",
       "7608  Two giant cranes holding a bridge collapse int...       1   \n",
       "7609  @aria_ahrary @TheTawniest The out of control w...       1   \n",
       "7610  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...       1   \n",
       "7611  Police investigating after an e-bike collided ...       1   \n",
       "7612  The Latest: More Homes Razed by Northern Calif...       1   \n",
       "\n",
       "      caracteres_usados  menciones_realizadas  permite_location  use_keyword  \\\n",
       "0                    69                     0                 0            0   \n",
       "1                    38                     0                 0            0   \n",
       "2                   133                     0                 0            0   \n",
       "3                    65                     0                 0            0   \n",
       "4                    88                     0                 0            0   \n",
       "...                 ...                   ...               ...          ...   \n",
       "7608                 83                     0                 0            0   \n",
       "7609                125                     2                 0            0   \n",
       "7610                 65                     0                 0            0   \n",
       "7611                137                     0                 0            0   \n",
       "7612                 94                     0                 0            0   \n",
       "\n",
       "      cita_url  use_hashtag menciona_ciudadd menciona_ciudad  \n",
       "0            0            1              Aso             Aso  \n",
       "1            0            0              Ron             Ron  \n",
       "2            0            0              Ati             Ati  \n",
       "3            0            1              Wil             Wil  \n",
       "4            0            2              Wil             Wil  \n",
       "...        ...          ...              ...             ...  \n",
       "7608         1            0               Ho              Ho  \n",
       "7609         0            0              Wil             Wil  \n",
       "7610         1            0               Wa              Wa  \n",
       "7611         0            0             Teni            Teni  \n",
       "7612         1            0              Wil             Wil  \n",
       "\n",
       "[7613 rows x 14 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.loc[train_set['menciona_ciudad']!=None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicc_paises={}\n",
    "def generar_lista_paises(texto):\n",
    "    global dicc_paises\n",
    "    texto=texto.lower()\n",
    "    esta=dicc_paises.get(texto,-1)\n",
    "    if esta==-1:\n",
    "        dicc_paises[texto]=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_paises=dicc_paises.keys()\n",
    "def menciona_pais(texto):\n",
    "    texto=texto.lower()\n",
    "    for pais in lista_paises:\n",
    "        if pais in texto:\n",
    "            return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        None\n",
       "1        None\n",
       "2        None\n",
       "3        None\n",
       "4        None\n",
       "         ... \n",
       "23013    None\n",
       "23014    None\n",
       "23015    None\n",
       "23016    None\n",
       "23017    None\n",
       "Name: country, Length: 23018, dtype: object"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ciudades_del_mundo['country'].apply(generar_lista_paises)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set['menciona_pais']=train_set['text'].apply(menciona_pais)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>caracteres_usados</th>\n",
       "      <th>menciones_realizadas</th>\n",
       "      <th>permite_location</th>\n",
       "      <th>use_keyword</th>\n",
       "      <th>cita_url</th>\n",
       "      <th>use_hashtag</th>\n",
       "      <th>menciona_pais</th>\n",
       "      <th>numero_de_palabras</th>\n",
       "      <th>palabras_unicas</th>\n",
       "      <th>numero_palabras_desconocidas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>deed reason earthquake may allah forgive</td>\n",
       "      <td>1</td>\n",
       "      <td>69</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>resident ask shelter place notify officer evac...</td>\n",
       "      <td>1</td>\n",
       "      <td>133</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>people receive wildfire evacuation order calif...</td>\n",
       "      <td>1</td>\n",
       "      <td>65</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>get sent photo ruby alaska smoke wildfire pour...</td>\n",
       "      <td>1</td>\n",
       "      <td>88</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  id keyword location  \\\n",
       "0           0   1     NaN      NaN   \n",
       "1           1   4     NaN      NaN   \n",
       "2           2   5     NaN      NaN   \n",
       "3           3   6     NaN      NaN   \n",
       "4           4   7     NaN      NaN   \n",
       "\n",
       "                                                text  target  \\\n",
       "0           deed reason earthquake may allah forgive       1   \n",
       "1              forest fire near la ronge sask canada       1   \n",
       "2  resident ask shelter place notify officer evac...       1   \n",
       "3  people receive wildfire evacuation order calif...       1   \n",
       "4  get sent photo ruby alaska smoke wildfire pour...       1   \n",
       "\n",
       "   caracteres_usados  menciones_realizadas  permite_location  use_keyword  \\\n",
       "0                 69                     0                 0            0   \n",
       "1                 38                     0                 0            0   \n",
       "2                133                     0                 0            0   \n",
       "3                 65                     0                 0            0   \n",
       "4                 88                     0                 0            0   \n",
       "\n",
       "   cita_url  use_hashtag  menciona_pais  numero_de_palabras  palabras_unicas  \\\n",
       "0         0            1              0                   6                6   \n",
       "1         0            0              1                   7                7   \n",
       "2         0            0              0                  11                9   \n",
       "3         0            1              0                   6                6   \n",
       "4         0            2              0                   9                9   \n",
       "\n",
       "   numero_palabras_desconocidas  \n",
       "0                             0  \n",
       "1                             0  \n",
       "2                             0  \n",
       "3                             0  \n",
       "4                             0  "
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Las siguientes features usarlas sólo despues de limpiar el texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set['text']=train_set['text'].apply(limpiar_texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set['text']=train_set['text'].apply(lemmatizar_con_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set['text']=train_set['text'].apply(limpiar_texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature: cantidad de palabras en el tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contar_cantidad_de_palabras(texto):\n",
    "    contador=0\n",
    "    texto=texto.lower()\n",
    "    lista_palabras=texto.split()\n",
    "    for palabra in lista_palabras:\n",
    "        contador+=1\n",
    "    return contador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set['numero_de_palabras']=train_set['text'].apply(contar_cantidad_de_palabras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature: cantidad de palabras únicas utilizadas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nota: aplicar la función solamente una vez que el texto fue limpiado\n",
    "def contar_cantidad_de_palabras_unicas(texto):\n",
    "    diccionario_auxiliar={}\n",
    "    palabras=texto.split()\n",
    "    for palabra in palabras:\n",
    "        esta=diccionario_auxiliar.get(palabra,\"no esta\")\n",
    "        if(esta==\"no esta\"):\n",
    "            diccionario_auxiliar[palabra]=\"esta\"\n",
    "    return(len(diccionario_auxiliar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set['palabras_unicas']=train_set['text'].apply(contar_cantidad_de_palabras_unicas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature: cantidad de palabras no reconocidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_glove='/home/tomas/organizacion_de_datos/tp2/glove.6B.100d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "diccionario_de_palabras={}\n",
    "def generar_diccionario_de_palabras():\n",
    "    embeddings=open(path_glove,'r',encoding='UTF-8')\n",
    "    while(True):\n",
    "        f=embeddings.readline()\n",
    "        if(len(f)==0):\n",
    "            break\n",
    "        f=f.split()\n",
    "        lista=f[0].split()\n",
    "        diccionario_de_palabras[lista[0]]='y'\n",
    "    embeddings.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "generar_diccionario_de_palabras()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contar_palabras_desconocidas(texto):\n",
    "    global diccionario_de_palabras\n",
    "    contador=0\n",
    "    texto=texto.lower()\n",
    "    lista_palabras=texto.split()\n",
    "    for palabra in lista_palabras:\n",
    "        esta=diccionario_de_palabras.get(palabra,'no esta')\n",
    "        if esta=='no esta':\n",
    "            contador+=1\n",
    "    return contador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set['numero_palabras_desconocidas']=train_set['text'].apply(contar_palabras_desconocidas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>caracteres_usados</th>\n",
       "      <th>menciones_realizadas</th>\n",
       "      <th>permite_location</th>\n",
       "      <th>use_keyword</th>\n",
       "      <th>cita_url</th>\n",
       "      <th>use_hashtag</th>\n",
       "      <th>menciona_pais</th>\n",
       "      <th>numero_de_palabras</th>\n",
       "      <th>palabras_unicas</th>\n",
       "      <th>numero_palabras_desconocidas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>deed reason earthquake may allah forgive</td>\n",
       "      <td>1</td>\n",
       "      <td>69</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>No</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Si</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>resident ask shelter place notify officer evac...</td>\n",
       "      <td>1</td>\n",
       "      <td>133</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>people receive wildfire evacuation order calif...</td>\n",
       "      <td>1</td>\n",
       "      <td>65</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>No</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>get sent photo ruby alaska smoke wildfire pour...</td>\n",
       "      <td>1</td>\n",
       "      <td>88</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>No</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  id keyword location  \\\n",
       "0           0   1     NaN      NaN   \n",
       "1           1   4     NaN      NaN   \n",
       "2           2   5     NaN      NaN   \n",
       "3           3   6     NaN      NaN   \n",
       "4           4   7     NaN      NaN   \n",
       "\n",
       "                                                text  target  \\\n",
       "0           deed reason earthquake may allah forgive       1   \n",
       "1              forest fire near la ronge sask canada       1   \n",
       "2  resident ask shelter place notify officer evac...       1   \n",
       "3  people receive wildfire evacuation order calif...       1   \n",
       "4  get sent photo ruby alaska smoke wildfire pour...       1   \n",
       "\n",
       "   caracteres_usados  menciones_realizadas  permite_location  use_keyword  \\\n",
       "0                 69                     0                 0            0   \n",
       "1                 38                     0                 0            0   \n",
       "2                133                     0                 0            0   \n",
       "3                 65                     0                 0            0   \n",
       "4                 88                     0                 0            0   \n",
       "\n",
       "   cita_url  use_hashtag menciona_pais  numero_de_palabras  palabras_unicas  \\\n",
       "0         0            1            No                   6                6   \n",
       "1         0            0            Si                   7                7   \n",
       "2         0            0            No                  11                9   \n",
       "3         0            1            No                   6                6   \n",
       "4         0            2            No                   9                9   \n",
       "\n",
       "   numero_palabras_desconocidas  \n",
       "0                             0  \n",
       "1                             0  \n",
       "2                             0  \n",
       "3                             0  \n",
       "4                             0  "
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set=pd.read_csv('test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set['menciona_pais']=test_set['text'].apply(menciona_pais)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set['text']=test_set['text'].apply(limpiar_texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set['text']=test_set['text'].apply(lemmatizar_con_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set['text']=test_set['text'].apply(limpiar_texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set['numero_de_palabras']=test_set['text'].apply(contar_cantidad_de_palabras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set['palabras_unicas']=test_set['text'].apply(contar_cantidad_de_palabras_unicas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set['numero_palabras_desconocidas']=test_set['text'].apply(contar_palabras_desconocidas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertir(texto):\n",
    "    if texto==\"Si\":\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set['menciona_pais']=test_set['menciona_pais'].apply(convertir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>caracteres_usados</th>\n",
       "      <th>menciones_realizadas</th>\n",
       "      <th>permite_location</th>\n",
       "      <th>use_keyword</th>\n",
       "      <th>cita_url</th>\n",
       "      <th>use_hashtag</th>\n",
       "      <th>menciona_pais</th>\n",
       "      <th>numero_de_palabras</th>\n",
       "      <th>palabras_unicas</th>\n",
       "      <th>numero_palabras_desconocidas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>happen terrible car crash</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>heard earthquake sorry different city stay saf...</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sorry forest fire spot pond geese flee across ...</td>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>apocalypse light spokane wildfire</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>typhoon soudelor kill china taiwan</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3258</th>\n",
       "      <td>3258</td>\n",
       "      <td>10861</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>earthquake safety los angeles ûò safety faste...</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3259</th>\n",
       "      <td>3259</td>\n",
       "      <td>10865</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>storm ri bad last hurricane hardest hit yard l...</td>\n",
       "      <td>139</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3260</th>\n",
       "      <td>3260</td>\n",
       "      <td>10868</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>green line derailment chicago</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3261</th>\n",
       "      <td>3261</td>\n",
       "      <td>10874</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>meg issue hazardous weather outlook hwo</td>\n",
       "      <td>65</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3262</th>\n",
       "      <td>3262</td>\n",
       "      <td>10875</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cityofcalgary activate municipal emergency pla...</td>\n",
       "      <td>68</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3263 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0     id keyword location  \\\n",
       "0              0      0     NaN      NaN   \n",
       "1              1      2     NaN      NaN   \n",
       "2              2      3     NaN      NaN   \n",
       "3              3      9     NaN      NaN   \n",
       "4              4     11     NaN      NaN   \n",
       "...          ...    ...     ...      ...   \n",
       "3258        3258  10861     NaN      NaN   \n",
       "3259        3259  10865     NaN      NaN   \n",
       "3260        3260  10868     NaN      NaN   \n",
       "3261        3261  10874     NaN      NaN   \n",
       "3262        3262  10875     NaN      NaN   \n",
       "\n",
       "                                                   text  caracteres_usados  \\\n",
       "0                             happen terrible car crash                 34   \n",
       "1     heard earthquake sorry different city stay saf...                 64   \n",
       "2     sorry forest fire spot pond geese flee across ...                 96   \n",
       "3                     apocalypse light spokane wildfire                 40   \n",
       "4                    typhoon soudelor kill china taiwan                 45   \n",
       "...                                                 ...                ...   \n",
       "3258  earthquake safety los angeles ûò safety faste...                 55   \n",
       "3259  storm ri bad last hurricane hardest hit yard l...                139   \n",
       "3260                      green line derailment chicago                 55   \n",
       "3261            meg issue hazardous weather outlook hwo                 65   \n",
       "3262  cityofcalgary activate municipal emergency pla...                 68   \n",
       "\n",
       "      menciones_realizadas  permite_location  use_keyword  cita_url  \\\n",
       "0                        0                 0            0         0   \n",
       "1                        0                 0            0         0   \n",
       "2                        0                 0            0         0   \n",
       "3                        0                 0            0         0   \n",
       "4                        0                 0            0         0   \n",
       "...                    ...               ...          ...       ...   \n",
       "3258                     0                 0            0         0   \n",
       "3259                     0                 0            0         0   \n",
       "3260                     0                 0            0         1   \n",
       "3261                     0                 0            0         1   \n",
       "3262                     0                 0            0         0   \n",
       "\n",
       "      use_hashtag  menciona_pais  numero_de_palabras  palabras_unicas  \\\n",
       "0               0              0                   4                4   \n",
       "1               1              0                   8                8   \n",
       "2               0              0                  11               11   \n",
       "3               2              0                   4                4   \n",
       "4               0              1                   5                5   \n",
       "...           ...            ...                 ...              ...   \n",
       "3258            0              0                   8                7   \n",
       "3259            0              0                  15               15   \n",
       "3260            0              0                   4                4   \n",
       "3261            0              0                   6                6   \n",
       "3262            2              0                   6                6   \n",
       "\n",
       "      numero_palabras_desconocidas  \n",
       "0                                0  \n",
       "1                                0  \n",
       "2                                0  \n",
       "3                                0  \n",
       "4                                0  \n",
       "...                            ...  \n",
       "3258                             2  \n",
       "3259                             0  \n",
       "3260                             0  \n",
       "3261                             1  \n",
       "3262                             2  \n",
       "\n",
       "[3263 rows x 15 columns]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
