{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "#import xgboost as xgb\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import VotingClassifier, BaggingRegressor, AdaBoostRegressor\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from time import time\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Embedding,LSTM,GRU\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer=SnowballStemmer(language='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemmizar_texto(texto):\n",
    "    texto=' '.join([stemmer.stem(palabra) for palabra in texto.split() ])\n",
    "    return texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Santi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "english_stopwords=set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set=pd.read_csv(\"train.csv\",encoding='utf-8')\n",
    "test_set=pd.read_csv(\"test.csv\",encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quitar_stopwords(texto):\n",
    "    texto=' '.join([word for word in texto.split() if word not in english_stopwords])\n",
    "    return texto\n",
    "\n",
    "def quitar_menciones(texto):\n",
    "    texto=' '.join([palabra for palabra in texto.split() if '@' not in palabra])\n",
    "    return texto\n",
    "\n",
    "def quitar_links(texto):\n",
    "    texto=' '.join([palabra for palabra in texto.split() if 'http:/' not in palabra])\n",
    "    return texto\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Diccionario_de_lenguaje_de_internet={\n",
    "\"2moro\": \"Tomorrow\",\n",
    "\"2nite\": \"Tonight\",\n",
    "\"4EAE\": \"For Ever And Ever\",\n",
    "\"ABT\":\"About\",\n",
    "\"ADN\": \"Any Day Now\",\n",
    "\"AFAIC\": \"As Far As I’m Concerned\",\n",
    "\"AFAICT\": \"As Far As I Can Tell\",\n",
    "\"AFAIK\": \"As Far As I Know\",\n",
    "\"AFAIR\": \"As Far As I Remember\",\n",
    "\"AKA\": \"Also Known As\",\n",
    "\"AMA\": \"Ask Me Anything\",\n",
    "\"ASAIC\": \"As Soon As I Can\",\n",
    "\"ASAP\": \"As Soon As Possible\",\n",
    "\"ATM\": \"At The Moment\",\n",
    "\"B4\": \"Before\",\n",
    "\"B4N\": \"Bye For Now\",\n",
    "\"Bae\": \"Babe/Before Anyone Else\",\n",
    "\"BBL\": \"Be Back Later\",\n",
    "\"BBT\": \"Be Back Tomorrow\",\n",
    "\"BCNU\": \"Be Seeing You\",\n",
    "\"BD\": \"Big Deal\",\n",
    "\"BF\": \"Boyfriend\",\n",
    "\"BFF\": \"Best Friends Forever\",\n",
    "\"BMT\": \"Before My Time\",\n",
    "\"BOL\": \"Be On Later\",\n",
    "\"BOT\": \"Back On Topic\",\n",
    "\"BRB\": \"Be Right Back\",\n",
    "\"BRO\": \"Brother\",\n",
    "\"BT\": \"But\",\n",
    "\"BTW\": \"By The Way\",\n",
    "\"CFY\": \"Calling For You\",\n",
    "\"CU\": \"See You\",\n",
    "\"CUL\": \"See You Later\",\n",
    "\"Cuz\": \"Because\",\n",
    "\"CYA\": \"Cover Your Ass\",\n",
    "\"DAE\": \"Does Anyone Else\",\n",
    "\"DBA\": \"Doing Business As\",\n",
    "\"DFTBA\": \"Don’t Forget To Be Awesome\",\n",
    "\"DIKU\": \"Do I Know You\",\n",
    "\"DM\": \"Direct Message\",\n",
    "\"DND\": \"Do Not Disturb\",\n",
    "\"DR\": \"Double Rainbow\",\n",
    "\"DWBH\": \"Don’t Worry, Be Happy\",\n",
    "\"ELI5\": \"Explain Like I’m 5\",\n",
    "\"EOM\": \"End Of Message\",\n",
    "\"EOS\": \"End Of Story\",\n",
    "\"F2F\": \"Face To Face\",\n",
    "\"FAQ\": \"Frequently Asked Question\",\n",
    "\"FB\": \"Facebook\",\n",
    "\"FBF\": \"Flash Back Friday\",\n",
    "\"FF\": \"Follow Friday\",\n",
    "\"FIFY\": \"Fixed It For You\",\n",
    "\"FITB\": \"Fill In The Blank\",\n",
    "\"FML\": \"Fuck My Life\",\n",
    "\"FOMO\": \"Fear Of Missing Out\",\n",
    "\"FTFY\": \"Fixed That For You\",\n",
    "\"FTL\": \"For The Loss\",\n",
    "\"FTW\": \"For The Win\",\n",
    "\"FWB\": \"Friends With Benefits\",\n",
    "\"FWIW\": \"For What It’s Worth\",\n",
    "\"FYE\": \"For Your Entertainment\",\n",
    "\"FYEO\": \"For Your Eyes Only\",\n",
    "\"FYI\": \"For Your Information\",\n",
    "\"GA\": \"Go Ahead\",\n",
    "\"GAL\": \"Get A Life\",\n",
    "\"GF\": \"Girlfriend\",\n",
    "\"GM\": \"Good Morning\",\n",
    "\"GN\": \"Good Night\",\n",
    "\"Gr8\": \"Great\",\n",
    "\"GTR\": \"Getting Ready\",\n",
    "\"HAND\": \"Have A Nice Day\",\n",
    "\"HB\": \"Hurry Back\",\n",
    "\"HBD\": \"Happy Birthday\",\n",
    "\"HBU\": \"How About You\",\n",
    "\"HMB\": \"Hit Be Back\",\n",
    "\"HMU\": \"Hit Me Up\",\n",
    "\"HRU\": \"How Are You\",\n",
    "\"HTH\": \"Hope This Helps\",\n",
    "\"IAC\": \"In Any Case\",\n",
    "\"IC\": \"I See\",\n",
    "\"ICYMI\": \"In Case You Missed It\",\n",
    "\"IDC\": \"I Don’t Care\",\n",
    "\"IDK\": \"I Don’t Know\",\n",
    "\"IG\": \"Instagram\",\n",
    "\"IIRC\": \"If I Remember Correctly\",\n",
    "\"IKR\": \"I Know Right\",\n",
    "\"ILY\": \"I Love You\",\n",
    "\"IMHO\": \"In My Humble Opinion\",\n",
    "\"IMMD\": \"It Made My Day\",\n",
    "\"IMY\": \"I Miss You\",\n",
    "\"IRL\": \"In Real Life\",\n",
    "\"IS\": \"I’m Sorry\",\n",
    "\"ISO\": \"In Search Of\",\n",
    "\"IU2U\": \"It’s Up To You\",\n",
    "\"J4F\": \"Just For Fun\",\n",
    "\"JAM\": \"Just A Minute\",\n",
    "\"JFY\": \"Just For You\",\n",
    "\"JIC\": \"Just In Case\",\n",
    "\"JK\": \"Just Kidding\",\n",
    "\"JSYK\": \"Just So You Know\",\n",
    "\"KK\": \"Okay\",\n",
    "\"L8\": \"Late\",\n",
    "\"L8R\": \"Later\",\n",
    "\"LMA\": \"Leave Me Alone\",\n",
    "\"LMAO\": \"Laughing My Ass Off\",\n",
    "\"LMBO\": \"Laughing My Butt Off\",\n",
    "\"LMK\": \"Let Me Know\",\n",
    "\"LOL\": \"Laugh Out Loud\",\n",
    "\"LTNS\": \"Long Time No See\",\n",
    "\"LYLAS\": \"Love You Like A Sister\",\n",
    "\"M/F\": \"Male or Female\",\n",
    "\"M8\": \"Mate\",\n",
    "\"MP\": \"My pleasure\",\n",
    "\"MSM\": \"Mainstream Media\",\n",
    "\"MU\": \"Miss You\",\n",
    "\"MYOB\": \"Mind Your Own Business\",\n",
    "\"NAGI\": \"Not A Good Idea\",\n",
    "\"NBD\": \"No Big Deal\",\n",
    "\"NE1\": \"Anyone\",\n",
    "\"NM\": \"Not Much\",\n",
    "\"NP\": \"No Problem\",\n",
    "\"NSFL\": \"Not Safe For Life\",\n",
    "\"NSFW\": \"Not Safe For Work\",\n",
    "\"NTS\": \"Note To Self\",\n",
    "\"NVM\": \"Never Mind\",\n",
    "\"OC\": \"Original Content\",\n",
    "\"OH\": \"Overheard\",\n",
    "\"OIC\": \"Oh ! I See\",\n",
    "\"OMD\": \"Oh My Damn\",\n",
    "\"OMG\": \"Oh My Goodness\",\n",
    "\"OMW\": \"On My Way\",\n",
    "\"OT\": \"Off Topic\",\n",
    "\"OFC\": \"Of course\",\n",
    "\"PAW\": \"Parents Are Watching\",\n",
    "\"Pls\": \"Please\",\n",
    "\"POTD\": \"Photo Of The Day\",\n",
    "\"POV\": \"Point Of View\",\n",
    "\"PPL\": \"People\",\n",
    "\"PTB\": \"Please Text Back\",\n",
    "\"Q4U\": \"Question For You\",\n",
    "\"QQ\": \"Crying\",\n",
    "\"RBTL\": \"Read Between The Lines\",\n",
    "\"RIP\": \"Rest In Peace\",\n",
    "\"RL\": \"Real Life\",\n",
    "\"ROFL\": \"Rolling On the Floor Laughing\",\n",
    "\"RT\": \"Retweet\",\n",
    "\"RTM\": \"Read The Manual\",\n",
    "\"SIS\": \"Sister\",\n",
    "\"SITD\": \"Still In The Dark\",\n",
    "\"SM\": \"Social Media\",\n",
    "\"SMH\": \"Shaking My Head\",\n",
    "\"SMY\": \"Somebody\",\n",
    "\"SNH\": \"Sarcasm Noted Here\",\n",
    "\"SOL\": \"Sooner Or Later\",\n",
    "\"Some1\": \"Someone\",\n",
    "\"SRSLY\": \"Seriously\",\n",
    "\"STBY\": \"Sucks To Be You\",\n",
    "\"Str8\": \"Straight\",\n",
    "\"SYS\": \"See You Soon\",\n",
    "\"TBA\": \"To Be Announced\",\n",
    "\"TBH\": \"To Be Honest\",\n",
    "\"TBT\": \"Throwback Thursday\",\n",
    "\"TBT\": \"Truth Be Told\",\n",
    "\"TFH\": \"Thread From Hell\",\n",
    "\"TFTI\": \"Thanks For The Invite\",\n",
    "\"TGIF\": \"Thank God It’s Friday\",\n",
    "\"THX\": \"Thanks\",\n",
    "\"TIA\": \"Thanks in Advance\",\n",
    "\"TIL\": \"Today I Learned\",\n",
    "\"TIME\": \"Tears In My Eyes\",\n",
    "\"TL;DR\": \"Too Long; Didn’t Read\",\n",
    "\"TLDR\":\"Too long didn’t read\",\n",
    "\"TL DR\":\"Too long didn’t read\",\n",
    "\"TLC\": \"Tender Loving Care\",\n",
    "\"TMI\": \"Too Much Information\",\n",
    "\"TTYL\": \"Talk To You Later\",\n",
    "\"TTYS\": \"Talk To You Soon\",\n",
    "\"Txt\": \"Text\",\n",
    "\"TYVM\": \"Thank You Very Much\",\n",
    "\"U\": \"You\",\n",
    "\"U4F\": \"You Forever\",\n",
    "\"UR\": \"Your\",\n",
    "\"VBG\": \"Very Big Grin\",\n",
    "\"VSF\": \"Very Sad Face\",\n",
    "\"WB\": \"Welcome Back\",\n",
    "\"WBU\": \"What About You?\",\n",
    "\"WEG\": \"Wicked Evil Grin\",\n",
    "\"WKND\": \"Weekend\",\n",
    "\"WOM\": \"Word of Mouth\",\n",
    "\"WOTD\": \"Word Of The Day\",\n",
    "\"Wru\": \"Who Are You\",\n",
    "\"WTH\": \"What The Heck?\",\n",
    "\"WTPA\": \"Where The Party At?\",\n",
    "\"WU?\": \"What's Up\",\n",
    "\"WU\":\"What's Up\",\n",
    "\"WYCM\": \"Will You Call Me?\",\n",
    "\"WYWH\": \"Wish You Were Here\",\n",
    "\"XOXO\": \"Hugs and Kisses\",\n",
    "\"YGM\": \"You’ve Got Mail\",\n",
    "\"YNK\": \"You Never Know\",\n",
    "\"YOLO\": \"You Only Live Once\",\n",
    "\"YT\": \"YouTube\",\n",
    "\"YW\": \"You’re Welcome\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reemplazar_lenguaje_internet(texto):\n",
    "    texto=texto.upper()\n",
    "    palabras=texto.split()\n",
    "    palabras_procesadas=[]\n",
    "    for palabra in palabras:\n",
    "        traduccion=Diccionario_de_lenguaje_de_internet.get(palabra,'not internet slang')\n",
    "        if(traduccion!='not internet slang'):\n",
    "            lista_aux=traduccion.split()\n",
    "            for x in lista_aux:\n",
    "                palabras_procesadas.append(x.lower())\n",
    "        else:\n",
    "            palabras_procesadas.append(palabra.lower())\n",
    "    texto=' '.join([word for word in palabras_procesadas])\n",
    "    return texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "diccionario_contracciones = {  \"ain't\": \"is not\",\n",
    "                               \"aint\":\"is not\",\n",
    "                               \"aren't\": \"are not\",\n",
    "                               \"arent\":\"are not\",\n",
    "                               \"can't\": \"can not\",\n",
    "                               \"cant\":\"can not\",\n",
    "                               \"cause\": \"because\",\n",
    "                               \"could've\": \"could have\",\n",
    "                               \"couldve\":\"could have\",\n",
    "                               \"couldn't\": \"could not\",\n",
    "                               \"couldnt\":\"could not\",\n",
    "                               \"didn't\": \"did not\", \n",
    "                               \"didnt\":\"did not\",\n",
    "                               \"doesn't\": \"does not\",\n",
    "                               \"doesnt\":\"does not\",\n",
    "                               \"don't\": \"do not\",\n",
    "                               \"dont\":\"do not\",\n",
    "                               \"hadn't\": \"had not\",\n",
    "                               \"hadnt\":\"had not\",\n",
    "                               \"hasn't\": \"has not\",\n",
    "                               \"hasnt\":\"has not\",\n",
    "                               \"haven't\": \"have not\",\n",
    "                               \"havent\":\"have not\",\n",
    "                               \"he'd\": \"he would\",\n",
    "                               \"hed\":\"he would\",\n",
    "                               \"he'll\": \"he will\",\n",
    "                               \"he ll\":\"he will\",\n",
    "                               \"he's\": \"he is\",\n",
    "                               \"hes\":\"he is\",\n",
    "                               \"how'd\": \"how did\",\n",
    "                               \"howd\":\"how did\",\n",
    "                               \"how'd'y\": \"how do you\",\n",
    "                               \"howdy\":\"how do you\",\n",
    "                               \"how'll\": \"how will\",\n",
    "                               \"howll\":\"how will\",\n",
    "                               \"how's\": \"how is\",\n",
    "                               \"hows\":\"how is\",\n",
    "                               \"I'd\": \"I would\",\n",
    "                               \"id\":\"i would\",\n",
    "                               \"I'd've\": \"I would have\",\n",
    "                               \"idve\":\"i would have\",\n",
    "                               \"I'll\": \"I will\", \n",
    "                               \"Ill\":\"i will\", #duda: si algun tweet habla de un enfermo puede traer ruido\n",
    "                               \"I'll've\": \"I will have\",\n",
    "                               \"I'm\": \"I am\", \n",
    "                               \"im\":\"i am\",\n",
    "                               \"I've\": \"I have\",\n",
    "                               \"ive\":\"i have\",\n",
    "                               \"i'd\": \"i would\",\n",
    "                               \"i'd've\": \"i would have\",\n",
    "                               \"i'll\": \"i will\",\n",
    "                               \"i'll've\": \"i will have\",\n",
    "                               \"i'm\": \"i am\", \n",
    "                               \"im\":\"i am\",\n",
    "                               \"i've\": \"i have\",\n",
    "                               \"isn't\": \"is not\",\n",
    "                               \"isnt\":\"is not\",\n",
    "                               \"it'd\": \"it would\",\n",
    "                               \"itd\":\"it would\",\n",
    "                               \"it'd've\": \"it would have\",\n",
    "                               \"it'll\": \"it will\",\n",
    "                               \"itll\":\"it will\",\n",
    "                               \"it'll've\": \"it will have\",\n",
    "                               \"it's\": \"it is\",\n",
    "                               \"its\":\"it is\",\n",
    "                               \"let's\": \"let us\",\n",
    "                               \"lets\":\"let us\",\n",
    "                               \"ma'am\": \"madam\",\n",
    "                               \"maam\":\"madam\",\n",
    "                               \"mayn't\": \"may not\",\n",
    "                               \"maynt\":\"may not\",\n",
    "                               \"might've\": \"might have\",\n",
    "                               \"mightve\":\"might have\",\n",
    "                               \"mightn't\": \"might not\",\n",
    "                               \"mightnt\":\"might not\",\n",
    "                               \"mightn't've\": \"might not have\",\n",
    "                               \"must've\": \"must have\",\n",
    "                               \"mustve\":\"must have\",\n",
    "                               \"mustn't\": \"must not\",\n",
    "                               \"mustnt\":\"must not\",\n",
    "                               \"mustn't've\": \"must not have\",\n",
    "                               \"needn't\": \"need not\",\n",
    "                               \"neednt\":\"need not\",\n",
    "                               \"needn't've\": \"need not have\",\n",
    "                               \"o'clock\": \"of the clock\",\n",
    "                               \"oclock\":\"of the clock\",\n",
    "                               \"oughtn't\": \"ought not\",\n",
    "                               \"oughtnt\":\"ought not\",\n",
    "                               \"oughtn't've\": \"ought not have\",\n",
    "                               \"shan't\": \"shall not\",\n",
    "                               \"shant\":\"shall not\",\n",
    "                               \"sha'n't\": \"shall not\",\n",
    "                               \"shan't've\": \"shall not have\",\n",
    "                               \"she'd\": \"she would\",\n",
    "                               \"shed\":\"she would\",\n",
    "                               \"she'd've\": \"she would have\",\n",
    "                               \"she'll\": \"she will\",\n",
    "                               \"shell\":\"she will\",#nuevamente aca tengo ruido por dos palabras igual escritas\n",
    "                               \"she'll've\": \"she will have\", \n",
    "                               \"she's\": \"she is\",\n",
    "                               \"shes\":\"she is\",\n",
    "                               \"should've\": \"should have\",\n",
    "                               \"shouldve\":\"should have\",\n",
    "                               \"shouldn't\": \"should not\",\n",
    "                               \"shouldnt\": \"should not\",\n",
    "                               \"shouldn't've\": \"should not have\",\n",
    "                               \"so've\": \"so have\",\n",
    "                               \"so's\": \"so as\",\n",
    "                               \"this's\": \"this is\",\n",
    "                               \"that'd\": \"that would\",\n",
    "                               \"that'd've\": \"that would have\",\n",
    "                               \"that's\": \"that is\",\n",
    "                               \"thats\":\"that is\",\n",
    "                               \"there'd\": \"there would\",\n",
    "                               \"thered\":\"there would\",\n",
    "                               \"there'd've\": \"there would have\",\n",
    "                               \"there's\": \"there is\",\n",
    "                               \"theres\":\"there is\",\n",
    "                               \"here's\": \"here is\",\n",
    "                               \"heres\":\"here is\",\n",
    "                               \"they'd\": \"they would\",\n",
    "                               \"theyd\":\"they would\",\n",
    "                               \"they'd've\": \"they would have\",\n",
    "                               \"they'll\": \"they will\",\n",
    "                               \"theyll\":\"they will\",\n",
    "                               \"they'll've\": \"they will have\", \n",
    "                               \"they're\": \"they are\",\n",
    "                               \"theyre\":\"they are\",\n",
    "                               \"they've\": \"they have\", \n",
    "                               \"theyve\":\"they have\",\n",
    "                               \"to've\": \"to have\", \n",
    "                               \"tove\":\"to have\",\n",
    "                               \"wasn't\": \"was not\",\n",
    "                               \"wasnt\":\"was not\",\n",
    "                               \"we'd\": \"we would\",\n",
    "                               \"wed\":\"we would\",#aca nuevamente hay conflicto\n",
    "                               \"we'd've\": \"we would have\",\n",
    "                               \"we'll\": \"we will\",\n",
    "                               \"we'll've\": \"we will have\",\n",
    "                               \"we're\": \"we are\",\n",
    "                               \"we've\": \"we have\",\n",
    "                               \"weren't\": \"were not\",\n",
    "                               \"werent\":\"were not\",\n",
    "                               \"what'll\": \"what will\",\n",
    "                               \"whatll\":\"what will\",\n",
    "                               \"what'll've\": \"what will have\",\n",
    "                               \"what're\": \"what are\", \n",
    "                               \"whatre\":\"what are\",\n",
    "                               \"what's\": \"what is\",\n",
    "                               \"whats\":\"what is\",\n",
    "                               \"what've\": \"what have\",\n",
    "                               \"whatve\":\"what have\",\n",
    "                               \"when's\": \"when is\",\n",
    "                               \"whens\":\"when is\",\n",
    "                               \"when've\": \"when have\",\n",
    "                               \"whenve\":\"when have\",\n",
    "                               \"where'd\": \"where did\",\n",
    "                               \"whered\":\"where did\",\n",
    "                               \"where's\": \"where is\",\n",
    "                               \"wheres\":\"where is\",\n",
    "                               \"where've\": \"where have\",\n",
    "                               \"whereve\":\"where have\",\n",
    "                               \"who'll\": \"who will\", \n",
    "                               \"wholl\":\"who will\",\n",
    "                               \"who'll've\": \"who will have\",\n",
    "                               \"who's\": \"who is\",\n",
    "                               \"whos\":\"who is\",\n",
    "                               \"who've\": \"who have\",\n",
    "                               \"whove\":\"who have\",\n",
    "                               \"why's\": \"why is\",\n",
    "                               \"whys\":\"why is\",\n",
    "                               \"why've\": \"why have\",\n",
    "                               \"whyve\":\"why have\",\n",
    "                               \"will've\": \"will have\",\n",
    "                               \"willve\":\"will have\",\n",
    "                               \"won't\": \"will not\",\n",
    "                               \"wont\":\"will not\",\n",
    "                               \"won't've\": \"will not have\",\n",
    "                               \"would've\": \"would have\",\n",
    "                               \"wouldve\":\"would have\",\n",
    "                               \"wouldn't\": \"would not\",\n",
    "                               \"wouldnt\":\"would not\",\n",
    "                               \"wouldn't've\": \"would not have\",\n",
    "                               \"y'all\": \"you all\",\n",
    "                               \"yall\":\"you all\",\n",
    "                               \"y'all'd\": \"you all would\",\n",
    "                               \"yalld\":\"you all would\",\n",
    "                               \"y'all'd've\": \"you all would have\",\n",
    "                               \"y'all're\": \"you all are\",\n",
    "                               \"yallre\":\"you all are\",\n",
    "                               \"y'all've\": \"you all have\",\n",
    "                               \"you'd\": \"you would\",\n",
    "                               \"youd\":\"you would\",\n",
    "                               \"you'd've\": \"you would have\",\n",
    "                               \"you'll\": \"you will\",\n",
    "                               \"youll\":\"you will\",\n",
    "                               \"you'll've\": \"you will have\",\n",
    "                               \"you're\": \"you are\",\n",
    "                               \"youre\":\"you are\",\n",
    "                               \"you've\": \"you have\",\n",
    "                               \"youve\":\"you have\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reemplazar_contracciones(texto):\n",
    "    texto=texto.lower()\n",
    "    palabras=texto.split()\n",
    "    palabras_procesadas=[]\n",
    "    for palabra in palabras:\n",
    "        traduccion=diccionario_contracciones.get(palabra,'no es contraccion')\n",
    "        if(traduccion!='no es contraccion'):\n",
    "            lista_aux=traduccion.split()\n",
    "            for x in lista_aux:\n",
    "                palabras_procesadas.append(x.lower())\n",
    "        else:\n",
    "            palabras_procesadas.append(palabra.lower())\n",
    "    texto=' '.join([word for word in palabras_procesadas])\n",
    "    return texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eliminar_palabras_de_una_sola_letra(texto):\n",
    "    texto=texto.lower()\n",
    "    lista_palabras=texto.split()\n",
    "    palabras_procesadas=[]\n",
    "    for palabra in lista_palabras:\n",
    "        if(len(palabra)!=1):\n",
    "            palabras_procesadas.append(palabra)\n",
    "    texto=\" \".join([word for word in palabras_procesadas])\n",
    "    return texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#es necesario importar la libreria re (regular expression) para poder usar esta funcion\n",
    "#ya esta bajada en el comienzo del notebook pero si lo que quieren es analizar la documentación\n",
    "#lo pueden revisar en google\n",
    "#falta borrar los arroba\n",
    "def limpiar_texto(texto):\n",
    "    texto = reemplazar_lenguaje_internet(texto)\n",
    "    texto=reemplazar_contracciones(texto)\n",
    "    texto=texto.lower()\n",
    "    texto = re.sub('\\[.*?\\]', '', texto) # remove text in square brackets\n",
    "    texto = re.sub('https?://\\S+|www\\.\\S+', '', texto) # remove URLs\n",
    "    texto = re.sub('<.*?>+', '', texto) # remove html tags\n",
    "    texto = re.sub('[%s]' % re.escape(string.punctuation), '', texto) # remove punctuation\n",
    "    texto = re.sub('\\n', '', texto) # remove words conatinaing numbers\n",
    "    texto = re.sub('\\w*\\d\\w*', '', texto)\n",
    "    texto = re.sub('[‘’“”…]', '', texto)\n",
    "    texto = quitar_stopwords(texto)\n",
    "    texto = quitar_menciones(texto)\n",
    "    texto = eliminar_palabras_de_una_sola_letra(texto)\n",
    "    return texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizar_texto(texto):\n",
    "    lista_palabras=nltk.word_tokenize(texto)\n",
    "    texto=' '.join([lemmatizer.lemmatize(word) for word in lista_palabras])\n",
    "    return texto\n",
    "#nota: la función esta definida aca, pero para ver los resultados que produce hay\n",
    "#que ir al comienzo del notebook y ejecutar la celda de lemmatizar texto luego\n",
    "#de haber limpiado el texto original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(palabra):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([palabra])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizar_con_pos(texto):\n",
    "    lista_palabras=texto.split()\n",
    "    texto=' '.join(lemmatizer.lemmatize(palabra,get_wordnet_pos(palabra)) for palabra in lista_palabras)\n",
    "    return texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature: Cantidad de caracteres usados en tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-21-fa59aa1b33af>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cuenta_caract['caracteres_usados'] = cuenta_caract['text'].str.len()\n",
      "<ipython-input-21-fa59aa1b33af>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cuenta_caract_t['caracteres_usados'] = cuenta_caract_t['text'].str.len()\n"
     ]
    }
   ],
   "source": [
    "cuenta_caract = train_set[['id','text']]\n",
    "cuenta_caract['caracteres_usados'] = cuenta_caract['text'].str.len()\n",
    "cuenta_caract_t = test_set[['id','text']]\n",
    "cuenta_caract_t['caracteres_usados'] = cuenta_caract_t['text'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuenta_caract = cuenta_caract[['id','caracteres_usados']]\n",
    "cuenta_caract_t = cuenta_caract_t[['id','caracteres_usados']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set=train_set.merge(cuenta_caract,on='id',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set=test_set.merge(cuenta_caract_t,on='id',how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature: cantidad de menciones por tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-25-17f2c8243a58>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cant_menciones['menciones_realizadas']=cant_menciones['text'].str.count('@')\n",
      "<ipython-input-25-17f2c8243a58>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cant_menciones_t['menciones_realizadas']=cant_menciones_t['text'].str.count('@')\n"
     ]
    }
   ],
   "source": [
    "cant_menciones = train_set[['id','text']]\n",
    "cant_menciones['menciones_realizadas']=cant_menciones['text'].str.count('@')\n",
    "cant_menciones_t = test_set[['id','text']]\n",
    "cant_menciones_t['menciones_realizadas']=cant_menciones_t['text'].str.count('@')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cant_menciones = cant_menciones[['id','menciones_realizadas']]\n",
    "cant_menciones_t=cant_menciones_t[['id','menciones_realizadas']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set=train_set.merge(cant_menciones,on='id',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set=test_set.merge(cant_menciones_t,on='id',how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature: ID que comparten localización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiene_localizacion = train_set[['id','location']]\n",
    "tiene_localizacion_t = test_set[['id','location']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Santi\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:671: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n",
      "<ipython-input-30-17f4be06ad34>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tiene_localizacion['location'].loc[~(tiene_localizacion['location'].isnull())]=1\n",
      "<ipython-input-30-17f4be06ad34>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tiene_localizacion_t['location'].loc[~(tiene_localizacion_t['location'].isnull())]=1\n"
     ]
    }
   ],
   "source": [
    "tiene_localizacion['location'].loc[~(tiene_localizacion['location'].isnull())]=1\n",
    "tiene_localizacion_t['location'].loc[~(tiene_localizacion_t['location'].isnull())]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Santi\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:4147: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().fillna(\n"
     ]
    }
   ],
   "source": [
    "tiene_localizacion.fillna(0,inplace=True)\n",
    "tiene_localizacion_t.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    5080\n",
       "0    2533\n",
       "Name: location, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tiene_localizacion['location'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiene_localizacion = tiene_localizacion.rename(columns={'location':'permite_location'})\n",
    "tiene_localizacion_t = tiene_localizacion_t.rename(columns={'location':'permite_location'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set=train_set.merge(tiene_localizacion,on='id',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set=test_set.merge(tiene_localizacion_t,on='id',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature: usa Keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-36-1791a4cc735b>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  usa_keyword['keyword'].loc[~(usa_keyword['keyword'].isnull())] = 1\n",
      "<ipython-input-36-1791a4cc735b>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  usa_keyword_t['keyword'].loc[~(usa_keyword_t['keyword'].isnull())] = 1\n"
     ]
    }
   ],
   "source": [
    "usa_keyword = train_set[['id','keyword']]\n",
    "usa_keyword['keyword'].loc[~(usa_keyword['keyword'].isnull())] = 1\n",
    "usa_keyword_t = test_set[['id','keyword']]\n",
    "usa_keyword_t['keyword'].loc[~(usa_keyword_t['keyword'].isnull())] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "usa_keyword.fillna(0,inplace=True)\n",
    "usa_keyword_t.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "usa_keyword = usa_keyword.rename(columns={'keyword':'use_keyword'})\n",
    "usa_keyword_t = usa_keyword_t.rename(columns={'keyword':'use_keyword'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = train_set.merge(usa_keyword,on='id',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set=test_set.merge(usa_keyword_t,on='id',how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature: Cita URL en Tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "usa_url = train_set[['id','text']]\n",
    "usa_url_t = test_set[['id','text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-42-e0ed60eeef45>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  usa_url['cita_url'] = usa_url['text'].str.count('http')\n",
      "<ipython-input-42-e0ed60eeef45>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  usa_url_t['cita_url'] = usa_url_t['text'].str.count('http')\n"
     ]
    }
   ],
   "source": [
    "usa_url['cita_url'] = usa_url['text'].str.count('http')\n",
    "usa_url_t['cita_url'] = usa_url_t['text'].str.count('http')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "usa_url = usa_url[['id','cita_url']]\n",
    "usa_url_t = usa_url_t[['id','cita_url']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = train_set.merge(usa_url,on='id',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set=test_set.merge(usa_url_t,on='id',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature: usa Hashtag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-46-eef0326b5282>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  usa_hashtag['use_hashtag']=usa_hashtag['text'].str.count('#')\n",
      "<ipython-input-46-eef0326b5282>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  usa_hashtag_t['use_hashtag']=usa_hashtag_t['text'].str.count('#')\n"
     ]
    }
   ],
   "source": [
    "usa_hashtag = train_set[['id','text']]\n",
    "usa_hashtag['use_hashtag']=usa_hashtag['text'].str.count('#')\n",
    "usa_hashtag_t = test_set[['id','text']]\n",
    "usa_hashtag_t['use_hashtag']=usa_hashtag_t['text'].str.count('#')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "usa_hashtag = usa_hashtag[['id','use_hashtag']]\n",
    "usa_hashtag_t = usa_hashtag_t[['id','use_hashtag']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = train_set.merge(usa_hashtag,on='id',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set=test_set.merge(usa_hashtag_t,on='id',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature: Cantidad de palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-50-9a582d2d4513>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cant_palabras['cant_palabras'] = cant_palabras['text'].str.count(' ') + 1\n",
      "<ipython-input-50-9a582d2d4513>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cant_palabras_t['cant_palabras'] = cant_palabras_t['text'].str.count(' ') + 1\n"
     ]
    }
   ],
   "source": [
    "cant_palabras = train_set[['id', 'text']]\n",
    "cant_palabras['cant_palabras'] = cant_palabras['text'].str.count(' ') + 1\n",
    "cant_palabras_t = test_set[['id', 'text']]\n",
    "cant_palabras_t['cant_palabras'] = cant_palabras_t['text'].str.count(' ') + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "cant_palabras = cant_palabras[['id', 'cant_palabras']]\n",
    "cant_palabras_t = cant_palabras_t[['id', 'cant_palabras']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = train_set.merge(cant_palabras,on='id',how='left')\n",
    "test_set = test_set.merge(cant_palabras_t,on='id',how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature: Cantidad de abreviaciones de internet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contar_abreviaciones(data):\n",
    "    lista_cant = []\n",
    "    for tweet in data:\n",
    "        cant = 0\n",
    "        tweet = quitar_stopwords(tweet)\n",
    "        tweet = tweet.upper()\n",
    "        tweet = tweet.split()\n",
    "        for word in tweet:\n",
    "            if (word in Diccionario_de_lenguaje_de_internet):\n",
    "                cant = cant + 1\n",
    "        lista_cant = lista_cant + [cant]\n",
    "    return lista_cant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-54-1cd6de0a6bdd>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cant_abreviaciones['cant_abreviaciones'] = contar_abreviaciones(cant_abreviaciones['text'])\n",
      "<ipython-input-54-1cd6de0a6bdd>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cant_abreviaciones_t['cant_abreviaciones'] = contar_abreviaciones(cant_abreviaciones_t['text'])\n"
     ]
    }
   ],
   "source": [
    "cant_abreviaciones = train_set[['id', 'text']]\n",
    "cant_abreviaciones['cant_abreviaciones'] = contar_abreviaciones(cant_abreviaciones['text'])\n",
    "cant_abreviaciones_t = test_set[['id', 'text']]\n",
    "cant_abreviaciones_t['cant_abreviaciones'] = contar_abreviaciones(cant_abreviaciones_t['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "cant_abreviaciones = cant_abreviaciones[['id', 'cant_abreviaciones']]\n",
    "cant_abreviaciones_t = cant_abreviaciones_t[['id', 'cant_abreviaciones']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = train_set.merge(cant_abreviaciones,on='id',how='left')\n",
    "test_set = test_set.merge(cant_abreviaciones_t,on='id',how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature: Location en Estados Unidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_estados_usa = [\"ALABAMA\", \"AL\", \"ALASKA\", \"AK\", \n",
    "                     \"ARIZONA\", \"AZ\", \"ARKANSAS\", \"AR\", \n",
    "                     \"CALIFORNIA\", \"CA\", \"COLORADO\", \"CO\", \n",
    "                     \"CONNECTICUT\", \"CT\", \"DELAWARE\", \"DE\", \n",
    "                     \"FLORIDA\", \"FL\", \"GEORGIA\", \"GA\", \n",
    "                     \"HAWAII\", \"HI\", \"IDAHO\", \"ID\", \n",
    "                     \"ILLINOIS\", \"IL\", \"INDIANA\", \"IN\", \n",
    "                     \"IOWA\", \"IA\", \"KANSAS\", \"KS\", \n",
    "                     \"KENTUCKY\", \"KY\", \"LOUISIANA\", \"LA\", \n",
    "                     \"MAINE\", \"ME\", \"MARYLAND\", \"MD\", \n",
    "                     \"MASSACHUSETTS\", \"MA\", \"MICHIGAN\", \"MI\", \n",
    "                     \"MINNESOTA\", \"MN\", \"MISSISSIPPI\", \"MS\", \n",
    "                     \"MISSOURI\", \"MO\", \"MONTANA\", \"MT\", \n",
    "                     \"NEBRASKA\", \"NE\", \"NEVADA\", \"NV\", \n",
    "                     \"NEW HAMPSHIRE\", \"NH\", \"NEW JERSEY\", \n",
    "                     \"NJ\", \"NEW MEXICO\", \"NM\", \"NEW YORK\", \n",
    "                     \"NY\", \"NORTH CAROLINA\", \"NC\", \"NORTH DAKOTA\", \n",
    "                     \"ND\", \"OHIO\", \"OH\", \"OKLAHOMA\", \"OK\", \n",
    "                     \"OREGON\", \"OR\", \"PENNSYLVANIA\", \"PA\", \n",
    "                     \"RHODE ISLAND\", \"RI\", \"SOUTH CAROLINA\", \"SC\", \"CAROLINA\", \n",
    "                     \"SOUTH DAKOTA\", \"SD\", \"TENNESSEE\", \"TN\", \n",
    "                     \"TEXAS\", \"TX\", \"UTAH\", \"UT\", \n",
    "                     \"VERMONT\", \"VT\", \"VIRGINIA\", \"VA\", \n",
    "                     \"WASHINGTON\", \"WA\", \"WEST VIRGINIA\", \"WV\", \n",
    "                     \"WISCONSIN\", \"WI\", \"WYOMING\", \"WY\",\n",
    "                     \"USA\", \"UNITED STATES\", \"SAN FRANCISCO\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ver_location_usa(data):\n",
    "    lista_ubicacion_usa = []\n",
    "    for location in data:\n",
    "        if (location is None):\n",
    "            lista_ubicacion_usa += [0]\n",
    "        else:\n",
    "            location_usa = False\n",
    "            location = str(location)\n",
    "            location = location.upper()\n",
    "            if (location in lista_estados_usa):\n",
    "                location_usa = True\n",
    "            location = location.split()\n",
    "            for word in location:\n",
    "                if (word in lista_estados_usa):\n",
    "                    location_usa = True\n",
    "                    break\n",
    "            if (location_usa):\n",
    "                lista_ubicacion_usa += [1]\n",
    "            else:\n",
    "                lista_ubicacion_usa += [0]\n",
    "    return lista_ubicacion_usa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-59-aa4731089d51>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  location_usa['location_usa'] = ver_location_usa(location_usa['location'])\n",
      "<ipython-input-59-aa4731089d51>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  location_usa_t['location_usa'] = ver_location_usa(location_usa_t['location'])\n"
     ]
    }
   ],
   "source": [
    "location_usa = train_set[['id', 'location']]\n",
    "location_usa['location_usa'] = ver_location_usa(location_usa['location'])\n",
    "location_usa_t = test_set[['id', 'location']]\n",
    "location_usa_t['location_usa'] = ver_location_usa(location_usa_t['location'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_usa = location_usa[['id', 'location_usa']]\n",
    "location_usa_t = location_usa_t[['id', 'location_usa']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = train_set.merge(location_usa,on='id',how='left')\n",
    "test_set = test_set.merge(location_usa_t,on='id',how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature: Tweet tiene caritas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_caritas = [\":)\", \";)\", \":(\", \";(\", \"XD\", \"xD\", \"xd\", \":P\", \":p\", \"-_-\", \":O\", \":'(\", \":D\", \":-D\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buscar_caritas(data):\n",
    "    tiene_carita = []\n",
    "    for tweet in data:\n",
    "        tweet = quitar_menciones(tweet)\n",
    "        tweet = quitar_links(tweet)\n",
    "        hay_carita = False\n",
    "        for carita in lista_caritas:\n",
    "            if (carita in tweet):\n",
    "                hay_carita = True\n",
    "                break\n",
    "        if (hay_carita):\n",
    "            tiene_carita += [1]\n",
    "        else:\n",
    "            tiene_carita += [0]\n",
    "    return tiene_carita"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'quitar_links' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-64-31913808ce48>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mhas_emoji\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mhas_emoji\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_emoji'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuscar_caritas\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhas_emoji\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mhas_emoji_t\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mhas_emoji_t\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_emoji'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuscar_caritas\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhas_emoji_t\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-63-7670ef92ab1c>\u001b[0m in \u001b[0;36mbuscar_caritas\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mtweet\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mtweet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mquitar_menciones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0mtweet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mquitar_links\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[0mhay_carita\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mcarita\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlista_caritas\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'quitar_links' is not defined"
     ]
    }
   ],
   "source": [
    "has_emoji = train_set[['id', 'text']]\n",
    "has_emoji['has_emoji'] = buscar_caritas(has_emoji['text'])\n",
    "has_emoji_t = test_set[['id', 'text']]\n",
    "has_emoji_t['has_emoji'] = buscar_caritas(has_emoji_t['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_emoji = has_emoji[['id', 'has_emoji']]\n",
    "has_emoji_t = has_emoji_t[['id', 'has_emoji']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = train_set.merge(has_emoji,on='id',how='left')\n",
    "test_set = test_set.merge(has_emoji_t,on='id',how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature: Palabras con misma letra repetida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_letras_seguidas = [\"aaa\", \"bbb\", \"ccc\", \"ddd\", \"eee\", \"fff\", \"ggg\", \"hhh\", \"iii\", \"jjj\", \"kkk\", \"lll\", \"mmm\", \"nnn\", \"ooo\", \"ppp\", \"qqq\", \"rrr\", \"sss\", \"ttt\", \"uuu\", \"vvv\", \"www\", \"xxx\", \"yyy\", \"zzz\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buscar_letras_seguidas(data):\n",
    "    tiene_letras_seguidas = []\n",
    "    for tweet in data:\n",
    "        tweet = quitar_menciones(tweet)\n",
    "        tweet = quitar_links(tweet)\n",
    "        hay_letra_seguida = False\n",
    "        for letra in lista_letras_seguidas:\n",
    "            if (letra in tweet):\n",
    "                hay_letra_seguida = True\n",
    "                break\n",
    "        if (hay_letra_seguida):\n",
    "            tiene_letras_seguidas += [1]\n",
    "        else:\n",
    "            tiene_letras_seguidas += [0]\n",
    "    return tiene_letras_seguidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_repeated_letter = train_set[['id', 'text']]\n",
    "has_repeated_letter['has_repeated_letter'] = buscar_letras_seguidas(has_repeated_letter['text'])\n",
    "has_repeated_letter_t = test_set[['id', 'text']]\n",
    "has_repeated_letter_t['has_repeated_letter'] = buscar_letras_seguidas(has_repeated_letter_t['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_repeated_letter = has_repeated_letter[['id', 'has_repeated_letter']]\n",
    "has_repeated_letter_t = has_repeated_letter_t[['id', 'has_repeated_letter']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = train_set.merge(has_repeated_letter,on='id',how='left')\n",
    "test_set = test_set.merge(has_repeated_letter_t,on='id',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.to_csv('train_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set.to_csv('test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature: cantidad de palabras únicas usadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set=pd.read_csv(\"train_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_cantidad_de_palabras_unicas=[]\n",
    "def contar_cantidad_de_palabras_unicas(texto):\n",
    "    diccionario_auxiliar={}\n",
    "    palabras=texto.split()\n",
    "    for palabra in palabras:\n",
    "        esta=diccionario_auxiliar.get(palabra,\"no esta\")\n",
    "        if(esta==\"no esta\"):\n",
    "            diccionario_auxiliar[palabra]=\"esta\"\n",
    "    lista_cantidad_de_palabras_unicas.append(len(diccionario_auxiliar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set['numero_de_palabras_unicas']=train_set['text'].apply"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Feature: Hace mencion a alguna ciudad o país"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ciudades_del_mundo=pd.read_csv('world-cities_csv.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ciudades_del_mundo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LISTA_CIUDADES=[]\n",
    "def generar_lista_ciudades(texto):\n",
    "    texto=str(texto)\n",
    "    texto=texto.lower()\n",
    "    lista_auxiliar=texto.split()\n",
    "    if len(lista_auxiliar)!=1:\n",
    "        texto=' '.join([word for word in lista_auxiliar])\n",
    "    global LISTA_CIUDADES\n",
    "    LISTA_CIUDADES.append(texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LISTA_CIUDADES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ciudades=ciudades_del_mundo['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ciudades=list(ciudades)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ciudades)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def menciona_ciudad(texto):\n",
    "    texto=texto.lower()\n",
    "    global ciudades\n",
    "    for ciudad in ciudades:\n",
    "        lista_auxiliar=ciudad.split()\n",
    "        if len(lista_auxiliar)!=1:\n",
    "            ciudad=' '.join([word for word in lista_auxiliar])\n",
    "        if ciudad.lower() in texto:\n",
    "            return ciudad\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ciudades_por_longitud(lista_ciudades):\n",
    "    diccionario={}\n",
    "    for ciudad in lista_ciudades:\n",
    "        longitud=len(ciudad)\n",
    "        contador=diccionario.get(longitud,-1)\n",
    "        if contador==-1:\n",
    "            diccionario[longitud]=1\n",
    "        else:\n",
    "            diccionario[longitud]=contador+1\n",
    "    for key in diccionario.keys():\n",
    "        print(\"llave: \" + str(key) + \"   \"+ str(diccionario[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ciudades_por_longitud(ciudades)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set=pd.read_csv('train_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set['menciona_ciudad']=train_set['text'].apply(menciona_ciudad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.loc[train_set['menciona_ciudad']!=None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicc_paises={}\n",
    "def generar_lista_paises(texto):\n",
    "    global dicc_paises\n",
    "    texto=texto.lower()\n",
    "    esta=dicc_paises.get(texto,-1)\n",
    "    if esta==-1:\n",
    "        dicc_paises[texto]=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_paises=dicc_paises.keys()\n",
    "def menciona_pais(texto):\n",
    "    texto=texto.lower()\n",
    "    for pais in lista_paises:\n",
    "        if pais in texto:\n",
    "            return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ciudades_del_mundo['country'].apply(generar_lista_paises)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set['menciona_pais']=train_set['text'].apply(menciona_pais)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Las siguientes features usarlas sólo despues de limpiar el texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set['text']=train_set['text'].apply(limpiar_texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set['text']=train_set['text'].apply(lemmatizar_con_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set['text']=train_set['text'].apply(limpiar_texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature: cantidad de palabras en el tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contar_cantidad_de_palabras(texto):\n",
    "    contador=0\n",
    "    texto=texto.lower()\n",
    "    lista_palabras=texto.split()\n",
    "    for palabra in lista_palabras:\n",
    "        contador+=1\n",
    "    return contador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set['numero_de_palabras']=train_set['text'].apply(contar_cantidad_de_palabras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature: cantidad de palabras únicas utilizadas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nota: aplicar la función solamente una vez que el texto fue limpiado\n",
    "def contar_cantidad_de_palabras_unicas(texto):\n",
    "    diccionario_auxiliar={}\n",
    "    palabras=texto.split()\n",
    "    for palabra in palabras:\n",
    "        esta=diccionario_auxiliar.get(palabra,\"no esta\")\n",
    "        if(esta==\"no esta\"):\n",
    "            diccionario_auxiliar[palabra]=\"esta\"\n",
    "    return(len(diccionario_auxiliar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set['palabras_unicas']=train_set['text'].apply(contar_cantidad_de_palabras_unicas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature: cantidad de palabras no reconocidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_glove='/home/tomas/organizacion_de_datos/tp2/glove.6B.100d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diccionario_de_palabras={}\n",
    "def generar_diccionario_de_palabras():\n",
    "    embeddings=open(path_glove,'r',encoding='UTF-8')\n",
    "    while(True):\n",
    "        f=embeddings.readline()\n",
    "        if(len(f)==0):\n",
    "            break\n",
    "        f=f.split()\n",
    "        lista=f[0].split()\n",
    "        diccionario_de_palabras[lista[0]]='y'\n",
    "    embeddings.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generar_diccionario_de_palabras()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contar_palabras_desconocidas(texto):\n",
    "    global diccionario_de_palabras\n",
    "    contador=0\n",
    "    texto=texto.lower()\n",
    "    lista_palabras=texto.split()\n",
    "    for palabra in lista_palabras:\n",
    "        esta=diccionario_de_palabras.get(palabra,'no esta')\n",
    "        if esta=='no esta':\n",
    "            contador+=1\n",
    "    return contador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set['numero_palabras_desconocidas']=train_set['text'].apply(contar_palabras_desconocidas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set=pd.read_csv('test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set['menciona_pais']=test_set['text'].apply(menciona_pais)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set['text']=test_set['text'].apply(limpiar_texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set['text']=test_set['text'].apply(lemmatizar_con_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set['text']=test_set['text'].apply(limpiar_texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set['numero_de_palabras']=test_set['text'].apply(contar_cantidad_de_palabras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set['palabras_unicas']=test_set['text'].apply(contar_cantidad_de_palabras_unicas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set['numero_palabras_desconocidas']=test_set['text'].apply(contar_palabras_desconocidas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertir(texto):\n",
    "    if texto==\"Si\":\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set['menciona_pais']=test_set['menciona_pais'].apply(convertir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
