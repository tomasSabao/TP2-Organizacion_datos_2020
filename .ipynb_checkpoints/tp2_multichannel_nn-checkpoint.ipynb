{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Red neuronal profunda usando word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tomas/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/tomas/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/tomas/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/tomas/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/tomas/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/tomas/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/tomas/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/tomas/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/tomas/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/tomas/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/tomas/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/tomas/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import sklearn as sk\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from time import time\n",
    "from collections import defaultdict\n",
    "from tensorflow import keras\n",
    "\n",
    "path_glove='/home/tomas/organizacion_de_datos/tp2/glove.6B.100d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Embedding,LSTM,GRU\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/tomas/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/tomas/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/tomas/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_stopwords=set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set=pd.read_csv(\"train_set_featurizado.csv\",encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def a_string(texto):\n",
    "    return str(texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set=pd.read_csv(\"test_data_featurizada.csv\",encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set['text']=train_set['text'].apply(a_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set['text']=test_set['text'].apply(a_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Procesamiento del texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quitar_stopwords(texto):\n",
    "    texto=' '.join([word for word in texto.split() if word not in english_stopwords])\n",
    "    return texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quitar_menciones(texto):\n",
    "    texto=' '.join([palabra for palabra in texto.split() if '@' not in palabra])\n",
    "    return texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Diccionario_de_lenguaje_de_internet={\n",
    "\"2day\": \"Today\",\n",
    "\"2moro\": \"Tomorrow\",\n",
    "\"2nite\": \"Tonight\",\n",
    "\"4EAE\": \"For Ever And Ever\",\n",
    "\"ABT\":\"About\",\n",
    "\"ADN\": \"Any Day Now\",\n",
    "\"AFAIC\": \"As Far As I’m Concerned\",\n",
    "\"AFAICT\": \"As Far As I Can Tell\",\n",
    "\"AFAIK\": \"As Far As I Know\",\n",
    "\"AFAIR\": \"As Far As I Remember\",\n",
    "\"AKA\": \"Also Known As\",\n",
    "\"AMA\": \"Ask Me Anything\",\n",
    "\"ASAIC\": \"As Soon As I Can\",\n",
    "\"ASAP\": \"As Soon As Possible\",\n",
    "\"ATM\": \"At The Moment\",\n",
    "\"B4\": \"Before\",\n",
    "\"B4N\": \"Bye For Now\",\n",
    "\"Bae\": \"Babe/Before Anyone Else\",\n",
    "\"BBL\": \"Be Back Later\",\n",
    "\"BBT\": \"Be Back Tomorrow\",\n",
    "\"BCNU\": \"Be Seeing You\",\n",
    "\"BD\": \"Big Deal\",\n",
    "\"BF\": \"Boyfriend\",\n",
    "\"BFF\": \"Best Friends Forever\",\n",
    "\"BMT\": \"Before My Time\",\n",
    "\"BOL\": \"Be On Later\",\n",
    "\"BOT\": \"Back On Topic\",\n",
    "\"BRB\": \"Be Right Back\",\n",
    "\"BRO\": \"Brother\",\n",
    "\"BT\": \"But\",\n",
    "\"BTW\": \"By The Way\",\n",
    "\"CFY\": \"Calling For You\",\n",
    "\"CU\": \"See You\",\n",
    "\"CUL\": \"See You Later\",\n",
    "\"Cuz\": \"Because\",\n",
    "\"CYA\": \"Cover Your Ass\",\n",
    "\"DAE\": \"Does Anyone Else\",\n",
    "\"DBA\": \"Doing Business As\",\n",
    "\"DFTBA\": \"Don’t Forget To Be Awesome\",\n",
    "\"DIKU\": \"Do I Know You\",\n",
    "\"DM\": \"Direct Message\",\n",
    "\"DND\": \"Do Not Disturb\",\n",
    "\"DR\": \"Double Rainbow\",\n",
    "\"DWBH\": \"Don’t Worry, Be Happy\",\n",
    "\"ELI5\": \"Explain Like I’m 5\",\n",
    "\"EOM\": \"End Of Message\",\n",
    "\"EOS\": \"End Of Story\",\n",
    "\"F2F\": \"Face To Face\",\n",
    "\"FAQ\": \"Frequently Asked Question\",\n",
    "\"FB\": \"Facebook\",\n",
    "\"FBF\": \"Flash Back Friday\",\n",
    "\"FF\": \"Follow Friday\",\n",
    "\"FIFY\": \"Fixed It For You\",\n",
    "\"FITB\": \"Fill In The Blank\",\n",
    "\"FML\": \"Fuck My Life\",\n",
    "\"FOMO\": \"Fear Of Missing Out\",\n",
    "\"FTFY\": \"Fixed That For You\",\n",
    "\"FTL\": \"For The Loss\",\n",
    "\"FTW\": \"For The Win\",\n",
    "\"FWB\": \"Friends With Benefits\",\n",
    "\"FWIW\": \"For What It’s Worth\",\n",
    "\"FYE\": \"For Your Entertainment\",\n",
    "\"FYEO\": \"For Your Eyes Only\",\n",
    "\"FYI\": \"For Your Information\",\n",
    "\"GA\": \"Go Ahead\",\n",
    "\"GAL\": \"Get A Life\",\n",
    "\"GF\": \"Girlfriend\",\n",
    "\"GM\": \"Good Morning\",\n",
    "\"GN\": \"Good Night\",\n",
    "\"Gr8\": \"Great\",\n",
    "\"GTR\": \"Getting Ready\",\n",
    "\"HAND\": \"Have A Nice Day\",\n",
    "\"HB\": \"Hurry Back\",\n",
    "\"HBD\": \"Happy Birthday\",\n",
    "\"HBU\": \"How About You\",\n",
    "\"HMB\": \"Hit Be Back\",\n",
    "\"HMU\": \"Hit Me Up\",\n",
    "\"HRU\": \"How Are You\",\n",
    "\"HTH\": \"Hope This Helps\",\n",
    "\"IAC\": \"In Any Case\",\n",
    "\"IC\": \"I See\",\n",
    "\"ICYMI\": \"In Case You Missed It\",\n",
    "\"IDC\": \"I Don’t Care\",\n",
    "\"IDK\": \"I Don’t Know\",\n",
    "\"IG\": \"Instagram\",\n",
    "\"IIRC\": \"If I Remember Correctly\",\n",
    "\"IKR\": \"I Know Right\",\n",
    "\"ILY\": \"I Love You\",\n",
    "\"IMHO\": \"In My Humble Opinion\",\n",
    "\"IMMD\": \"It Made My Day\",\n",
    "\"IMY\": \"I Miss You\",\n",
    "\"IRL\": \"In Real Life\",\n",
    "\"IS\": \"I’m Sorry\",\n",
    "\"ISO\": \"In Search Of\",\n",
    "\"IU2U\": \"It’s Up To You\",\n",
    "\"J4F\": \"Just For Fun\",\n",
    "\"JAM\": \"Just A Minute\",\n",
    "\"JFY\": \"Just For You\",\n",
    "\"JIC\": \"Just In Case\",\n",
    "\"JK\": \"Just Kidding\",\n",
    "\"JSYK\": \"Just So You Know\",\n",
    "\"KK\": \"Okay\",\n",
    "\"L8\": \"Late\",\n",
    "\"L8R\": \"Later\",\n",
    "\"LMA\": \"Leave Me Alone\",\n",
    "\"LMAO\": \"Laughing My Ass Off\",\n",
    "\"LMBO\": \"Laughing My Butt Off\",\n",
    "\"LMK\": \"Let Me Know\",\n",
    "\"LOL\": \"Laugh Out Loud\",\n",
    "\"LTNS\": \"Long Time No See\",\n",
    "\"LYLAS\": \"Love You Like A Sister\",\n",
    "\"M/F\": \"Male or Female\",\n",
    "\"M8\": \"Mate\",\n",
    "\"MP\": \"My pleasure\",\n",
    "\"MSM\": \"Mainstream Media\",\n",
    "\"MU\": \"Miss You\",\n",
    "\"MYOB\": \"Mind Your Own Business\",\n",
    "\"NAGI\": \"Not A Good Idea\",\n",
    "\"NBD\": \"No Big Deal\",\n",
    "\"NE1\": \"Anyone\",\n",
    "\"NM\": \"Not Much\",\n",
    "\"NP\": \"No Problem\",\n",
    "\"NSFL\": \"Not Safe For Life\",\n",
    "\"NSFW\": \"Not Safe For Work\",\n",
    "\"NTS\": \"Note To Self\",\n",
    "\"NVM\": \"Never Mind\",\n",
    "\"OC\": \"Original Content\",\n",
    "\"OH\": \"Overheard\",\n",
    "\"OIC\": \"Oh ! I See\",\n",
    "\"OMD\": \"Oh My Damn\",\n",
    "\"OMG\": \"Oh My Goodness\",\n",
    "\"OMW\": \"On My Way\",\n",
    "\"OT\": \"Off Topic\",\n",
    "\"OFC\": \"Of course\",\n",
    "\"PAW\": \"Parents Are Watching\",\n",
    "\"Pls\": \"Please\",\n",
    "\"POTD\": \"Photo Of The Day\",\n",
    "\"POV\": \"Point Of View\",\n",
    "\"PPL\": \"People\",\n",
    "\"PTB\": \"Please Text Back\",\n",
    "\"Q4U\": \"Question For You\",\n",
    "\"QQ\": \"Crying\",\n",
    "\"RBTL\": \"Read Between The Lines\",\n",
    "\"RIP\": \"Rest In Peace\",\n",
    "\"RL\": \"Real Life\",\n",
    "\"ROFL\": \"Rolling On the Floor Laughing\",\n",
    "\"RT\": \"Retweet\",\n",
    "\"RTM\": \"Read The Manual\",\n",
    "\"SIS\": \"Sister\",\n",
    "\"SITD\": \"Still In The Dark\",\n",
    "\"SM\": \"Social Media\",\n",
    "\"SMH\": \"Shaking My Head\",\n",
    "\"SMY\": \"Somebody\",\n",
    "\"SNH\": \"Sarcasm Noted Here\",\n",
    "\"SOL\": \"Sooner Or Later\",\n",
    "\"Some1\": \"Someone\",\n",
    "\"SRSLY\": \"Seriously\",\n",
    "\"STBY\": \"Sucks To Be You\",\n",
    "\"Str8\": \"Straight\",\n",
    "\"SYS\": \"See You Soon\",\n",
    "\"TBA\": \"To Be Announced\",\n",
    "\"TBH\": \"To Be Honest\",\n",
    "\"TBT\": \"Throwback Thursday\",\n",
    "\"TBT\": \"Truth Be Told\",\n",
    "\"TFH\": \"Thread From Hell\",\n",
    "\"TFTI\": \"Thanks For The Invite\",\n",
    "\"TGIF\": \"Thank God It’s Friday\",\n",
    "\"THX\": \"Thanks\",\n",
    "\"TIA\": \"Thanks in Advance\",\n",
    "\"TIL\": \"Today I Learned\",\n",
    "\"TIME\": \"Tears In My Eyes\",\n",
    "\"TL;DR\": \"Too Long; Didn’t Read\",\n",
    "\"TLDR\":\"Too long didn’t read\",\n",
    "\"TL DR\":\"Too long didn’t read\",\n",
    "\"TLC\": \"Tender Loving Care\",\n",
    "\"TMI\": \"Too Much Information\",\n",
    "\"TTYL\": \"Talk To You Later\",\n",
    "\"TTYS\": \"Talk To You Soon\",\n",
    "\"Txt\": \"Text\",\n",
    "\"TYVM\": \"Thank You Very Much\",\n",
    "\"U\": \"You\",\n",
    "\"U4F\": \"You Forever\",\n",
    "\"UR\": \"Your\",\n",
    "\"VBG\": \"Very Big Grin\",\n",
    "\"VSF\": \"Very Sad Face\",\n",
    "\"WB\": \"Welcome Back\",\n",
    "\"WBU\": \"What About You?\",\n",
    "\"WEG\": \"Wicked Evil Grin\",\n",
    "\"WKND\": \"Weekend\",\n",
    "\"WOM\": \"Word of Mouth\",\n",
    "\"WOTD\": \"Word Of The Day\",\n",
    "\"Wru\": \"Who Are You\",\n",
    "\"WTH\": \"What The Heck?\",\n",
    "\"WTPA\": \"Where The Party At?\",\n",
    "\"WU?\": \"What's Up\",\n",
    "\"WU\":\"What's Up\",\n",
    "\"WYCM\": \"Will You Call Me?\",\n",
    "\"WYWH\": \"Wish You Were Here\",\n",
    "\"XOXO\": \"Hugs and Kisses\",\n",
    "\"YGM\": \"You’ve Got Mail\",\n",
    "\"YNK\": \"You Never Know\",\n",
    "\"YOLO\": \"You Only Live Once\",\n",
    "\"YT\": \"YouTube\",\n",
    "\"YW\": \"You’re Welcome\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reemplazar_lenguaje_internet(texto):\n",
    "    texto=texto.upper()\n",
    "    palabras=texto.split()\n",
    "    palabras_procesadas=[]\n",
    "    for palabra in palabras:\n",
    "        traduccion=Diccionario_de_lenguaje_de_internet.get(palabra,'not internet slang')\n",
    "        if(traduccion!='not internet slang'):\n",
    "            lista_aux=traduccion.split()\n",
    "            for x in lista_aux:\n",
    "                palabras_procesadas.append(x.lower())\n",
    "        else:\n",
    "            palabras_procesadas.append(palabra.lower())\n",
    "    texto=' '.join([word for word in palabras_procesadas])\n",
    "    return texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "diccionario_contracciones = {  \"ain't\": \"is not\",\n",
    "                               \"aint\":\"is not\",\n",
    "                               \"aren't\": \"are not\",\n",
    "                               \"arent\":\"are not\",\n",
    "                               \"can't\": \"can not\",\n",
    "                               \"cant\":\"can not\",\n",
    "                               \"cause\": \"because\",\n",
    "                               \"could've\": \"could have\",\n",
    "                               \"couldve\":\"could have\",\n",
    "                               \"couldn't\": \"could not\",\n",
    "                               \"couldnt\":\"could not\",\n",
    "                               \"didn't\": \"did not\", \n",
    "                               \"didnt\":\"did not\",\n",
    "                               \"doesn't\": \"does not\",\n",
    "                               \"doesnt\":\"does not\",\n",
    "                               \"don't\": \"do not\",\n",
    "                               \"dont\":\"do not\",\n",
    "                               \"hadn't\": \"had not\",\n",
    "                               \"hadnt\":\"had not\",\n",
    "                               \"hasn't\": \"has not\",\n",
    "                               \"hasnt\":\"has not\",\n",
    "                               \"haven't\": \"have not\",\n",
    "                               \"havent\":\"have not\",\n",
    "                               \"he'd\": \"he would\",\n",
    "                               \"hed\":\"he would\",\n",
    "                               \"he'll\": \"he will\",\n",
    "                               \"he ll\":\"he will\",\n",
    "                               \"he's\": \"he is\",\n",
    "                               \"hes\":\"he is\",\n",
    "                               \"how'd\": \"how did\",\n",
    "                               \"howd\":\"how did\",\n",
    "                               \"how'd'y\": \"how do you\",\n",
    "                               \"howdy\":\"how do you\",\n",
    "                               \"how'll\": \"how will\",\n",
    "                               \"howll\":\"how will\",\n",
    "                               \"how's\": \"how is\",\n",
    "                               \"hows\":\"how is\",\n",
    "                               \"I'd\": \"I would\",\n",
    "                               \"id\":\"i would\",\n",
    "                               \"I'd've\": \"I would have\",\n",
    "                               \"idve\":\"i would have\",\n",
    "                               \"I'll\": \"I will\", \n",
    "                               \"Ill\":\"i will\", #duda: si algun tweet habla de un enfermo puede traer ruido\n",
    "                               \"I'll've\": \"I will have\",\n",
    "                               \"I'm\": \"I am\", \n",
    "                               \"im\":\"i am\",\n",
    "                               \"I've\": \"I have\",\n",
    "                               \"ive\":\"i have\",\n",
    "                               \"i'd\": \"i would\",\n",
    "                               \"i'd've\": \"i would have\",\n",
    "                               \"i'll\": \"i will\",\n",
    "                               \"i'll've\": \"i will have\",\n",
    "                               \"i'm\": \"i am\", \n",
    "                               \"im\":\"i am\",\n",
    "                               \"i've\": \"i have\",\n",
    "                               \"isn't\": \"is not\",\n",
    "                               \"isnt\":\"is not\",\n",
    "                               \"it'd\": \"it would\",\n",
    "                               \"itd\":\"it would\",\n",
    "                               \"it'd've\": \"it would have\",\n",
    "                               \"it'll\": \"it will\",\n",
    "                               \"itll\":\"it will\",\n",
    "                               \"it'll've\": \"it will have\",\n",
    "                               \"it's\": \"it is\",\n",
    "                               \"its\":\"it is\",\n",
    "                               \"let's\": \"let us\",\n",
    "                               \"lets\":\"let us\",\n",
    "                               \"ma'am\": \"madam\",\n",
    "                               \"maam\":\"madam\",\n",
    "                               \"mayn't\": \"may not\",\n",
    "                               \"maynt\":\"may not\",\n",
    "                               \"might've\": \"might have\",\n",
    "                               \"mightve\":\"might have\",\n",
    "                               \"mightn't\": \"might not\",\n",
    "                               \"mightnt\":\"might not\",\n",
    "                               \"mightn't've\": \"might not have\",\n",
    "                               \"must've\": \"must have\",\n",
    "                               \"mustve\":\"must have\",\n",
    "                               \"mustn't\": \"must not\",\n",
    "                               \"mustnt\":\"must not\",\n",
    "                               \"mustn't've\": \"must not have\",\n",
    "                               \"needn't\": \"need not\",\n",
    "                               \"neednt\":\"need not\",\n",
    "                               \"needn't've\": \"need not have\",\n",
    "                               \"o'clock\": \"of the clock\",\n",
    "                               \"oclock\":\"of the clock\",\n",
    "                               \"oughtn't\": \"ought not\",\n",
    "                               \"oughtnt\":\"ought not\",\n",
    "                               \"oughtn't've\": \"ought not have\",\n",
    "                               \"shan't\": \"shall not\",\n",
    "                               \"shant\":\"shall not\",\n",
    "                               \"sha'n't\": \"shall not\",\n",
    "                               \"shan't've\": \"shall not have\",\n",
    "                               \"she'd\": \"she would\",\n",
    "                               \"shed\":\"she would\",\n",
    "                               \"she'd've\": \"she would have\",\n",
    "                               \"she'll\": \"she will\",\n",
    "                               \"shell\":\"she will\",#nuevamente aca tengo ruido por dos palabras igual escritas\n",
    "                               \"she'll've\": \"she will have\", \n",
    "                               \"she's\": \"she is\",\n",
    "                               \"shes\":\"she is\",\n",
    "                               \"should've\": \"should have\",\n",
    "                               \"shouldve\":\"should have\",\n",
    "                               \"shouldn't\": \"should not\",\n",
    "                               \"shouldnt\": \"should not\",\n",
    "                               \"shouldn't've\": \"should not have\",\n",
    "                               \"so've\": \"so have\",\n",
    "                               \"so's\": \"so as\",\n",
    "                               \"this's\": \"this is\",\n",
    "                               \"that'd\": \"that would\",\n",
    "                               \"that'd've\": \"that would have\",\n",
    "                               \"that's\": \"that is\",\n",
    "                               \"thats\":\"that is\",\n",
    "                               \"there'd\": \"there would\",\n",
    "                               \"thered\":\"there would\",\n",
    "                               \"there'd've\": \"there would have\",\n",
    "                               \"there's\": \"there is\",\n",
    "                               \"theres\":\"there is\",\n",
    "                               \"here's\": \"here is\",\n",
    "                               \"heres\":\"here is\",\n",
    "                               \"they'd\": \"they would\",\n",
    "                               \"theyd\":\"they would\",\n",
    "                               \"they'd've\": \"they would have\",\n",
    "                               \"they'll\": \"they will\",\n",
    "                               \"theyll\":\"they will\",\n",
    "                               \"they'll've\": \"they will have\", \n",
    "                               \"they're\": \"they are\",\n",
    "                               \"theyre\":\"they are\",\n",
    "                               \"they've\": \"they have\", \n",
    "                               \"theyve\":\"they have\",\n",
    "                               \"to've\": \"to have\", \n",
    "                               \"tove\":\"to have\",\n",
    "                               \"wasn't\": \"was not\",\n",
    "                               \"wasnt\":\"was not\",\n",
    "                               \"we'd\": \"we would\",\n",
    "                               \"wed\":\"we would\",#aca nuevamente hay conflicto\n",
    "                               \"we'd've\": \"we would have\",\n",
    "                               \"we'll\": \"we will\",\n",
    "                               \"we'll've\": \"we will have\",\n",
    "                               \"we're\": \"we are\",\n",
    "                               \"we've\": \"we have\",\n",
    "                               \"weren't\": \"were not\",\n",
    "                               \"werent\":\"were not\",\n",
    "                               \"what'll\": \"what will\",\n",
    "                               \"whatll\":\"what will\",\n",
    "                               \"what'll've\": \"what will have\",\n",
    "                               \"what're\": \"what are\", \n",
    "                               \"whatre\":\"what are\",\n",
    "                               \"what's\": \"what is\",\n",
    "                               \"whats\":\"what is\",\n",
    "                               \"what've\": \"what have\",\n",
    "                               \"whatve\":\"what have\",\n",
    "                               \"when's\": \"when is\",\n",
    "                               \"whens\":\"when is\",\n",
    "                               \"when've\": \"when have\",\n",
    "                               \"whenve\":\"when have\",\n",
    "                               \"where'd\": \"where did\",\n",
    "                               \"whered\":\"where did\",\n",
    "                               \"where's\": \"where is\",\n",
    "                               \"wheres\":\"where is\",\n",
    "                               \"where've\": \"where have\",\n",
    "                               \"whereve\":\"where have\",\n",
    "                               \"who'll\": \"who will\", \n",
    "                               \"wholl\":\"who will\",\n",
    "                               \"who'll've\": \"who will have\",\n",
    "                               \"who's\": \"who is\",\n",
    "                               \"whos\":\"who is\",\n",
    "                               \"who've\": \"who have\",\n",
    "                               \"whove\":\"who have\",\n",
    "                               \"why's\": \"why is\",\n",
    "                               \"whys\":\"why is\",\n",
    "                               \"why've\": \"why have\",\n",
    "                               \"whyve\":\"why have\",\n",
    "                               \"will've\": \"will have\",\n",
    "                               \"willve\":\"will have\",\n",
    "                               \"won't\": \"will not\",\n",
    "                               \"wont\":\"will not\",\n",
    "                               \"won't've\": \"will not have\",\n",
    "                               \"would've\": \"would have\",\n",
    "                               \"wouldve\":\"would have\",\n",
    "                               \"wouldn't\": \"would not\",\n",
    "                               \"wouldnt\":\"would not\",\n",
    "                               \"wouldn't've\": \"would not have\",\n",
    "                               \"y'all\": \"you all\",\n",
    "                               \"yall\":\"you all\",\n",
    "                               \"y'all'd\": \"you all would\",\n",
    "                               \"yalld\":\"you all would\",\n",
    "                               \"y'all'd've\": \"you all would have\",\n",
    "                               \"y'all're\": \"you all are\",\n",
    "                               \"yallre\":\"you all are\",\n",
    "                               \"y'all've\": \"you all have\",\n",
    "                               \"you'd\": \"you would\",\n",
    "                               \"youd\":\"you would\",\n",
    "                               \"you'd've\": \"you would have\",\n",
    "                               \"you'll\": \"you will\",\n",
    "                               \"youll\":\"you will\",\n",
    "                               \"you'll've\": \"you will have\",\n",
    "                               \"you're\": \"you are\",\n",
    "                               \"youre\":\"you are\",\n",
    "                               \"you've\": \"you have\",\n",
    "                               \"youve\":\"you have\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reemplazar_contracciones(texto):\n",
    "    texto=texto.lower()\n",
    "    palabras=texto.split()\n",
    "    palabras_procesadas=[]\n",
    "    for palabra in palabras:\n",
    "        traduccion=diccionario_contracciones.get(palabra,'no es contraccion')\n",
    "        if(traduccion!='no es contraccion'):\n",
    "            lista_aux=traduccion.split()\n",
    "            for x in lista_aux:\n",
    "                palabras_procesadas.append(x.lower())\n",
    "        else:\n",
    "            palabras_procesadas.append(palabra.lower())\n",
    "    texto=' '.join([word for word in palabras_procesadas])\n",
    "    return texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#es necesario importar la libreria re (regular expression) para poder usar esta funcion\n",
    "#ya esta bajada en el comienzo del notebook pero si lo que quieren es analizar la documentación\n",
    "#lo pueden revisar en google\n",
    "#falta borrar los arroba\n",
    "def limpiar_texto(texto):\n",
    "    texto = reemplazar_lenguaje_internet(texto)\n",
    "    texto=reemplazar_contracciones(texto)\n",
    "    texto=texto.lower()\n",
    "    texto = re.sub('\\[.*?\\]', '', texto) # remove text in square brackets\n",
    "    texto = re.sub('https?://\\S+|www\\.\\S+', '', texto) # remove URLs\n",
    "    texto = re.sub('<.*?>+', '', texto) # remove html tags\n",
    "    texto = re.sub('[%s]' % re.escape(string.punctuation), '', texto) # remove punctuation\n",
    "    texto = re.sub('\\n', '', texto) # remove words conatinaing numbers\n",
    "    texto = re.sub('\\w*\\d\\w*', '', texto)\n",
    "    texto = re.sub('[‘’“”…]', '', texto)\n",
    "    texto = quitar_stopwords(texto)\n",
    "    texto = quitar_menciones(texto)\n",
    "\n",
    "    return texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set['text']=train_set['text'].apply(limpiar_texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizar_texto(texto):\n",
    "    lista_palabras=nltk.word_tokenize(texto)\n",
    "    texto=' '.join([lemmatizer.lemmatize(word) for word in lista_palabras])\n",
    "    return texto\n",
    "#nota: la función esta definida aca, pero para ver los resultados que produce hay\n",
    "#que ir al comienzo del notebook y ejecutar la celda de lemmatizar texto luego\n",
    "#de haber limpiado el texto original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(palabra):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([palabra])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizar_con_pos(texto):\n",
    "    lista_palabras=texto.split()\n",
    "    texto=' '.join(lemmatizer.lemmatize(palabra,get_wordnet_pos(palabra)) for palabra in lista_palabras)\n",
    "    return texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set['text']=train_set['text'].apply(lemmatizar_con_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenización del texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Embedding,LSTM,GRU\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_copy=train_set.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y=train_set['target'].copy().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr,X_te,Y_tr,Y_te=train_test_split(X_copy,Y,test_size=0.1,random_state=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape de X_train: (6851,)\n",
      "Shape de X_test:(3263,)\n",
      "Shape de todos_los_textos:(10114,)\n",
      "longitud maxima: 24\n",
      "tamanio de vocabulario: 17167\n"
     ]
    }
   ],
   "source": [
    "Y_para_entrenar=Y_tr\n",
    "X_para_entrenar=X_tr['text'].values\n",
    "\n",
    "X_para_evaluar=X_te['text'].values\n",
    "\n",
    "#se hace un análisis del target usando NLP\n",
    "X_train=X_para_entrenar\n",
    "Y_train=Y_para_entrenar\n",
    "X_test=test_set['text'].copy().values\n",
    "\n",
    "print(\"Shape de X_train: \"+ str(X_train.shape))\n",
    "print(\"Shape de X_test:\" + str(X_test.shape))\n",
    "todos_los_textos=np.concatenate([X_train,X_test])\n",
    "print(\"Shape de todos_los_textos:\" + str(todos_los_textos.shape))\n",
    "\n",
    "#estan todos los textos concatenados, hay que 'entrenar' al tokenizador\n",
    "objeto_tokenizador=Tokenizer()\n",
    "objeto_tokenizador.fit_on_texts(todos_los_textos)\n",
    "\n",
    "#necesito una cota de la longitud de cada palabra de los textos que \n",
    "#se van a analizar\n",
    "longitud_maxima=max([len(s.split()) for s in todos_los_textos])\n",
    "print(\"longitud maxima: \" + str(longitud_maxima))\n",
    "\n",
    "#necesito saber cuantas palabras tengo en mi 'diccionario' de palabras\n",
    "tamanio_de_vocabulario=len(objeto_tokenizador.word_index) +1\n",
    "print(\"tamanio de vocabulario: \" + str(tamanio_de_vocabulario))\n",
    "\n",
    "#ahora que tengo esto, es tiempo de tokenizar cada uno de los tweets\n",
    "#y agregar el padding necesario\n",
    "X_train_tokens=objeto_tokenizador.texts_to_sequences(X_train)\n",
    "X_train_pad=pad_sequences(X_train_tokens,maxlen=longitud_maxima,padding='post')\n",
    "\n",
    "X_test_tokens=objeto_tokenizador.texts_to_sequences(X_test)\n",
    "X_test_pad=pad_sequences(X_test_tokens,maxlen=longitud_maxima,padding='post')\n",
    "\n",
    "X_para_evaluar=pad_sequences(objeto_tokenizador.texts_to_sequences(X_para_evaluar),maxlen=longitud_maxima,padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_matrix(filepath, word_index, embedding_dim):\n",
    "    vocab_size = len(word_index) + 1  # Adding again 1 because of reserved 0 index\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "    with open(filepath) as f:\n",
    "        for line in f:\n",
    "            word, *vector = line.split()\n",
    "            if word in word_index:\n",
    "                idx = word_index[word] \n",
    "                embedding_matrix[idx] = np.array(\n",
    "                    vector, dtype=np.float32)[:embedding_dim]\n",
    "\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6375021844236034\n"
     ]
    }
   ],
   "source": [
    "dimension_embedding=100\n",
    "embedding_matrix = create_embedding_matrix(path_glove,objeto_tokenizador.word_index, dimension_embedding)\n",
    "\n",
    "\n",
    "nonzero_elements = np.count_nonzero(np.count_nonzero(embedding_matrix, axis=1))\n",
    "print(nonzero_elements/tamanio_de_vocabulario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras import Model\n",
    "def crear_modelo(num_filtros,tamanio_kernel1,tamanio_kernel2,tamanio_kernel3,tamanio_vocab,dimension_embedding,max_len):\n",
    "    \n",
    "    #red neuronal de 3 canales, representando 3 distintos n-gramas\n",
    "    #voy a usar n=2,3,5\n",
    "    #no se va a usar glove embeddings por la escasa \n",
    "    \n",
    "    #canal 1\n",
    "    input1=Input(shape=(max_len,))\n",
    "    embedding1=Embedding(tamanio_vocab,dimension_embedding)(input1)\n",
    "    convolution1=Conv1D(num_filtros,tamanio_kernel1,activation='relu')(embedding1)\n",
    "    pooling1=MaxPooling1D(pool_size=2)(convolution1)\n",
    "    flat1=Flatten()(pooling1)\n",
    "\n",
    "    #canal 2\n",
    "    input2=Input(shape=(max_len,))\n",
    "    embedding2=Embedding(tamanio_vocab,dimension_embedding)(input2)\n",
    "    convolution2=Conv1D(num_filtros,tamanio_kernel2,activation='relu')(embedding2)\n",
    "    pooling2=MaxPooling1D(pool_size=2)(convolution2)\n",
    "    flat2=Flatten()(pooling2)\n",
    "\n",
    "    #canal 3\n",
    "    input3=Input(shape=(max_len,))\n",
    "    embedding3=Embedding(tamanio_vocab,dimension_embedding)(input3)\n",
    "    convolution3=Conv1D(num_filtros,tamanio_kernel3,activation='relu')(embedding3)\n",
    "    pooling3=MaxPooling1D(pool_size=2)(convolution3)\n",
    "    flat3=Flatten()(pooling3)\n",
    "\n",
    "   #merge\n",
    "    merged=concatenate([flat1,flat2,flat3])\n",
    "    \n",
    "    #interpretacion\n",
    "    dense1=Dense(10,activation='relu')(merged)\n",
    "    output=Dense(1,activation='sigmoid')(dense1)\n",
    "    model=Model(inputs=[input1,input2,input3],outputs=[output])\n",
    "    \n",
    "    #compilar\n",
    "    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    \n",
    "    #resumen\n",
    "    print(model.summary())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/tomas/.local/lib/python3.6/site-packages/tensorflow/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/tomas/.local/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/tomas/.local/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 24)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 24)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 24)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 24, 100)      1716700     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 24, 100)      1716700     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 24, 100)      1716700     input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 22, 64)       19264       embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 20, 64)       32064       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 20, 64)       32064       embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, 11, 64)       0           conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 10, 64)       0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 10, 64)       0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 704)          0           max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 640)          0           max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 640)          0           max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 1984)         0           flatten[0][0]                    \n",
      "                                                                 flatten_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 10)           19850       concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            11          dense[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 5,253,353\n",
      "Trainable params: 5,253,353\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "modelo=crear_modelo(64,3,5,5,tamanio_de_vocabulario,dimension_embedding,longitud_maxima)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "6851/6851 [==============================] - 9s 1ms/sample - loss: 0.6124 - acc: 0.6685\n",
      "Epoch 2/15\n",
      "6851/6851 [==============================] - 8s 1ms/sample - loss: 0.3151 - acc: 0.8702\n",
      "Epoch 3/15\n",
      "6851/6851 [==============================] - 8s 1ms/sample - loss: 0.1411 - acc: 0.9510\n",
      "Epoch 4/15\n",
      "6851/6851 [==============================] - 11s 2ms/sample - loss: 0.0864 - acc: 0.9718\n",
      "Epoch 5/15\n",
      "6851/6851 [==============================] - 11s 2ms/sample - loss: 0.0655 - acc: 0.9756\n",
      "Epoch 6/15\n",
      "6851/6851 [==============================] - 12s 2ms/sample - loss: 0.0602 - acc: 0.9793\n",
      "Epoch 7/15\n",
      "6851/6851 [==============================] - 12s 2ms/sample - loss: 0.0540 - acc: 0.9796\n",
      "Epoch 8/15\n",
      "6851/6851 [==============================] - 11s 2ms/sample - loss: 0.0479 - acc: 0.9801\n",
      "Epoch 9/15\n",
      "6851/6851 [==============================] - 10s 1ms/sample - loss: 0.0468 - acc: 0.9806\n",
      "Epoch 10/15\n",
      "6851/6851 [==============================] - 10s 1ms/sample - loss: 0.0472 - acc: 0.9797\n",
      "Epoch 11/15\n",
      "6851/6851 [==============================] - 9s 1ms/sample - loss: 0.0399 - acc: 0.9826\n",
      "Epoch 12/15\n",
      "6851/6851 [==============================] - 9s 1ms/sample - loss: 0.0432 - acc: 0.9794\n",
      "Epoch 13/15\n",
      "6851/6851 [==============================] - 8s 1ms/sample - loss: 0.0396 - acc: 0.9810\n",
      "Epoch 14/15\n",
      "6851/6851 [==============================] - 8s 1ms/sample - loss: 0.0388 - acc: 0.9813\n",
      "Epoch 15/15\n",
      "6851/6851 [==============================] - 9s 1ms/sample - loss: 0.0371 - acc: 0.9826\n"
     ]
    }
   ],
   "source": [
    "history=modelo.fit([X_train_pad, X_train_pad, X_train_pad],Y_train,epochs=15,batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss,accuracy=modelo.evaluate([X_train_pad,X_train_pad,X_train_pad],Y_train,verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9657\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy =modelo.evaluate([X_para_evaluar,X_para_evaluar,X_para_evaluar], Y_para_evaluar, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy:  0.7546\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicciones=modelo.predict([X_test_pad,X_test_pad,X_test_pad])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultado=[]\n",
    "for numero in predicciones:\n",
    "    if numero>=0.5:\n",
    "        resultado.append(1)\n",
    "    else:\n",
    "        resultado.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleSubmission=pd.read_csv('sample_submission.csv')\n",
    "output=pd.DataFrame({'id':sampleSubmission.id,'target':resultado})\n",
    "output.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#settings:\n",
    "epochs=20\n",
    "#longitud_maxima (calculada antes)\n",
    "#embedding dimension(calculada antes)\n",
    "output_file='output.txt'\n",
    "\n",
    "modelo=KerasClassifier(build_fn=crear_modelo,epochs=epochs,batch_size=10,verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid=dict(num_filtros=[32,64,128],tamanio_kernel1=[3,5,7],tamanio_kernel2=[1,2,3],tamanio_kernel3=[4,6,5],tamanio_vocab=[tamanio_de_vocabulario],dimension_embedding=[dimension_embedding],max_len=[longitud_maxima])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid=RandomizedSearchCV(estimator=modelo, param_distributions=param_grid,cv=4,verbose=1,n_iter=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras import Model\n",
    "def crear_modelo(num_filtros,tamanio_kernel,tamanio_vocab,dimension_embedding,max_len):\n",
    "    \n",
    "    #red neuronal de 3 canales, representando 3 distintos n-gramas\n",
    "    #voy a usar n=2,3,5\n",
    "    #no se va a usar glove embeddings por la escasa \n",
    "    \n",
    "    #canal 1\n",
    "    input1=Input(shape=(max_len,))\n",
    "    embedding1=Embedding(tamanio_vocab,dimension_embedding)(input1)\n",
    "    convolution1=Conv1D(filters=32,kernel_size=2,activation='relu')(embedding1)\n",
    "    pooling1=MaxPooling1D(pool_size=2)(convolution1)\n",
    "    flat1=Flatten()(pooling1)\n",
    "\n",
    "    #canal 2\n",
    "    input2=Input(shape=(max_len,))\n",
    "    embedding2=Embedding(tamanio_vocab,dimension_embedding)(input2)\n",
    "    convolution2=Conv1D(filters=32,kernel_size=2,activation='relu')(embedding2)\n",
    "    pooling2=MaxPooling1D(pool_size=2)(convolution2)\n",
    "    flat2=Flatten()(pooling2)\n",
    "\n",
    "    #canal 3\n",
    "    input3=Input(shape=(max_len,))\n",
    "    embedding3=Embedding(tamanio_vocab,dimension_embedding)(input3)\n",
    "    convolution3=Conv1D(filters=32,kernel_size=2,activation='relu')(embedding3)\n",
    "    pooling3=MaxPooling1D(pool_size=2)(convolution3)\n",
    "    flat3=Flatten()(pooling3)\n",
    "\n",
    "   #merge\n",
    "    merged=concatenate([flat1,flat2,flat3])\n",
    "    \n",
    "    #interpretacion\n",
    "    dense1=Dense(10,activation='relu')(merged)\n",
    "    output=Dense(1,activation='sigmoid')(dense1)\n",
    "    model=Model(inputs=[input1,input2,input3],outputs=[output])\n",
    "    \n",
    "    #compilar\n",
    "    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    \n",
    "    #resumen\n",
    "    print(model.summary())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>caracteres_usados</th>\n",
       "      <th>menciones_realizadas</th>\n",
       "      <th>permite_location</th>\n",
       "      <th>use_keyword</th>\n",
       "      <th>cita_url</th>\n",
       "      <th>use_hashtag</th>\n",
       "      <th>numero_de_palabras</th>\n",
       "      <th>palabras_unicas</th>\n",
       "      <th>numero_palabras_desconocidas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>deed reason earthquake may allah forgive</td>\n",
       "      <td>1</td>\n",
       "      <td>69</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>resident ask shelter place notify officer evac...</td>\n",
       "      <td>1</td>\n",
       "      <td>133</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>people receive wildfire evacuation order calif...</td>\n",
       "      <td>1</td>\n",
       "      <td>65</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>get sent photo ruby alaska smoke wildfire pour...</td>\n",
       "      <td>1</td>\n",
       "      <td>88</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN           deed reason earthquake may allah forgive   \n",
       "1   4     NaN      NaN              forest fire near la ronge sask canada   \n",
       "2   5     NaN      NaN  resident ask shelter place notify officer evac...   \n",
       "3   6     NaN      NaN  people receive wildfire evacuation order calif...   \n",
       "4   7     NaN      NaN  get sent photo ruby alaska smoke wildfire pour...   \n",
       "\n",
       "   target  caracteres_usados  menciones_realizadas  permite_location  \\\n",
       "0       1                 69                     0                 0   \n",
       "1       1                 38                     0                 0   \n",
       "2       1                133                     0                 0   \n",
       "3       1                 65                     0                 0   \n",
       "4       1                 88                     0                 0   \n",
       "\n",
       "   use_keyword  cita_url  use_hashtag  numero_de_palabras  palabras_unicas  \\\n",
       "0            0         0            1                   6                6   \n",
       "1            0         0            0                   7                7   \n",
       "2            0         0            0                  11                9   \n",
       "3            0         0            1                   6                6   \n",
       "4            0         0            2                   9                9   \n",
       "\n",
       "   numero_palabras_desconocidas  \n",
       "0                             0  \n",
       "1                             0  \n",
       "2                             0  \n",
       "3                             0  \n",
       "4                             0  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "def crear_modelo(num_filtro,tamanio_kernel,tamanio_vocab,dimension_embedding,max_len):\n",
    "    \n",
    "    #canal 1 texto\n",
    "    input1=Input(shape=(max_len,))\n",
    "    embedding1=Embedding(tamanio_vocab,dimension_embedding)(input1)\n",
    "    convolution1=Conv1D(num_filtro,tamanio_kernel,activation='relu')(embedding1)\n",
    "    pooling1=MaxPooling1D(pool_size=2)(convolution1)\n",
    "    flat1=Flatten()(pooling1)\n",
    "    \n",
    "    #canal 2  caracteres usados\n",
    "    input2=Input(shape=(1,))\n",
    "    dense_2_1=Dense(50,activation='relu')(input2)\n",
    "    dense_2_2=Dense(5,activation='relu')(dense_2_1)\n",
    "    \n",
    "    #canal3 menciones realizadas\n",
    "    input3=Input(shape=(1,))\n",
    "    dense_3_1=Dense(50,activation='relu')(input3)\n",
    "    dense_3_2=Dense(5,activation='relu')(dense_3_1)\n",
    "    \n",
    "    #canal 4 numero de palabras en el tweet\n",
    "    input4=Input(shape=(1,))\n",
    "    dense_4_1=Dense(50,activation='relu')(input4)\n",
    "    dense_4_2=Dense(5,activation='relu')(dense_4_1)\n",
    "    \n",
    "    #canal 5 numero de palabras únicas en el tweet\n",
    "    input5=Input(shape=(1,))\n",
    "    dense_5_1=Dense(50,activation='relu')(input5)\n",
    "    dense_5_2=Dense(5,activation='relu')(dense_5_1)\n",
    "        \n",
    "    #canal 6 numero de palabras desconocidas en el tweet\n",
    "    input6=Input(shape=(1,))\n",
    "    dense_6_1=Dense(50,activation='relu')(input6)\n",
    "    dense_6_2=Dense(5,activation='relu')(dense_6_1)\n",
    "    \n",
    "    #concatenate layer \n",
    "    merged=concatenate([flat1,dense_2_2,dense_3_2,dense_4_2,dense_5_2,dense_6_2])  \n",
    "    dense_final=Dense(20,activation='relu')(merged)\n",
    "    output=Dense(1,activation='sigmoid')(dense_final)\n",
    "    \n",
    "    model=Model(inputs=[input1,input2,input3,input4,input5,input6],outputs=[output])\n",
    "    \n",
    "    #compilar\n",
    "    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    print(modelo.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_16 (InputLayer)           [(None, 24)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, 24, 100)      1716700     input_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 21, 32)       12832       embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "input_17 (InputLayer)           [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_18 (InputLayer)           [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_19 (InputLayer)           [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_20 (InputLayer)           [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_21 (InputLayer)           [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 10, 32)       0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_26 (Dense)                (None, 50)           100         input_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_28 (Dense)                (None, 50)           100         input_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_30 (Dense)                (None, 50)           100         input_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_32 (Dense)                (None, 50)           100         input_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_34 (Dense)                (None, 50)           100         input_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 320)          0           max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_27 (Dense)                (None, 5)            255         dense_26[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_29 (Dense)                (None, 5)            255         dense_28[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_31 (Dense)                (None, 5)            255         dense_30[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_33 (Dense)                (None, 5)            255         dense_32[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_35 (Dense)                (None, 5)            255         dense_34[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 345)          0           flatten_5[0][0]                  \n",
      "                                                                 dense_27[0][0]                   \n",
      "                                                                 dense_29[0][0]                   \n",
      "                                                                 dense_31[0][0]                   \n",
      "                                                                 dense_33[0][0]                   \n",
      "                                                                 dense_35[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_36 (Dense)                (None, 20)           6920        concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_37 (Dense)                (None, 1)            21          dense_36[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 1,738,248\n",
      "Trainable params: 1,738,248\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "modelo=crear_modelo(32,4,tamanio_de_vocabulario,dimension_embedding,longitud_maxima)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "caracteres_train=X_tr['caracteres_usados']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "menciones_train=X_tr['menciones_realizadas']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'Unnamed: 0.1', 'id', 'keyword', 'location', 'text',\n",
       "       'target', 'caracteres_usados', 'menciones_realizadas',\n",
       "       'permite_location', 'use_keyword', 'cita_url', 'use_hashtag',\n",
       "       'numero_de_palabras', 'palabras_unicas',\n",
       "       'numero_palabras_desconocidas'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_train=X_tr['permite_location']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_de_palabras_train=X_tr['numero_de_palabras']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_de_palabras_unicas_train=X_tr['palabras_unicas']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_de_palabras_desconocidas_train=X_tr['numero_palabras_desconocidas']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "6851/6851 [==============================] - 5s 724us/sample - loss: 0.6577 - acc: 0.6250\n",
      "Epoch 2/20\n",
      "6851/6851 [==============================] - 3s 492us/sample - loss: 0.3934 - acc: 0.8400\n",
      "Epoch 3/20\n",
      "6851/6851 [==============================] - 3s 472us/sample - loss: 0.2083 - acc: 0.9245\n",
      "Epoch 4/20\n",
      "6851/6851 [==============================] - 3s 450us/sample - loss: 0.1199 - acc: 0.9584\n",
      "Epoch 5/20\n",
      "6851/6851 [==============================] - 3s 490us/sample - loss: 0.0831 - acc: 0.9712\n",
      "Epoch 6/20\n",
      "6851/6851 [==============================] - 3s 470us/sample - loss: 0.0653 - acc: 0.9781\n",
      "Epoch 7/20\n",
      "6851/6851 [==============================] - 3s 486us/sample - loss: 0.0560 - acc: 0.9785\n",
      "Epoch 8/20\n",
      "6851/6851 [==============================] - 4s 533us/sample - loss: 0.0505 - acc: 0.9800\n",
      "Epoch 9/20\n",
      "6851/6851 [==============================] - 3s 399us/sample - loss: 0.0471 - acc: 0.9799\n",
      "Epoch 10/20\n",
      "6851/6851 [==============================] - 3s 403us/sample - loss: 0.0444 - acc: 0.9810\n",
      "Epoch 11/20\n",
      "6851/6851 [==============================] - 3s 401us/sample - loss: 0.0418 - acc: 0.9819\n",
      "Epoch 12/20\n",
      "6851/6851 [==============================] - 3s 373us/sample - loss: 0.0376 - acc: 0.9822\n",
      "Epoch 13/20\n",
      "6851/6851 [==============================] - 3s 411us/sample - loss: 0.0411 - acc: 0.9812\n",
      "Epoch 14/20\n",
      "6851/6851 [==============================] - 3s 432us/sample - loss: 0.0393 - acc: 0.9819\n",
      "Epoch 15/20\n",
      "6851/6851 [==============================] - 3s 448us/sample - loss: 0.0394 - acc: 0.9812\n",
      "Epoch 16/20\n",
      "6851/6851 [==============================] - 3s 425us/sample - loss: 0.0396 - acc: 0.9800\n",
      "Epoch 17/20\n",
      "6851/6851 [==============================] - 4s 532us/sample - loss: 0.0388 - acc: 0.9828\n",
      "Epoch 18/20\n",
      "6851/6851 [==============================] - 5s 693us/sample - loss: 0.0359 - acc: 0.9828\n",
      "Epoch 19/20\n",
      "6851/6851 [==============================] - 4s 650us/sample - loss: 0.0356 - acc: 0.9823\n",
      "Epoch 20/20\n",
      "6851/6851 [==============================] - 4s 597us/sample - loss: 0.0351 - acc: 0.9826\n"
     ]
    }
   ],
   "source": [
    "history=modelo.fit([X_train_pad,caracteres_train,menciones_train,location_train,num_de_palabras_train,num_de_palabras_unicas_train,num_de_palabras_desconocidas_train],Y_train,epochs=20,batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss,accuracy=modelo.evaluate([X_train_pad,caracteres_train,menciones_train,location_train,num_de_palabras_train,num_de_palabras_unicas_train,num_de_palabras_desconocidas_train],Y_train,verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9853\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "segundo_input=X_te['caracteres_usados']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "tercer_input=X_te['menciones_realizadas']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuarto_input=X_te['permite_location']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "quinto_input=X_te['numero_de_palabras']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "sexto_input=X_te['palabras_unicas']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "septimo_input=X_te['numero_palabras_desconocidas']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss,accuracy=modelo.evaluate([X_para_evaluar,segundo_input,tercer_input,cuarto_input,quinto_input,sexto_input,septimo_input],Y_te,verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.7598\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing Accuracy: {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
