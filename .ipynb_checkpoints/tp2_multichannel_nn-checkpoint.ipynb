{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Red neuronal profunda usando word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import sklearn as sk\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from time import time\n",
    "from collections import defaultdict\n",
    "from tensorflow import keras\n",
    "\n",
    "path_glove='/home/tomas/organizacion_de_datos/tp2/glove.6B.100d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Embedding,LSTM,GRU\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/tomas/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/tomas/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/tomas/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_stopwords=set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set=pd.read_csv(\"train.csv\",encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set=pd.read_csv(\"test.csv\",encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Procesamiento del texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quitar_stopwords(texto):\n",
    "    texto=' '.join([word for word in texto.split() if word not in english_stopwords])\n",
    "    return texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quitar_menciones(texto):\n",
    "    texto=' '.join([palabra for palabra in texto.split() if '@' not in palabra])\n",
    "    return texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Diccionario_de_lenguaje_de_internet={\n",
    "\"2day\": \"Today\",\n",
    "\"2moro\": \"Tomorrow\",\n",
    "\"2nite\": \"Tonight\",\n",
    "\"4EAE\": \"For Ever And Ever\",\n",
    "\"ABT\":\"About\",\n",
    "\"ADN\": \"Any Day Now\",\n",
    "\"AFAIC\": \"As Far As I’m Concerned\",\n",
    "\"AFAICT\": \"As Far As I Can Tell\",\n",
    "\"AFAIK\": \"As Far As I Know\",\n",
    "\"AFAIR\": \"As Far As I Remember\",\n",
    "\"AKA\": \"Also Known As\",\n",
    "\"AMA\": \"Ask Me Anything\",\n",
    "\"ASAIC\": \"As Soon As I Can\",\n",
    "\"ASAP\": \"As Soon As Possible\",\n",
    "\"ATM\": \"At The Moment\",\n",
    "\"B4\": \"Before\",\n",
    "\"B4N\": \"Bye For Now\",\n",
    "\"Bae\": \"Babe/Before Anyone Else\",\n",
    "\"BBL\": \"Be Back Later\",\n",
    "\"BBT\": \"Be Back Tomorrow\",\n",
    "\"BCNU\": \"Be Seeing You\",\n",
    "\"BD\": \"Big Deal\",\n",
    "\"BF\": \"Boyfriend\",\n",
    "\"BFF\": \"Best Friends Forever\",\n",
    "\"BMT\": \"Before My Time\",\n",
    "\"BOL\": \"Be On Later\",\n",
    "\"BOT\": \"Back On Topic\",\n",
    "\"BRB\": \"Be Right Back\",\n",
    "\"BRO\": \"Brother\",\n",
    "\"BT\": \"But\",\n",
    "\"BTW\": \"By The Way\",\n",
    "\"CFY\": \"Calling For You\",\n",
    "\"CU\": \"See You\",\n",
    "\"CUL\": \"See You Later\",\n",
    "\"Cuz\": \"Because\",\n",
    "\"CYA\": \"Cover Your Ass\",\n",
    "\"DAE\": \"Does Anyone Else\",\n",
    "\"DBA\": \"Doing Business As\",\n",
    "\"DFTBA\": \"Don’t Forget To Be Awesome\",\n",
    "\"DIKU\": \"Do I Know You\",\n",
    "\"DM\": \"Direct Message\",\n",
    "\"DND\": \"Do Not Disturb\",\n",
    "\"DR\": \"Double Rainbow\",\n",
    "\"DWBH\": \"Don’t Worry, Be Happy\",\n",
    "\"ELI5\": \"Explain Like I’m 5\",\n",
    "\"EOM\": \"End Of Message\",\n",
    "\"EOS\": \"End Of Story\",\n",
    "\"F2F\": \"Face To Face\",\n",
    "\"FAQ\": \"Frequently Asked Question\",\n",
    "\"FB\": \"Facebook\",\n",
    "\"FBF\": \"Flash Back Friday\",\n",
    "\"FF\": \"Follow Friday\",\n",
    "\"FIFY\": \"Fixed It For You\",\n",
    "\"FITB\": \"Fill In The Blank\",\n",
    "\"FML\": \"Fuck My Life\",\n",
    "\"FOMO\": \"Fear Of Missing Out\",\n",
    "\"FTFY\": \"Fixed That For You\",\n",
    "\"FTL\": \"For The Loss\",\n",
    "\"FTW\": \"For The Win\",\n",
    "\"FWB\": \"Friends With Benefits\",\n",
    "\"FWIW\": \"For What It’s Worth\",\n",
    "\"FYE\": \"For Your Entertainment\",\n",
    "\"FYEO\": \"For Your Eyes Only\",\n",
    "\"FYI\": \"For Your Information\",\n",
    "\"GA\": \"Go Ahead\",\n",
    "\"GAL\": \"Get A Life\",\n",
    "\"GF\": \"Girlfriend\",\n",
    "\"GM\": \"Good Morning\",\n",
    "\"GN\": \"Good Night\",\n",
    "\"Gr8\": \"Great\",\n",
    "\"GTR\": \"Getting Ready\",\n",
    "\"HAND\": \"Have A Nice Day\",\n",
    "\"HB\": \"Hurry Back\",\n",
    "\"HBD\": \"Happy Birthday\",\n",
    "\"HBU\": \"How About You\",\n",
    "\"HMB\": \"Hit Be Back\",\n",
    "\"HMU\": \"Hit Me Up\",\n",
    "\"HRU\": \"How Are You\",\n",
    "\"HTH\": \"Hope This Helps\",\n",
    "\"IAC\": \"In Any Case\",\n",
    "\"IC\": \"I See\",\n",
    "\"ICYMI\": \"In Case You Missed It\",\n",
    "\"IDC\": \"I Don’t Care\",\n",
    "\"IDK\": \"I Don’t Know\",\n",
    "\"IG\": \"Instagram\",\n",
    "\"IIRC\": \"If I Remember Correctly\",\n",
    "\"IKR\": \"I Know Right\",\n",
    "\"ILY\": \"I Love You\",\n",
    "\"IMHO\": \"In My Humble Opinion\",\n",
    "\"IMMD\": \"It Made My Day\",\n",
    "\"IMY\": \"I Miss You\",\n",
    "\"IRL\": \"In Real Life\",\n",
    "\"IS\": \"I’m Sorry\",\n",
    "\"ISO\": \"In Search Of\",\n",
    "\"IU2U\": \"It’s Up To You\",\n",
    "\"J4F\": \"Just For Fun\",\n",
    "\"JAM\": \"Just A Minute\",\n",
    "\"JFY\": \"Just For You\",\n",
    "\"JIC\": \"Just In Case\",\n",
    "\"JK\": \"Just Kidding\",\n",
    "\"JSYK\": \"Just So You Know\",\n",
    "\"KK\": \"Okay\",\n",
    "\"L8\": \"Late\",\n",
    "\"L8R\": \"Later\",\n",
    "\"LMA\": \"Leave Me Alone\",\n",
    "\"LMAO\": \"Laughing My Ass Off\",\n",
    "\"LMBO\": \"Laughing My Butt Off\",\n",
    "\"LMK\": \"Let Me Know\",\n",
    "\"LOL\": \"Laugh Out Loud\",\n",
    "\"LTNS\": \"Long Time No See\",\n",
    "\"LYLAS\": \"Love You Like A Sister\",\n",
    "\"M/F\": \"Male or Female\",\n",
    "\"M8\": \"Mate\",\n",
    "\"MP\": \"My pleasure\",\n",
    "\"MSM\": \"Mainstream Media\",\n",
    "\"MU\": \"Miss You\",\n",
    "\"MYOB\": \"Mind Your Own Business\",\n",
    "\"NAGI\": \"Not A Good Idea\",\n",
    "\"NBD\": \"No Big Deal\",\n",
    "\"NE1\": \"Anyone\",\n",
    "\"NM\": \"Not Much\",\n",
    "\"NP\": \"No Problem\",\n",
    "\"NSFL\": \"Not Safe For Life\",\n",
    "\"NSFW\": \"Not Safe For Work\",\n",
    "\"NTS\": \"Note To Self\",\n",
    "\"NVM\": \"Never Mind\",\n",
    "\"OC\": \"Original Content\",\n",
    "\"OH\": \"Overheard\",\n",
    "\"OIC\": \"Oh ! I See\",\n",
    "\"OMD\": \"Oh My Damn\",\n",
    "\"OMG\": \"Oh My Goodness\",\n",
    "\"OMW\": \"On My Way\",\n",
    "\"OT\": \"Off Topic\",\n",
    "\"OFC\": \"Of course\",\n",
    "\"PAW\": \"Parents Are Watching\",\n",
    "\"Pls\": \"Please\",\n",
    "\"POTD\": \"Photo Of The Day\",\n",
    "\"POV\": \"Point Of View\",\n",
    "\"PPL\": \"People\",\n",
    "\"PTB\": \"Please Text Back\",\n",
    "\"Q4U\": \"Question For You\",\n",
    "\"QQ\": \"Crying\",\n",
    "\"RBTL\": \"Read Between The Lines\",\n",
    "\"RIP\": \"Rest In Peace\",\n",
    "\"RL\": \"Real Life\",\n",
    "\"ROFL\": \"Rolling On the Floor Laughing\",\n",
    "\"RT\": \"Retweet\",\n",
    "\"RTM\": \"Read The Manual\",\n",
    "\"SIS\": \"Sister\",\n",
    "\"SITD\": \"Still In The Dark\",\n",
    "\"SM\": \"Social Media\",\n",
    "\"SMH\": \"Shaking My Head\",\n",
    "\"SMY\": \"Somebody\",\n",
    "\"SNH\": \"Sarcasm Noted Here\",\n",
    "\"SOL\": \"Sooner Or Later\",\n",
    "\"Some1\": \"Someone\",\n",
    "\"SRSLY\": \"Seriously\",\n",
    "\"STBY\": \"Sucks To Be You\",\n",
    "\"Str8\": \"Straight\",\n",
    "\"SYS\": \"See You Soon\",\n",
    "\"TBA\": \"To Be Announced\",\n",
    "\"TBH\": \"To Be Honest\",\n",
    "\"TBT\": \"Throwback Thursday\",\n",
    "\"TBT\": \"Truth Be Told\",\n",
    "\"TFH\": \"Thread From Hell\",\n",
    "\"TFTI\": \"Thanks For The Invite\",\n",
    "\"TGIF\": \"Thank God It’s Friday\",\n",
    "\"THX\": \"Thanks\",\n",
    "\"TIA\": \"Thanks in Advance\",\n",
    "\"TIL\": \"Today I Learned\",\n",
    "\"TIME\": \"Tears In My Eyes\",\n",
    "\"TL;DR\": \"Too Long; Didn’t Read\",\n",
    "\"TLDR\":\"Too long didn’t read\",\n",
    "\"TL DR\":\"Too long didn’t read\",\n",
    "\"TLC\": \"Tender Loving Care\",\n",
    "\"TMI\": \"Too Much Information\",\n",
    "\"TTYL\": \"Talk To You Later\",\n",
    "\"TTYS\": \"Talk To You Soon\",\n",
    "\"Txt\": \"Text\",\n",
    "\"TYVM\": \"Thank You Very Much\",\n",
    "\"U\": \"You\",\n",
    "\"U4F\": \"You Forever\",\n",
    "\"UR\": \"Your\",\n",
    "\"VBG\": \"Very Big Grin\",\n",
    "\"VSF\": \"Very Sad Face\",\n",
    "\"WB\": \"Welcome Back\",\n",
    "\"WBU\": \"What About You?\",\n",
    "\"WEG\": \"Wicked Evil Grin\",\n",
    "\"WKND\": \"Weekend\",\n",
    "\"WOM\": \"Word of Mouth\",\n",
    "\"WOTD\": \"Word Of The Day\",\n",
    "\"Wru\": \"Who Are You\",\n",
    "\"WTH\": \"What The Heck?\",\n",
    "\"WTPA\": \"Where The Party At?\",\n",
    "\"WU?\": \"What's Up\",\n",
    "\"WU\":\"What's Up\",\n",
    "\"WYCM\": \"Will You Call Me?\",\n",
    "\"WYWH\": \"Wish You Were Here\",\n",
    "\"XOXO\": \"Hugs and Kisses\",\n",
    "\"YGM\": \"You’ve Got Mail\",\n",
    "\"YNK\": \"You Never Know\",\n",
    "\"YOLO\": \"You Only Live Once\",\n",
    "\"YT\": \"YouTube\",\n",
    "\"YW\": \"You’re Welcome\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reemplazar_lenguaje_internet(texto):\n",
    "    texto=texto.upper()\n",
    "    palabras=texto.split()\n",
    "    palabras_procesadas=[]\n",
    "    for palabra in palabras:\n",
    "        traduccion=Diccionario_de_lenguaje_de_internet.get(palabra,'not internet slang')\n",
    "        if(traduccion!='not internet slang'):\n",
    "            lista_aux=traduccion.split()\n",
    "            for x in lista_aux:\n",
    "                palabras_procesadas.append(x.lower())\n",
    "        else:\n",
    "            palabras_procesadas.append(palabra.lower())\n",
    "    texto=' '.join([word for word in palabras_procesadas])\n",
    "    return texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "diccionario_contracciones = {  \"ain't\": \"is not\",\n",
    "                               \"aint\":\"is not\",\n",
    "                               \"aren't\": \"are not\",\n",
    "                               \"arent\":\"are not\",\n",
    "                               \"can't\": \"can not\",\n",
    "                               \"cant\":\"can not\",\n",
    "                               \"cause\": \"because\",\n",
    "                               \"could've\": \"could have\",\n",
    "                               \"couldve\":\"could have\",\n",
    "                               \"couldn't\": \"could not\",\n",
    "                               \"couldnt\":\"could not\",\n",
    "                               \"didn't\": \"did not\", \n",
    "                               \"didnt\":\"did not\",\n",
    "                               \"doesn't\": \"does not\",\n",
    "                               \"doesnt\":\"does not\",\n",
    "                               \"don't\": \"do not\",\n",
    "                               \"dont\":\"do not\",\n",
    "                               \"hadn't\": \"had not\",\n",
    "                               \"hadnt\":\"had not\",\n",
    "                               \"hasn't\": \"has not\",\n",
    "                               \"hasnt\":\"has not\",\n",
    "                               \"haven't\": \"have not\",\n",
    "                               \"havent\":\"have not\",\n",
    "                               \"he'd\": \"he would\",\n",
    "                               \"hed\":\"he would\",\n",
    "                               \"he'll\": \"he will\",\n",
    "                               \"he ll\":\"he will\",\n",
    "                               \"he's\": \"he is\",\n",
    "                               \"hes\":\"he is\",\n",
    "                               \"how'd\": \"how did\",\n",
    "                               \"howd\":\"how did\",\n",
    "                               \"how'd'y\": \"how do you\",\n",
    "                               \"howdy\":\"how do you\",\n",
    "                               \"how'll\": \"how will\",\n",
    "                               \"howll\":\"how will\",\n",
    "                               \"how's\": \"how is\",\n",
    "                               \"hows\":\"how is\",\n",
    "                               \"I'd\": \"I would\",\n",
    "                               \"id\":\"i would\",\n",
    "                               \"I'd've\": \"I would have\",\n",
    "                               \"idve\":\"i would have\",\n",
    "                               \"I'll\": \"I will\", \n",
    "                               \"Ill\":\"i will\", #duda: si algun tweet habla de un enfermo puede traer ruido\n",
    "                               \"I'll've\": \"I will have\",\n",
    "                               \"I'm\": \"I am\", \n",
    "                               \"im\":\"i am\",\n",
    "                               \"I've\": \"I have\",\n",
    "                               \"ive\":\"i have\",\n",
    "                               \"i'd\": \"i would\",\n",
    "                               \"i'd've\": \"i would have\",\n",
    "                               \"i'll\": \"i will\",\n",
    "                               \"i'll've\": \"i will have\",\n",
    "                               \"i'm\": \"i am\", \n",
    "                               \"im\":\"i am\",\n",
    "                               \"i've\": \"i have\",\n",
    "                               \"isn't\": \"is not\",\n",
    "                               \"isnt\":\"is not\",\n",
    "                               \"it'd\": \"it would\",\n",
    "                               \"itd\":\"it would\",\n",
    "                               \"it'd've\": \"it would have\",\n",
    "                               \"it'll\": \"it will\",\n",
    "                               \"itll\":\"it will\",\n",
    "                               \"it'll've\": \"it will have\",\n",
    "                               \"it's\": \"it is\",\n",
    "                               \"its\":\"it is\",\n",
    "                               \"let's\": \"let us\",\n",
    "                               \"lets\":\"let us\",\n",
    "                               \"ma'am\": \"madam\",\n",
    "                               \"maam\":\"madam\",\n",
    "                               \"mayn't\": \"may not\",\n",
    "                               \"maynt\":\"may not\",\n",
    "                               \"might've\": \"might have\",\n",
    "                               \"mightve\":\"might have\",\n",
    "                               \"mightn't\": \"might not\",\n",
    "                               \"mightnt\":\"might not\",\n",
    "                               \"mightn't've\": \"might not have\",\n",
    "                               \"must've\": \"must have\",\n",
    "                               \"mustve\":\"must have\",\n",
    "                               \"mustn't\": \"must not\",\n",
    "                               \"mustnt\":\"must not\",\n",
    "                               \"mustn't've\": \"must not have\",\n",
    "                               \"needn't\": \"need not\",\n",
    "                               \"neednt\":\"need not\",\n",
    "                               \"needn't've\": \"need not have\",\n",
    "                               \"o'clock\": \"of the clock\",\n",
    "                               \"oclock\":\"of the clock\",\n",
    "                               \"oughtn't\": \"ought not\",\n",
    "                               \"oughtnt\":\"ought not\",\n",
    "                               \"oughtn't've\": \"ought not have\",\n",
    "                               \"shan't\": \"shall not\",\n",
    "                               \"shant\":\"shall not\",\n",
    "                               \"sha'n't\": \"shall not\",\n",
    "                               \"shan't've\": \"shall not have\",\n",
    "                               \"she'd\": \"she would\",\n",
    "                               \"shed\":\"she would\",\n",
    "                               \"she'd've\": \"she would have\",\n",
    "                               \"she'll\": \"she will\",\n",
    "                               \"shell\":\"she will\",#nuevamente aca tengo ruido por dos palabras igual escritas\n",
    "                               \"she'll've\": \"she will have\", \n",
    "                               \"she's\": \"she is\",\n",
    "                               \"shes\":\"she is\",\n",
    "                               \"should've\": \"should have\",\n",
    "                               \"shouldve\":\"should have\",\n",
    "                               \"shouldn't\": \"should not\",\n",
    "                               \"shouldnt\": \"should not\",\n",
    "                               \"shouldn't've\": \"should not have\",\n",
    "                               \"so've\": \"so have\",\n",
    "                               \"so's\": \"so as\",\n",
    "                               \"this's\": \"this is\",\n",
    "                               \"that'd\": \"that would\",\n",
    "                               \"that'd've\": \"that would have\",\n",
    "                               \"that's\": \"that is\",\n",
    "                               \"thats\":\"that is\",\n",
    "                               \"there'd\": \"there would\",\n",
    "                               \"thered\":\"there would\",\n",
    "                               \"there'd've\": \"there would have\",\n",
    "                               \"there's\": \"there is\",\n",
    "                               \"theres\":\"there is\",\n",
    "                               \"here's\": \"here is\",\n",
    "                               \"heres\":\"here is\",\n",
    "                               \"they'd\": \"they would\",\n",
    "                               \"theyd\":\"they would\",\n",
    "                               \"they'd've\": \"they would have\",\n",
    "                               \"they'll\": \"they will\",\n",
    "                               \"theyll\":\"they will\",\n",
    "                               \"they'll've\": \"they will have\", \n",
    "                               \"they're\": \"they are\",\n",
    "                               \"theyre\":\"they are\",\n",
    "                               \"they've\": \"they have\", \n",
    "                               \"theyve\":\"they have\",\n",
    "                               \"to've\": \"to have\", \n",
    "                               \"tove\":\"to have\",\n",
    "                               \"wasn't\": \"was not\",\n",
    "                               \"wasnt\":\"was not\",\n",
    "                               \"we'd\": \"we would\",\n",
    "                               \"wed\":\"we would\",#aca nuevamente hay conflicto\n",
    "                               \"we'd've\": \"we would have\",\n",
    "                               \"we'll\": \"we will\",\n",
    "                               \"we'll've\": \"we will have\",\n",
    "                               \"we're\": \"we are\",\n",
    "                               \"we've\": \"we have\",\n",
    "                               \"weren't\": \"were not\",\n",
    "                               \"werent\":\"were not\",\n",
    "                               \"what'll\": \"what will\",\n",
    "                               \"whatll\":\"what will\",\n",
    "                               \"what'll've\": \"what will have\",\n",
    "                               \"what're\": \"what are\", \n",
    "                               \"whatre\":\"what are\",\n",
    "                               \"what's\": \"what is\",\n",
    "                               \"whats\":\"what is\",\n",
    "                               \"what've\": \"what have\",\n",
    "                               \"whatve\":\"what have\",\n",
    "                               \"when's\": \"when is\",\n",
    "                               \"whens\":\"when is\",\n",
    "                               \"when've\": \"when have\",\n",
    "                               \"whenve\":\"when have\",\n",
    "                               \"where'd\": \"where did\",\n",
    "                               \"whered\":\"where did\",\n",
    "                               \"where's\": \"where is\",\n",
    "                               \"wheres\":\"where is\",\n",
    "                               \"where've\": \"where have\",\n",
    "                               \"whereve\":\"where have\",\n",
    "                               \"who'll\": \"who will\", \n",
    "                               \"wholl\":\"who will\",\n",
    "                               \"who'll've\": \"who will have\",\n",
    "                               \"who's\": \"who is\",\n",
    "                               \"whos\":\"who is\",\n",
    "                               \"who've\": \"who have\",\n",
    "                               \"whove\":\"who have\",\n",
    "                               \"why's\": \"why is\",\n",
    "                               \"whys\":\"why is\",\n",
    "                               \"why've\": \"why have\",\n",
    "                               \"whyve\":\"why have\",\n",
    "                               \"will've\": \"will have\",\n",
    "                               \"willve\":\"will have\",\n",
    "                               \"won't\": \"will not\",\n",
    "                               \"wont\":\"will not\",\n",
    "                               \"won't've\": \"will not have\",\n",
    "                               \"would've\": \"would have\",\n",
    "                               \"wouldve\":\"would have\",\n",
    "                               \"wouldn't\": \"would not\",\n",
    "                               \"wouldnt\":\"would not\",\n",
    "                               \"wouldn't've\": \"would not have\",\n",
    "                               \"y'all\": \"you all\",\n",
    "                               \"yall\":\"you all\",\n",
    "                               \"y'all'd\": \"you all would\",\n",
    "                               \"yalld\":\"you all would\",\n",
    "                               \"y'all'd've\": \"you all would have\",\n",
    "                               \"y'all're\": \"you all are\",\n",
    "                               \"yallre\":\"you all are\",\n",
    "                               \"y'all've\": \"you all have\",\n",
    "                               \"you'd\": \"you would\",\n",
    "                               \"youd\":\"you would\",\n",
    "                               \"you'd've\": \"you would have\",\n",
    "                               \"you'll\": \"you will\",\n",
    "                               \"youll\":\"you will\",\n",
    "                               \"you'll've\": \"you will have\",\n",
    "                               \"you're\": \"you are\",\n",
    "                               \"youre\":\"you are\",\n",
    "                               \"you've\": \"you have\",\n",
    "                               \"youve\":\"you have\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reemplazar_contracciones(texto):\n",
    "    texto=texto.lower()\n",
    "    palabras=texto.split()\n",
    "    palabras_procesadas=[]\n",
    "    for palabra in palabras:\n",
    "        traduccion=diccionario_contracciones.get(palabra,'no es contraccion')\n",
    "        if(traduccion!='no es contraccion'):\n",
    "            lista_aux=traduccion.split()\n",
    "            for x in lista_aux:\n",
    "                palabras_procesadas.append(x.lower())\n",
    "        else:\n",
    "            palabras_procesadas.append(palabra.lower())\n",
    "    texto=' '.join([word for word in palabras_procesadas])\n",
    "    return texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#es necesario importar la libreria re (regular expression) para poder usar esta funcion\n",
    "#ya esta bajada en el comienzo del notebook pero si lo que quieren es analizar la documentación\n",
    "#lo pueden revisar en google\n",
    "#falta borrar los arroba\n",
    "def limpiar_texto(texto):\n",
    "    texto = reemplazar_lenguaje_internet(texto)\n",
    "    texto=reemplazar_contracciones(texto)\n",
    "    texto=texto.lower()\n",
    "    texto = re.sub('\\[.*?\\]', '', texto) # remove text in square brackets\n",
    "    texto = re.sub('https?://\\S+|www\\.\\S+', '', texto) # remove URLs\n",
    "    texto = re.sub('<.*?>+', '', texto) # remove html tags\n",
    "    texto = re.sub('[%s]' % re.escape(string.punctuation), '', texto) # remove punctuation\n",
    "    texto = re.sub('\\n', '', texto) # remove words conatinaing numbers\n",
    "    texto = re.sub('\\w*\\d\\w*', '', texto)\n",
    "    texto = re.sub('[‘’“”…]', '', texto)\n",
    "    texto = quitar_stopwords(texto)\n",
    "    texto = quitar_menciones(texto)\n",
    "\n",
    "    return texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set['text']=train_set['text'].apply(limpiar_texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizar_texto(texto):\n",
    "    lista_palabras=nltk.word_tokenize(texto)\n",
    "    texto=' '.join([lemmatizer.lemmatize(word) for word in lista_palabras])\n",
    "    return texto\n",
    "#nota: la función esta definida aca, pero para ver los resultados que produce hay\n",
    "#que ir al comienzo del notebook y ejecutar la celda de lemmatizar texto luego\n",
    "#de haber limpiado el texto original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(palabra):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([palabra])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizar_con_pos(texto):\n",
    "    lista_palabras=texto.split()\n",
    "    texto=' '.join(lemmatizer.lemmatize(palabra,get_wordnet_pos(palabra)) for palabra in lista_palabras)\n",
    "    return texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set['text']=train_set['text'].apply(lemmatizar_con_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenización del texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Embedding,LSTM,GRU\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape de X_train: (6851,)\n",
      "Shape de X_test:(3263,)\n",
      "Shape de todos_los_textos:(10114,)\n",
      "longitud maxima: 31\n",
      "tamanio de vocabulario: 27401\n"
     ]
    }
   ],
   "source": [
    "X=train_set['text'].copy().values\n",
    "Y=train_set['target'].copy().values\n",
    "#separamos el train set en dos, un conjunto de entrenamiento y uno de validacion, ratio: 90-10\n",
    "X_para_entrenar,X_para_evaluar,Y_para_entrenar,Y_para_evaluar=train_test_split(X,Y,test_size=0.1,random_state=1000)\n",
    "\n",
    "#se hace un análisis del target usando NLP\n",
    "X_train=X_para_entrenar\n",
    "Y_train=Y_para_entrenar\n",
    "X_test=test_set['text'].copy().values\n",
    "\n",
    "print(\"Shape de X_train: \"+ str(X_train.shape))\n",
    "print(\"Shape de X_test:\" + str(X_test.shape))\n",
    "todos_los_textos=np.concatenate([X_train,X_test])\n",
    "print(\"Shape de todos_los_textos:\" + str(todos_los_textos.shape))\n",
    "\n",
    "#estan todos los textos concatenados, hay que 'entrenar' al tokenizador\n",
    "objeto_tokenizador=Tokenizer()\n",
    "objeto_tokenizador.fit_on_texts(todos_los_textos)\n",
    "\n",
    "#necesito una cota de la longitud de cada palabra de los textos que \n",
    "#se van a analizar\n",
    "longitud_maxima=max([len(s.split()) for s in todos_los_textos])\n",
    "print(\"longitud maxima: \" + str(longitud_maxima))\n",
    "\n",
    "#necesito saber cuantas palabras tengo en mi 'diccionario' de palabras\n",
    "tamanio_de_vocabulario=len(objeto_tokenizador.word_index) +1\n",
    "print(\"tamanio de vocabulario: \" + str(tamanio_de_vocabulario))\n",
    "\n",
    "#ahora que tengo esto, es tiempo de tokenizar cada uno de los tweets\n",
    "#y agregar el padding necesario\n",
    "X_train_tokens=objeto_tokenizador.texts_to_sequences(X_train)\n",
    "X_train_pad=pad_sequences(X_train_tokens,maxlen=longitud_maxima,padding='post')\n",
    "\n",
    "X_test_tokens=objeto_tokenizador.texts_to_sequences(X_test)\n",
    "X_test_pad=pad_sequences(X_test_tokens,maxlen=longitud_maxima,padding='post')\n",
    "\n",
    "X_para_evaluar=pad_sequences(objeto_tokenizador.texts_to_sequences(X_para_evaluar),maxlen=longitud_maxima,padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_matrix(filepath, word_index, embedding_dim):\n",
    "    vocab_size = len(word_index) + 1  # Adding again 1 because of reserved 0 index\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "    with open(filepath) as f:\n",
    "        for line in f:\n",
    "            word, *vector = line.split()\n",
    "            if word in word_index:\n",
    "                idx = word_index[word] \n",
    "                embedding_matrix[idx] = np.array(\n",
    "                    vector, dtype=np.float32)[:embedding_dim]\n",
    "\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5121710886463998\n"
     ]
    }
   ],
   "source": [
    "dimension_embedding=100\n",
    "embedding_matrix = create_embedding_matrix(path_glove,objeto_tokenizador.word_index, dimension_embedding)\n",
    "\n",
    "\n",
    "nonzero_elements = np.count_nonzero(np.count_nonzero(embedding_matrix, axis=1))\n",
    "print(nonzero_elements/tamanio_de_vocabulario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras import Model\n",
    "def crear_modelo(num_filtros,tamanio_kernel1,tamanio_kernel2,tamanio_kernel3,tamanio_vocab,dimension_embedding,max_len):\n",
    "    \n",
    "    #red neuronal de 3 canales, representando 3 distintos n-gramas\n",
    "    #voy a usar n=2,3,5\n",
    "    #no se va a usar glove embeddings por la escasa \n",
    "    \n",
    "    #canal 1\n",
    "    input1=Input(shape=(max_len,))\n",
    "    embedding1=Embedding(tamanio_vocab,dimension_embedding)(input1)\n",
    "    convolution1=Conv1D(num_filtros,tamanio_kernel1,activation='relu')(embedding1)\n",
    "    pooling1=MaxPooling1D(pool_size=2)(convolution1)\n",
    "    flat1=Flatten()(pooling1)\n",
    "\n",
    "    #canal 2\n",
    "    input2=Input(shape=(max_len,))\n",
    "    embedding2=Embedding(tamanio_vocab,dimension_embedding)(input2)\n",
    "    convolution2=Conv1D(num_filtros,tamanio_kernel2,activation='relu')(embedding2)\n",
    "    pooling2=MaxPooling1D(pool_size=2)(convolution2)\n",
    "    flat2=Flatten()(pooling2)\n",
    "\n",
    "    #canal 3\n",
    "    input3=Input(shape=(max_len,))\n",
    "    embedding3=Embedding(tamanio_vocab,dimension_embedding)(input3)\n",
    "    convolution3=Conv1D(num_filtros,tamanio_kernel3,activation='relu')(embedding3)\n",
    "    pooling3=MaxPooling1D(pool_size=2)(convolution3)\n",
    "    flat3=Flatten()(pooling3)\n",
    "\n",
    "   #merge\n",
    "    merged=concatenate([flat1,flat2,flat3])\n",
    "    \n",
    "    #interpretacion\n",
    "    dense1=Dense(10,activation='relu')(merged)\n",
    "    output=Dense(1,activation='sigmoid')(dense1)\n",
    "    model=Model(inputs=[input1,input2,input3],outputs=[output])\n",
    "    \n",
    "    #compilar\n",
    "    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    \n",
    "    #resumen\n",
    "    print(model.summary())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_25 (InputLayer)           [(None, 31)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_26 (InputLayer)           [(None, 31)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_27 (InputLayer)           [(None, 31)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_24 (Embedding)        (None, 31, 100)      2740100     input_25[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_25 (Embedding)        (None, 31, 100)      2740100     input_26[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_26 (Embedding)        (None, 31, 100)      2740100     input_27[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_23 (Conv1D)              (None, 30, 32)       6432        embedding_24[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_24 (Conv1D)              (None, 29, 32)       9632        embedding_25[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_25 (Conv1D)              (None, 28, 32)       12832       embedding_26[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling1D) (None, 15, 32)       0           conv1d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling1D) (None, 14, 32)       0           conv1d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling1D) (None, 14, 32)       0           conv1d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_21 (Flatten)            (None, 480)          0           max_pooling1d_22[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_22 (Flatten)            (None, 448)          0           max_pooling1d_23[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_23 (Flatten)            (None, 448)          0           max_pooling1d_24[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 1376)         0           flatten_21[0][0]                 \n",
      "                                                                 flatten_22[0][0]                 \n",
      "                                                                 flatten_23[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 10)           13770       concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 1)            11          dense_12[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 8,262,977\n",
      "Trainable params: 8,262,977\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "modelo=crear_modelo(32,2,3,4,tamanio_de_vocabulario,dimension_embedding,longitud_maxima)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "6851/6851 [==============================] - 10s 1ms/sample - loss: 0.0096 - acc: 0.9953\n",
      "Epoch 2/15\n",
      "6851/6851 [==============================] - 10s 1ms/sample - loss: 0.0101 - acc: 0.9952\n",
      "Epoch 3/15\n",
      "6851/6851 [==============================] - 10s 1ms/sample - loss: 0.0094 - acc: 0.9956\n",
      "Epoch 4/15\n",
      "6851/6851 [==============================] - 10s 1ms/sample - loss: 0.0102 - acc: 0.9953\n",
      "Epoch 5/15\n",
      "6851/6851 [==============================] - 10s 1ms/sample - loss: 0.0092 - acc: 0.9958\n",
      "Epoch 6/15\n",
      "6851/6851 [==============================] - 10s 1ms/sample - loss: 0.0096 - acc: 0.9952\n",
      "Epoch 7/15\n",
      "6851/6851 [==============================] - 10s 1ms/sample - loss: 0.0088 - acc: 0.9956\n",
      "Epoch 8/15\n",
      "6851/6851 [==============================] - 10s 1ms/sample - loss: 0.0089 - acc: 0.9955\n",
      "Epoch 9/15\n",
      "6851/6851 [==============================] - 10s 1ms/sample - loss: 0.0083 - acc: 0.9956\n",
      "Epoch 10/15\n",
      "6851/6851 [==============================] - 10s 1ms/sample - loss: 0.0088 - acc: 0.9955\n",
      "Epoch 11/15\n",
      "6851/6851 [==============================] - 10s 1ms/sample - loss: 0.0089 - acc: 0.9952\n",
      "Epoch 12/15\n",
      "6851/6851 [==============================] - 10s 2ms/sample - loss: 0.0089 - acc: 0.9950\n",
      "Epoch 13/15\n",
      "6851/6851 [==============================] - 10s 2ms/sample - loss: 0.0082 - acc: 0.9956\n",
      "Epoch 14/15\n",
      "6851/6851 [==============================] - 10s 1ms/sample - loss: 0.0079 - acc: 0.9958\n",
      "Epoch 15/15\n",
      "6851/6851 [==============================] - 10s 1ms/sample - loss: 0.0085 - acc: 0.9950\n"
     ]
    }
   ],
   "source": [
    "history=modelo.fit([X_train_pad, X_train_pad, X_train_pad],Y_train,epochs=15,batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss,accuracy=modelo.evaluate([X_train_pad,X_train_pad,X_train_pad],Y_train,verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9968\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy =modelo.evaluate([X_para_evaluar,X_para_evaluar,X_para_evaluar], Y_para_evaluar, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy:  0.7953\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicciones=modelo.predict([X_test_pad,X_test_pad,X_test_pad])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultado=[]\n",
    "for numero in predicciones:\n",
    "    if numero>=0.5:\n",
    "        resultado.append(1)\n",
    "    else:\n",
    "        resultado.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleSubmission=pd.read_csv('sample_submission.csv')\n",
    "output=pd.DataFrame({'id':sampleSubmission.id,'target':resultado})\n",
    "output.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras import Model\n",
    "def crear_modelo(num_filtros,tamanio_kernel,tamanio_vocab,dimension_embedding,max_len):\n",
    "    \n",
    "    #red neuronal de 3 canales, representando 3 distintos n-gramas\n",
    "    #voy a usar n=2,3,5\n",
    "    #no se va a usar glove embeddings por la escasa \n",
    "    \n",
    "    #canal 1\n",
    "    input1=Input(shape=(max_len,))\n",
    "    embedding1=Embedding(tamanio_vocab,dimension_embedding)(input1)\n",
    "    convolution1=Conv1D(filters=32,kernel_size=2,activation='relu')(embedding1)\n",
    "    pooling1=MaxPooling1D(pool_size=2)(convolution1)\n",
    "    flat1=Flatten()(pooling1)\n",
    "\n",
    "    #canal 2\n",
    "    input2=Input(shape=(max_len,))\n",
    "    embedding2=Embedding(tamanio_vocab,dimension_embedding)(input2)\n",
    "    convolution2=Conv1D(filters=32,kernel_size=2,activation='relu')(embedding2)\n",
    "    pooling2=MaxPooling1D(pool_size=2)(convolution2)\n",
    "    flat2=Flatten()(pooling2)\n",
    "\n",
    "    #canal 3\n",
    "    input3=Input(shape=(max_len,))\n",
    "    embedding3=Embedding(tamanio_vocab,dimension_embedding)(input3)\n",
    "    convolution3=Conv1D(filters=32,kernel_size=2,activation='relu')(embedding3)\n",
    "    pooling3=MaxPooling1D(pool_size=2)(convolution3)\n",
    "    flat3=Flatten()(pooling3)\n",
    "\n",
    "   #merge\n",
    "    merged=concatenate([flat1,flat2,flat3])\n",
    "    \n",
    "    #interpretacion\n",
    "    dense1=Dense(10,activation='relu')(merged)\n",
    "    output=Dense(1,activation='sigmoid')(dense1)\n",
    "    model=Model(inputs=[input1,input2,input3],outputs=[output])\n",
    "    \n",
    "    #compilar\n",
    "    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    \n",
    "    #resumen\n",
    "    print(model.summary())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
