{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import sklearn as sk\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set=pd.read_csv(\"train.csv\",encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set=pd.read_csv(\"test.csv\",encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tomas/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/tomas/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/tomas/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/tomas/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/tomas/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/tomas/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/tomas/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/tomas/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/tomas/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/tomas/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/tomas/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/tomas/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape de X_train: (7613,)\n",
      "Shape de X_test:(3263,)\n",
      "Shape de todos_los_textos:(10876,)\n",
      "longitud maxima: 31\n",
      "tamanio de vocabulario: 29320\n",
      "WARNING:tensorflow:From /home/tomas/.local/lib/python3.6/site-packages/tensorflow/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/tomas/.local/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/tomas/.local/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 31, 100)           2932000   \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (None, 32)                12768     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 2,944,801\n",
      "Trainable params: 2,944,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Embedding,LSTM,GRU\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "#se hace un análisis del target usando NLP\n",
    "X_train=train_set['text'].copy().values\n",
    "Y_train=train_set['target'].copy().values\n",
    "X_test=test_set['text'].copy().values\n",
    "\n",
    "#pruebo usando un word embedding que yo entreno\n",
    "#práctica NO recomendada por la catedra, pero\n",
    "#quiero ver que resultados puedo conseguir con un\n",
    "#embedding mio. Hasta ahora el puntaje mio no \n",
    "#alcanza a 0.7\n",
    "print(\"Shape de X_train: \"+ str(X_train.shape))\n",
    "print(\"Shape de X_test:\" + str(X_test.shape))\n",
    "todos_los_textos=np.concatenate([X_train,X_test])\n",
    "print(\"Shape de todos_los_textos:\" + str(todos_los_textos.shape))\n",
    "\n",
    "#estan todos los textos concatenados, hay que 'entrenar' al tokenizador\n",
    "objeto_tokenizador=Tokenizer()\n",
    "objeto_tokenizador.fit_on_texts(todos_los_textos)\n",
    "\n",
    "#necesito una cota de la longitud de cada palabra de los textos que \n",
    "#se van a analizar\n",
    "longitud_maxima=max([len(s.split()) for s in todos_los_textos])\n",
    "print(\"longitud maxima: \" + str(longitud_maxima))\n",
    "\n",
    "#necesito saber cuantas palabras tengo en mi 'diccionario' de palabras\n",
    "tamanio_de_vocabulario=len(objeto_tokenizador.word_index) +1\n",
    "print(\"tamanio de vocabulario: \" + str(tamanio_de_vocabulario))\n",
    "\n",
    "#ahora que tengo esto, es tiempo de tokenizar cada uno de los tweets\n",
    "#y agregar el padding necesario\n",
    "X_train_tokens=objeto_tokenizador.texts_to_sequences(X_train)\n",
    "X_train_pad=pad_sequences(X_train_tokens,maxlen=longitud_maxima,padding='post')\n",
    "\n",
    "X_test_tokens=objeto_tokenizador.texts_to_sequences(X_test)\n",
    "X_test_pad=pad_sequences(X_test_tokens,maxlen=longitud_maxima,padding='post')\n",
    "\n",
    "#ahora hay que generar el modelo, se va a usar una red neuronal de 3 capas \n",
    "modelo=Sequential()\n",
    "modelo.add(Embedding(tamanio_de_vocabulario,100,input_length=longitud_maxima))\n",
    "modelo.add(GRU(units=32, dropout=0.2 ,recurrent_dropout=0.2))\n",
    "modelo.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "modelo.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
    "modelo.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_binary=to_categorical(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Epoch 1/25\n",
      "7613/7613 [==============================] - 7s 906us/sample - loss: 0.6839 - acc: 0.5702\n",
      "Epoch 2/25\n",
      "7613/7613 [==============================] - 5s 678us/sample - loss: 0.6835 - acc: 0.5706\n",
      "Epoch 3/25\n",
      "7613/7613 [==============================] - 5s 694us/sample - loss: 0.6084 - acc: 0.6643\n",
      "Epoch 4/25\n",
      "7613/7613 [==============================] - 5s 643us/sample - loss: 0.4003 - acc: 0.8518\n",
      "Epoch 5/25\n",
      "7613/7613 [==============================] - 5s 649us/sample - loss: 0.2541 - acc: 0.9158\n",
      "Epoch 6/25\n",
      "7613/7613 [==============================] - 5s 661us/sample - loss: 0.1696 - acc: 0.9468\n",
      "Epoch 7/25\n",
      "7613/7613 [==============================] - 5s 647us/sample - loss: 0.1043 - acc: 0.9712\n",
      "Epoch 8/25\n",
      "7613/7613 [==============================] - 5s 649us/sample - loss: 0.0596 - acc: 0.9849\n",
      "Epoch 9/25\n",
      "7613/7613 [==============================] - 5s 640us/sample - loss: 0.0419 - acc: 0.9904\n",
      "Epoch 10/25\n",
      "7613/7613 [==============================] - 5s 657us/sample - loss: 0.0347 - acc: 0.9908\n",
      "Epoch 11/25\n",
      "7613/7613 [==============================] - 5s 664us/sample - loss: 0.0255 - acc: 0.9937\n",
      "Epoch 12/25\n",
      "7613/7613 [==============================] - 5s 646us/sample - loss: 0.0265 - acc: 0.9930\n",
      "Epoch 13/25\n",
      "7613/7613 [==============================] - 5s 649us/sample - loss: 0.0243 - acc: 0.9942\n",
      "Epoch 14/25\n",
      "7613/7613 [==============================] - 5s 670us/sample - loss: 0.0218 - acc: 0.9953\n",
      "Epoch 15/25\n",
      "7613/7613 [==============================] - 5s 652us/sample - loss: 0.0215 - acc: 0.9947\n",
      "Epoch 16/25\n",
      "7613/7613 [==============================] - 5s 651us/sample - loss: 0.0212 - acc: 0.9941\n",
      "Epoch 17/25\n",
      "7613/7613 [==============================] - 5s 650us/sample - loss: 0.0187 - acc: 0.9945\n",
      "Epoch 18/25\n",
      "7613/7613 [==============================] - 5s 704us/sample - loss: 0.0181 - acc: 0.9957\n",
      "Epoch 19/25\n",
      "7613/7613 [==============================] - 5s 652us/sample - loss: 0.0180 - acc: 0.9949\n",
      "Epoch 20/25\n",
      "7613/7613 [==============================] - 5s 663us/sample - loss: 0.0137 - acc: 0.9954\n",
      "Epoch 21/25\n",
      "7613/7613 [==============================] - 5s 640us/sample - loss: 0.0158 - acc: 0.9950\n",
      "Epoch 22/25\n",
      "7613/7613 [==============================] - 5s 651us/sample - loss: 0.0117 - acc: 0.9961\n",
      "Epoch 23/25\n",
      "7613/7613 [==============================] - 6s 726us/sample - loss: 0.0126 - acc: 0.9958\n",
      "Epoch 24/25\n",
      "7613/7613 [==============================] - 5s 661us/sample - loss: 0.0119 - acc: 0.9950\n",
      "Epoch 25/25\n",
      "7613/7613 [==============================] - 5s 638us/sample - loss: 0.0132 - acc: 0.9945\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f8a68745c88>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#se entrena el modelo:\n",
    "print(\"Train...\")\n",
    "modelo.fit(X_train_pad,Y_train,batch_size=128,epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ahora se hace la prediccion\n",
    "predicciones=modelo.predict(X_test_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3263, 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicciones.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dado que no es un modelo recomendado por la catedra, voy a asumir que las predicciones\n",
    "#son muy poco confiables, entonces solo voy a asignarle si son verdaderos los tweets\n",
    "#si el modelo cree muy fuertemente en su predicción, caso contrario, lo considero falso\n",
    "lista=[]\n",
    "for elemento in predicciones:\n",
    "    if(elemento>=0.95):\n",
    "        lista.append(1)\n",
    "    else:\n",
    "        lista.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3263"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lista)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleSubmission=pd.read_csv('sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "output=pd.DataFrame({'id':sampleSubmission.id,'target':lista})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
