{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tomas/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/tomas/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/tomas/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/tomas/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/tomas/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/tomas/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/tomas/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/tomas/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/tomas/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/tomas/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/tomas/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/tomas/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import sklearn as sk\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from time import time\n",
    "from collections import defaultdict\n",
    "from tensorflow import keras\n",
    "\n",
    "path_glove='/home/tomas/organizacion_de_datos/tp2/glove.6B.100d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Embedding,LSTM,GRU\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def a_string(texto):\n",
    "    return str(texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set=pd.read_csv(\"train_set_featurizado.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set['text']=train_set['text'].apply(a_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set=pd.read_csv(\"test_data_featurizada.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set['text']=test_set['text'].apply(a_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenización del texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Embedding,LSTM,GRU\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_copy=train_set.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y=train_set['target'].copy().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr,X_te,Y_tr,Y_te=train_test_split(X_copy,Y,test_size=0.1,random_state=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape de X_train: (6851,)\n",
      "Shape de X_test:(3263,)\n",
      "Shape de todos_los_textos:(10114,)\n",
      "longitud maxima: 24\n",
      "tamanio de vocabulario: 17167\n"
     ]
    }
   ],
   "source": [
    "Y_para_entrenar=Y_tr\n",
    "X_para_entrenar=X_tr['text'].values\n",
    "\n",
    "X_para_evaluar=X_te['text'].values\n",
    "\n",
    "#se hace un análisis del target usando NLP\n",
    "X_train=X_para_entrenar\n",
    "Y_train=Y_para_entrenar\n",
    "X_test=test_set['text'].copy().values\n",
    "\n",
    "print(\"Shape de X_train: \"+ str(X_train.shape))\n",
    "print(\"Shape de X_test:\" + str(X_test.shape))\n",
    "todos_los_textos=np.concatenate([X_train,X_test])\n",
    "print(\"Shape de todos_los_textos:\" + str(todos_los_textos.shape))\n",
    "\n",
    "#estan todos los textos concatenados, hay que 'entrenar' al tokenizador\n",
    "objeto_tokenizador=Tokenizer()\n",
    "objeto_tokenizador.fit_on_texts(todos_los_textos)\n",
    "\n",
    "#necesito una cota de la longitud de cada palabra de los textos que \n",
    "#se van a analizar\n",
    "longitud_maxima=max([len(s.split()) for s in todos_los_textos])\n",
    "print(\"longitud maxima: \" + str(longitud_maxima))\n",
    "\n",
    "#necesito saber cuantas palabras tengo en mi 'diccionario' de palabras\n",
    "tamanio_de_vocabulario=len(objeto_tokenizador.word_index) +1\n",
    "print(\"tamanio de vocabulario: \" + str(tamanio_de_vocabulario))\n",
    "\n",
    "#ahora que tengo esto, es tiempo de tokenizar cada uno de los tweets\n",
    "#y agregar el padding necesario\n",
    "X_train_tokens=objeto_tokenizador.texts_to_sequences(X_train)\n",
    "X_train_pad=pad_sequences(X_train_tokens,maxlen=longitud_maxima,padding='post')\n",
    "\n",
    "X_test_tokens=objeto_tokenizador.texts_to_sequences(X_test)\n",
    "X_test_pad=pad_sequences(X_test_tokens,maxlen=longitud_maxima,padding='post')\n",
    "\n",
    "X_para_evaluar=pad_sequences(objeto_tokenizador.texts_to_sequences(X_para_evaluar),maxlen=longitud_maxima,padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_matrix(filepath, word_index, embedding_dim):\n",
    "    vocab_size = len(word_index) + 1  # Adding again 1 because of reserved 0 index\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "    with open(filepath) as f:\n",
    "        for line in f:\n",
    "            word, *vector = line.split()\n",
    "            if word in word_index:\n",
    "                idx = word_index[word] \n",
    "                embedding_matrix[idx] = np.array(\n",
    "                    vector, dtype=np.float32)[:embedding_dim]\n",
    "\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6375021844236034\n"
     ]
    }
   ],
   "source": [
    "dimension_embedding=100\n",
    "embedding_matrix = create_embedding_matrix(path_glove,objeto_tokenizador.word_index, dimension_embedding)\n",
    "\n",
    "\n",
    "nonzero_elements = np.count_nonzero(np.count_nonzero(embedding_matrix, axis=1))\n",
    "print(nonzero_elements/tamanio_de_vocabulario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras import Model\n",
    "def crear_modelo(num_filtros,tamanio_kernel1,tamanio_kernel2,tamanio_kernel3,tamanio_vocab,dimension_embedding,max_len):\n",
    "    \n",
    "    #red neuronal de 3 canales, representando 3 distintos n-gramas\n",
    "    #voy a usar n=2,3,5\n",
    "    #no se va a usar glove embeddings por la escasa \n",
    "    \n",
    "    #canal 1\n",
    "    input1=Input(shape=(max_len,))\n",
    "    embedding1=Embedding(tamanio_vocab,dimension_embedding)(input1)\n",
    "    convolution1=Conv1D(num_filtros,tamanio_kernel1,activation='relu')(embedding1)\n",
    "    pooling1=MaxPooling1D(pool_size=2)(convolution1)\n",
    "    flat1=Flatten()(pooling1)\n",
    "\n",
    "    #canal 2\n",
    "    input2=Input(shape=(max_len,))\n",
    "    embedding2=Embedding(tamanio_vocab,dimension_embedding)(input2)\n",
    "    convolution2=Conv1D(num_filtros,tamanio_kernel2,activation='relu')(embedding2)\n",
    "    pooling2=MaxPooling1D(pool_size=2)(convolution2)\n",
    "    flat2=Flatten()(pooling2)\n",
    "\n",
    "    #canal 3\n",
    "    input3=Input(shape=(max_len,))\n",
    "    embedding3=Embedding(tamanio_vocab,dimension_embedding)(input3)\n",
    "    convolution3=Conv1D(num_filtros,tamanio_kernel3,activation='relu')(embedding3)\n",
    "    pooling3=MaxPooling1D(pool_size=2)(convolution3)\n",
    "    flat3=Flatten()(pooling3)\n",
    "\n",
    "   #merge\n",
    "    merged=concatenate([flat1,flat2,flat3])\n",
    "    \n",
    "    #interpretacion\n",
    "    dense1=Dense(10,activation='relu')(merged)\n",
    "    output=Dense(1,activation='sigmoid')(dense1)\n",
    "    model=Model(inputs=[input1,input2,input3],outputs=[output])\n",
    "    \n",
    "    #compilar\n",
    "    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    \n",
    "    #resumen\n",
    "    print(model.summary())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/tomas/.local/lib/python3.6/site-packages/tensorflow/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/tomas/.local/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/tomas/.local/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 24)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 24)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 24)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 24, 100)      1716700     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 24, 100)      1716700     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 24, 100)      1716700     input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 22, 64)       19264       embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 20, 64)       32064       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 20, 64)       32064       embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, 11, 64)       0           conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 10, 64)       0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 10, 64)       0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 704)          0           max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 640)          0           max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 640)          0           max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 1984)         0           flatten[0][0]                    \n",
      "                                                                 flatten_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 10)           19850       concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            11          dense[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 5,253,353\n",
      "Trainable params: 5,253,353\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "modelo=crear_modelo(64,3,5,5,tamanio_de_vocabulario,dimension_embedding,longitud_maxima)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "6851/6851 [==============================] - 9s 1ms/sample - loss: 0.6124 - acc: 0.6685\n",
      "Epoch 2/15\n",
      "6851/6851 [==============================] - 8s 1ms/sample - loss: 0.3151 - acc: 0.8702\n",
      "Epoch 3/15\n",
      "6851/6851 [==============================] - 8s 1ms/sample - loss: 0.1411 - acc: 0.9510\n",
      "Epoch 4/15\n",
      "6851/6851 [==============================] - 11s 2ms/sample - loss: 0.0864 - acc: 0.9718\n",
      "Epoch 5/15\n",
      "6851/6851 [==============================] - 11s 2ms/sample - loss: 0.0655 - acc: 0.9756\n",
      "Epoch 6/15\n",
      "6851/6851 [==============================] - 12s 2ms/sample - loss: 0.0602 - acc: 0.9793\n",
      "Epoch 7/15\n",
      "6851/6851 [==============================] - 12s 2ms/sample - loss: 0.0540 - acc: 0.9796\n",
      "Epoch 8/15\n",
      "6851/6851 [==============================] - 11s 2ms/sample - loss: 0.0479 - acc: 0.9801\n",
      "Epoch 9/15\n",
      "6851/6851 [==============================] - 10s 1ms/sample - loss: 0.0468 - acc: 0.9806\n",
      "Epoch 10/15\n",
      "6851/6851 [==============================] - 10s 1ms/sample - loss: 0.0472 - acc: 0.9797\n",
      "Epoch 11/15\n",
      "6851/6851 [==============================] - 9s 1ms/sample - loss: 0.0399 - acc: 0.9826\n",
      "Epoch 12/15\n",
      "6851/6851 [==============================] - 9s 1ms/sample - loss: 0.0432 - acc: 0.9794\n",
      "Epoch 13/15\n",
      "6851/6851 [==============================] - 8s 1ms/sample - loss: 0.0396 - acc: 0.9810\n",
      "Epoch 14/15\n",
      "6851/6851 [==============================] - 8s 1ms/sample - loss: 0.0388 - acc: 0.9813\n",
      "Epoch 15/15\n",
      "6851/6851 [==============================] - 9s 1ms/sample - loss: 0.0371 - acc: 0.9826\n"
     ]
    }
   ],
   "source": [
    "history=modelo.fit([X_train_pad, X_train_pad, X_train_pad],Y_train,epochs=15,batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss,accuracy=modelo.evaluate([X_train_pad,X_train_pad,X_train_pad],Y_train,verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9657\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy =modelo.evaluate([X_para_evaluar,X_para_evaluar,X_para_evaluar], Y_para_evaluar, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy:  0.7546\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicciones=modelo.predict([X_test_pad,X_test_pad,X_test_pad])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultado=[]\n",
    "for numero in predicciones:\n",
    "    if numero>=0.5:\n",
    "        resultado.append(1)\n",
    "    else:\n",
    "        resultado.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleSubmission=pd.read_csv('sample_submission.csv')\n",
    "output=pd.DataFrame({'id':sampleSubmission.id,'target':resultado})\n",
    "output.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#settings:\n",
    "epochs=20\n",
    "#longitud_maxima (calculada antes)\n",
    "#embedding dimension(calculada antes)\n",
    "output_file='output.txt'\n",
    "\n",
    "modelo=KerasClassifier(build_fn=crear_modelo,epochs=epochs,batch_size=10,verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid=dict(num_filtros=[32,64,128],tamanio_kernel1=[3,5,7],tamanio_kernel2=[1,2,3],tamanio_kernel3=[4,6,5],tamanio_vocab=[tamanio_de_vocabulario],dimension_embedding=[dimension_embedding],max_len=[longitud_maxima])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid=RandomizedSearchCV(estimator=modelo, param_distributions=param_grid,cv=4,verbose=1,n_iter=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras import Model\n",
    "def crear_modelo(num_filtros,tamanio_kernel,tamanio_vocab,dimension_embedding,max_len):\n",
    "    \n",
    "    #red neuronal de 3 canales, representando 3 distintos n-gramas\n",
    "    #voy a usar n=2,3,5\n",
    "    #no se va a usar glove embeddings por la escasa \n",
    "    \n",
    "    #canal 1\n",
    "    input1=Input(shape=(max_len,))\n",
    "    embedding1=Embedding(tamanio_vocab,dimension_embedding)(input1)\n",
    "    convolution1=Conv1D(filters=32,kernel_size=2,activation='relu')(embedding1)\n",
    "    pooling1=MaxPooling1D(pool_size=2)(convolution1)\n",
    "    flat1=Flatten()(pooling1)\n",
    "\n",
    "    #canal 2\n",
    "    input2=Input(shape=(max_len,))\n",
    "    embedding2=Embedding(tamanio_vocab,dimension_embedding)(input2)\n",
    "    convolution2=Conv1D(filters=32,kernel_size=2,activation='relu')(embedding2)\n",
    "    pooling2=MaxPooling1D(pool_size=2)(convolution2)\n",
    "    flat2=Flatten()(pooling2)\n",
    "\n",
    "    #canal 3\n",
    "    input3=Input(shape=(max_len,))\n",
    "    embedding3=Embedding(tamanio_vocab,dimension_embedding)(input3)\n",
    "    convolution3=Conv1D(filters=32,kernel_size=2,activation='relu')(embedding3)\n",
    "    pooling3=MaxPooling1D(pool_size=2)(convolution3)\n",
    "    flat3=Flatten()(pooling3)\n",
    "\n",
    "   #merge\n",
    "    merged=concatenate([flat1,flat2,flat3])\n",
    "    \n",
    "    #interpretacion\n",
    "    dense1=Dense(10,activation='relu')(merged)\n",
    "    output=Dense(1,activation='sigmoid')(dense1)\n",
    "    model=Model(inputs=[input1,input2,input3],outputs=[output])\n",
    "    \n",
    "    #compilar\n",
    "    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    \n",
    "    #resumen\n",
    "    print(model.summary())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>caracteres_usados</th>\n",
       "      <th>menciones_realizadas</th>\n",
       "      <th>permite_location</th>\n",
       "      <th>use_keyword</th>\n",
       "      <th>cita_url</th>\n",
       "      <th>use_hashtag</th>\n",
       "      <th>numero_de_palabras</th>\n",
       "      <th>palabras_unicas</th>\n",
       "      <th>numero_palabras_desconocidas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>deed reason earthquake may allah forgive</td>\n",
       "      <td>1</td>\n",
       "      <td>69</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>resident ask shelter place notify officer evac...</td>\n",
       "      <td>1</td>\n",
       "      <td>133</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>people receive wildfire evacuation order calif...</td>\n",
       "      <td>1</td>\n",
       "      <td>65</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>get sent photo ruby alaska smoke wildfire pour...</td>\n",
       "      <td>1</td>\n",
       "      <td>88</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN           deed reason earthquake may allah forgive   \n",
       "1   4     NaN      NaN              forest fire near la ronge sask canada   \n",
       "2   5     NaN      NaN  resident ask shelter place notify officer evac...   \n",
       "3   6     NaN      NaN  people receive wildfire evacuation order calif...   \n",
       "4   7     NaN      NaN  get sent photo ruby alaska smoke wildfire pour...   \n",
       "\n",
       "   target  caracteres_usados  menciones_realizadas  permite_location  \\\n",
       "0       1                 69                     0                 0   \n",
       "1       1                 38                     0                 0   \n",
       "2       1                133                     0                 0   \n",
       "3       1                 65                     0                 0   \n",
       "4       1                 88                     0                 0   \n",
       "\n",
       "   use_keyword  cita_url  use_hashtag  numero_de_palabras  palabras_unicas  \\\n",
       "0            0         0            1                   6                6   \n",
       "1            0         0            0                   7                7   \n",
       "2            0         0            0                  11                9   \n",
       "3            0         0            1                   6                6   \n",
       "4            0         0            2                   9                9   \n",
       "\n",
       "   numero_palabras_desconocidas  \n",
       "0                             0  \n",
       "1                             0  \n",
       "2                             0  \n",
       "3                             0  \n",
       "4                             0  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import MaxPooling1D\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "def crear_modelo(num_filtro,tamanio_kernel,tamanio_vocab,dimension_embedding,max_len):\n",
    "    \n",
    "    #canal 1 texto\n",
    "    input1=Input(shape=(max_len,))\n",
    "    embedding1=Embedding(tamanio_vocab,dimension_embedding)(input1)\n",
    "    convolution1=Conv1D(num_filtro,tamanio_kernel,activation='relu')(embedding1)\n",
    "    pooling1=MaxPooling1D(pool_size=2)(convolution1)\n",
    "    flat1=Flatten()(pooling1)\n",
    "    \n",
    "    #canal 2  caracteres usados\n",
    "    input2=Input(shape=(1,))\n",
    "    dense_2_1=Dense(50,activation='relu')(input2)\n",
    "    dense_2_2=Dense(5,activation='relu')(dense_2_1)\n",
    "    \n",
    "    #canal3 menciones realizadas\n",
    "    input3=Input(shape=(1,))\n",
    "    dense_3_1=Dense(50,activation='relu')(input3)\n",
    "    dense_3_2=Dense(5,activation='relu')(dense_3_1)\n",
    "    \n",
    "    #canal 4 numero de palabras en el tweet\n",
    "    input4=Input(shape=(1,))\n",
    "    dense_4_1=Dense(50,activation='relu')(input4)\n",
    "    dense_4_2=Dense(5,activation='relu')(dense_4_1)\n",
    "    \n",
    "    #canal 5 numero de palabras únicas en el tweet\n",
    "    input5=Input(shape=(1,))\n",
    "    dense_5_1=Dense(50,activation='relu')(input5)\n",
    "    dense_5_2=Dense(5,activation='relu')(dense_5_1)\n",
    "        \n",
    "    #canal 6 numero de palabras desconocidas en el tweet\n",
    "    input6=Input(shape=(1,))\n",
    "    dense_6_1=Dense(50,activation='relu')(input6)\n",
    "    dense_6_2=Dense(5,activation='relu')(dense_6_1)\n",
    "    \n",
    "    #concatenate layer \n",
    "    merged=concatenate([flat1,dense_2_2,dense_3_2,dense_4_2,dense_5_2,dense_6_2])  \n",
    "    dense_final=Dense(20,activation='relu')(merged)\n",
    "    output=Dense(1,activation='sigmoid')(dense_final)\n",
    "    \n",
    "    model=Model(inputs=[input1,input2,input3,input4,input5,input6],outputs=[output])\n",
    "    \n",
    "    #compilar\n",
    "    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    print(modelo.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_16 (InputLayer)           [(None, 24)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, 24, 100)      1716700     input_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 21, 32)       12832       embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "input_17 (InputLayer)           [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_18 (InputLayer)           [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_19 (InputLayer)           [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_20 (InputLayer)           [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_21 (InputLayer)           [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 10, 32)       0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_26 (Dense)                (None, 50)           100         input_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_28 (Dense)                (None, 50)           100         input_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_30 (Dense)                (None, 50)           100         input_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_32 (Dense)                (None, 50)           100         input_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_34 (Dense)                (None, 50)           100         input_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 320)          0           max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_27 (Dense)                (None, 5)            255         dense_26[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_29 (Dense)                (None, 5)            255         dense_28[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_31 (Dense)                (None, 5)            255         dense_30[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_33 (Dense)                (None, 5)            255         dense_32[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_35 (Dense)                (None, 5)            255         dense_34[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 345)          0           flatten_5[0][0]                  \n",
      "                                                                 dense_27[0][0]                   \n",
      "                                                                 dense_29[0][0]                   \n",
      "                                                                 dense_31[0][0]                   \n",
      "                                                                 dense_33[0][0]                   \n",
      "                                                                 dense_35[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_36 (Dense)                (None, 20)           6920        concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_37 (Dense)                (None, 1)            21          dense_36[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 1,738,248\n",
      "Trainable params: 1,738,248\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "modelo=crear_modelo(32,4,tamanio_de_vocabulario,dimension_embedding,longitud_maxima)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "caracteres_train=X_tr['caracteres_usados']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "menciones_train=X_tr['menciones_realizadas']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'Unnamed: 0.1', 'id', 'keyword', 'location', 'text',\n",
       "       'target', 'caracteres_usados', 'menciones_realizadas',\n",
       "       'permite_location', 'use_keyword', 'cita_url', 'use_hashtag',\n",
       "       'numero_de_palabras', 'palabras_unicas',\n",
       "       'numero_palabras_desconocidas'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_train=X_tr['permite_location']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_de_palabras_train=X_tr['numero_de_palabras']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_de_palabras_unicas_train=X_tr['palabras_unicas']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_de_palabras_desconocidas_train=X_tr['numero_palabras_desconocidas']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "6851/6851 [==============================] - 5s 724us/sample - loss: 0.6577 - acc: 0.6250\n",
      "Epoch 2/20\n",
      "6851/6851 [==============================] - 3s 492us/sample - loss: 0.3934 - acc: 0.8400\n",
      "Epoch 3/20\n",
      "6851/6851 [==============================] - 3s 472us/sample - loss: 0.2083 - acc: 0.9245\n",
      "Epoch 4/20\n",
      "6851/6851 [==============================] - 3s 450us/sample - loss: 0.1199 - acc: 0.9584\n",
      "Epoch 5/20\n",
      "6851/6851 [==============================] - 3s 490us/sample - loss: 0.0831 - acc: 0.9712\n",
      "Epoch 6/20\n",
      "6851/6851 [==============================] - 3s 470us/sample - loss: 0.0653 - acc: 0.9781\n",
      "Epoch 7/20\n",
      "6851/6851 [==============================] - 3s 486us/sample - loss: 0.0560 - acc: 0.9785\n",
      "Epoch 8/20\n",
      "6851/6851 [==============================] - 4s 533us/sample - loss: 0.0505 - acc: 0.9800\n",
      "Epoch 9/20\n",
      "6851/6851 [==============================] - 3s 399us/sample - loss: 0.0471 - acc: 0.9799\n",
      "Epoch 10/20\n",
      "6851/6851 [==============================] - 3s 403us/sample - loss: 0.0444 - acc: 0.9810\n",
      "Epoch 11/20\n",
      "6851/6851 [==============================] - 3s 401us/sample - loss: 0.0418 - acc: 0.9819\n",
      "Epoch 12/20\n",
      "6851/6851 [==============================] - 3s 373us/sample - loss: 0.0376 - acc: 0.9822\n",
      "Epoch 13/20\n",
      "6851/6851 [==============================] - 3s 411us/sample - loss: 0.0411 - acc: 0.9812\n",
      "Epoch 14/20\n",
      "6851/6851 [==============================] - 3s 432us/sample - loss: 0.0393 - acc: 0.9819\n",
      "Epoch 15/20\n",
      "6851/6851 [==============================] - 3s 448us/sample - loss: 0.0394 - acc: 0.9812\n",
      "Epoch 16/20\n",
      "6851/6851 [==============================] - 3s 425us/sample - loss: 0.0396 - acc: 0.9800\n",
      "Epoch 17/20\n",
      "6851/6851 [==============================] - 4s 532us/sample - loss: 0.0388 - acc: 0.9828\n",
      "Epoch 18/20\n",
      "6851/6851 [==============================] - 5s 693us/sample - loss: 0.0359 - acc: 0.9828\n",
      "Epoch 19/20\n",
      "6851/6851 [==============================] - 4s 650us/sample - loss: 0.0356 - acc: 0.9823\n",
      "Epoch 20/20\n",
      "6851/6851 [==============================] - 4s 597us/sample - loss: 0.0351 - acc: 0.9826\n"
     ]
    }
   ],
   "source": [
    "history=modelo.fit([X_train_pad,caracteres_train,menciones_train,location_train,num_de_palabras_train,num_de_palabras_unicas_train,num_de_palabras_desconocidas_train],Y_train,epochs=20,batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss,accuracy=modelo.evaluate([X_train_pad,caracteres_train,menciones_train,location_train,num_de_palabras_train,num_de_palabras_unicas_train,num_de_palabras_desconocidas_train],Y_train,verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9853\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "segundo_input=X_te['caracteres_usados']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "tercer_input=X_te['menciones_realizadas']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuarto_input=X_te['permite_location']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "quinto_input=X_te['numero_de_palabras']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "sexto_input=X_te['palabras_unicas']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "septimo_input=X_te['numero_palabras_desconocidas']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss,accuracy=modelo.evaluate([X_para_evaluar,segundo_input,tercer_input,cuarto_input,quinto_input,sexto_input,septimo_input],Y_te,verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.7598\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing Accuracy: {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
