{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Red neuronal profunda usando word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import sklearn as sk\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from time import time\n",
    "from collections import defaultdict\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tomas/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/tomas/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/tomas/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/tomas/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/tomas/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/tomas/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/tomas/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/tomas/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/tomas/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/tomas/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/tomas/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/tomas/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Embedding,LSTM,GRU\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/tomas/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/tomas/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/tomas/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_stopwords=set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set=pd.read_csv(\"train.csv\",encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set=pd.read_csv(\"test.csv\",encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Procesamiento del texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quitar_stopwords(texto):\n",
    "    texto=' '.join([word for word in texto.split() if word not in english_stopwords])\n",
    "    return texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quitar_menciones(texto):\n",
    "    texto=' '.join([palabra for palabra in texto.split() if '@' not in palabra])\n",
    "    return texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Diccionario_de_lenguaje_de_internet={\n",
    "\"2day\": \"Today\",\n",
    "\"2moro\": \"Tomorrow\",\n",
    "\"2nite\": \"Tonight\",\n",
    "\"4EAE\": \"For Ever And Ever\",\n",
    "\"ABT\":\"About\",\n",
    "\"ADN\": \"Any Day Now\",\n",
    "\"AFAIC\": \"As Far As I’m Concerned\",\n",
    "\"AFAICT\": \"As Far As I Can Tell\",\n",
    "\"AFAIK\": \"As Far As I Know\",\n",
    "\"AFAIR\": \"As Far As I Remember\",\n",
    "\"AKA\": \"Also Known As\",\n",
    "\"AMA\": \"Ask Me Anything\",\n",
    "\"ASAIC\": \"As Soon As I Can\",\n",
    "\"ASAP\": \"As Soon As Possible\",\n",
    "\"ATM\": \"At The Moment\",\n",
    "\"B4\": \"Before\",\n",
    "\"B4N\": \"Bye For Now\",\n",
    "\"Bae\": \"Babe/Before Anyone Else\",\n",
    "\"BBL\": \"Be Back Later\",\n",
    "\"BBT\": \"Be Back Tomorrow\",\n",
    "\"BCNU\": \"Be Seeing You\",\n",
    "\"BD\": \"Big Deal\",\n",
    "\"BF\": \"Boyfriend\",\n",
    "\"BFF\": \"Best Friends Forever\",\n",
    "\"BMT\": \"Before My Time\",\n",
    "\"BOL\": \"Be On Later\",\n",
    "\"BOT\": \"Back On Topic\",\n",
    "\"BRB\": \"Be Right Back\",\n",
    "\"BRO\": \"Brother\",\n",
    "\"BT\": \"But\",\n",
    "\"BTW\": \"By The Way\",\n",
    "\"CFY\": \"Calling For You\",\n",
    "\"CU\": \"See You\",\n",
    "\"CUL\": \"See You Later\",\n",
    "\"Cuz\": \"Because\",\n",
    "\"CYA\": \"Cover Your Ass\",\n",
    "\"DAE\": \"Does Anyone Else\",\n",
    "\"DBA\": \"Doing Business As\",\n",
    "\"DFTBA\": \"Don’t Forget To Be Awesome\",\n",
    "\"DIKU\": \"Do I Know You\",\n",
    "\"DM\": \"Direct Message\",\n",
    "\"DND\": \"Do Not Disturb\",\n",
    "\"DR\": \"Double Rainbow\",\n",
    "\"DWBH\": \"Don’t Worry, Be Happy\",\n",
    "\"ELI5\": \"Explain Like I’m 5\",\n",
    "\"EOM\": \"End Of Message\",\n",
    "\"EOS\": \"End Of Story\",\n",
    "\"F2F\": \"Face To Face\",\n",
    "\"FAQ\": \"Frequently Asked Question\",\n",
    "\"FB\": \"Facebook\",\n",
    "\"FBF\": \"Flash Back Friday\",\n",
    "\"FF\": \"Follow Friday\",\n",
    "\"FIFY\": \"Fixed It For You\",\n",
    "\"FITB\": \"Fill In The Blank\",\n",
    "\"FML\": \"Fuck My Life\",\n",
    "\"FOMO\": \"Fear Of Missing Out\",\n",
    "\"FTFY\": \"Fixed That For You\",\n",
    "\"FTL\": \"For The Loss\",\n",
    "\"FTW\": \"For The Win\",\n",
    "\"FWB\": \"Friends With Benefits\",\n",
    "\"FWIW\": \"For What It’s Worth\",\n",
    "\"FYE\": \"For Your Entertainment\",\n",
    "\"FYEO\": \"For Your Eyes Only\",\n",
    "\"FYI\": \"For Your Information\",\n",
    "\"GA\": \"Go Ahead\",\n",
    "\"GAL\": \"Get A Life\",\n",
    "\"GF\": \"Girlfriend\",\n",
    "\"GM\": \"Good Morning\",\n",
    "\"GN\": \"Good Night\",\n",
    "\"Gr8\": \"Great\",\n",
    "\"GTR\": \"Getting Ready\",\n",
    "\"HAND\": \"Have A Nice Day\",\n",
    "\"HB\": \"Hurry Back\",\n",
    "\"HBD\": \"Happy Birthday\",\n",
    "\"HBU\": \"How About You\",\n",
    "\"HMB\": \"Hit Be Back\",\n",
    "\"HMU\": \"Hit Me Up\",\n",
    "\"HRU\": \"How Are You\",\n",
    "\"HTH\": \"Hope This Helps\",\n",
    "\"IAC\": \"In Any Case\",\n",
    "\"IC\": \"I See\",\n",
    "\"ICYMI\": \"In Case You Missed It\",\n",
    "\"IDC\": \"I Don’t Care\",\n",
    "\"IDK\": \"I Don’t Know\",\n",
    "\"IG\": \"Instagram\",\n",
    "\"IIRC\": \"If I Remember Correctly\",\n",
    "\"IKR\": \"I Know Right\",\n",
    "\"ILY\": \"I Love You\",\n",
    "\"IMHO\": \"In My Humble Opinion\",\n",
    "\"IMMD\": \"It Made My Day\",\n",
    "\"IMY\": \"I Miss You\",\n",
    "\"IRL\": \"In Real Life\",\n",
    "\"IS\": \"I’m Sorry\",\n",
    "\"ISO\": \"In Search Of\",\n",
    "\"IU2U\": \"It’s Up To You\",\n",
    "\"J4F\": \"Just For Fun\",\n",
    "\"JAM\": \"Just A Minute\",\n",
    "\"JFY\": \"Just For You\",\n",
    "\"JIC\": \"Just In Case\",\n",
    "\"JK\": \"Just Kidding\",\n",
    "\"JSYK\": \"Just So You Know\",\n",
    "\"KK\": \"Okay\",\n",
    "\"L8\": \"Late\",\n",
    "\"L8R\": \"Later\",\n",
    "\"LMA\": \"Leave Me Alone\",\n",
    "\"LMAO\": \"Laughing My Ass Off\",\n",
    "\"LMBO\": \"Laughing My Butt Off\",\n",
    "\"LMK\": \"Let Me Know\",\n",
    "\"LOL\": \"Laugh Out Loud\",\n",
    "\"LTNS\": \"Long Time No See\",\n",
    "\"LYLAS\": \"Love You Like A Sister\",\n",
    "\"M/F\": \"Male or Female\",\n",
    "\"M8\": \"Mate\",\n",
    "\"MP\": \"My pleasure\",\n",
    "\"MSM\": \"Mainstream Media\",\n",
    "\"MU\": \"Miss You\",\n",
    "\"MYOB\": \"Mind Your Own Business\",\n",
    "\"NAGI\": \"Not A Good Idea\",\n",
    "\"NBD\": \"No Big Deal\",\n",
    "\"NE1\": \"Anyone\",\n",
    "\"NM\": \"Not Much\",\n",
    "\"NP\": \"No Problem\",\n",
    "\"NSFL\": \"Not Safe For Life\",\n",
    "\"NSFW\": \"Not Safe For Work\",\n",
    "\"NTS\": \"Note To Self\",\n",
    "\"NVM\": \"Never Mind\",\n",
    "\"OC\": \"Original Content\",\n",
    "\"OH\": \"Overheard\",\n",
    "\"OIC\": \"Oh ! I See\",\n",
    "\"OMD\": \"Oh My Damn\",\n",
    "\"OMG\": \"Oh My Goodness\",\n",
    "\"OMW\": \"On My Way\",\n",
    "\"OT\": \"Off Topic\",\n",
    "\"OFC\": \"Of course\",\n",
    "\"PAW\": \"Parents Are Watching\",\n",
    "\"Pls\": \"Please\",\n",
    "\"POTD\": \"Photo Of The Day\",\n",
    "\"POV\": \"Point Of View\",\n",
    "\"PPL\": \"People\",\n",
    "\"PTB\": \"Please Text Back\",\n",
    "\"Q4U\": \"Question For You\",\n",
    "\"QQ\": \"Crying\",\n",
    "\"RBTL\": \"Read Between The Lines\",\n",
    "\"RIP\": \"Rest In Peace\",\n",
    "\"RL\": \"Real Life\",\n",
    "\"ROFL\": \"Rolling On the Floor Laughing\",\n",
    "\"RT\": \"Retweet\",\n",
    "\"RTM\": \"Read The Manual\",\n",
    "\"SIS\": \"Sister\",\n",
    "\"SITD\": \"Still In The Dark\",\n",
    "\"SM\": \"Social Media\",\n",
    "\"SMH\": \"Shaking My Head\",\n",
    "\"SMY\": \"Somebody\",\n",
    "\"SNH\": \"Sarcasm Noted Here\",\n",
    "\"SOL\": \"Sooner Or Later\",\n",
    "\"Some1\": \"Someone\",\n",
    "\"SRSLY\": \"Seriously\",\n",
    "\"STBY\": \"Sucks To Be You\",\n",
    "\"Str8\": \"Straight\",\n",
    "\"SYS\": \"See You Soon\",\n",
    "\"TBA\": \"To Be Announced\",\n",
    "\"TBH\": \"To Be Honest\",\n",
    "\"TBT\": \"Throwback Thursday\",\n",
    "\"TBT\": \"Truth Be Told\",\n",
    "\"TFH\": \"Thread From Hell\",\n",
    "\"TFTI\": \"Thanks For The Invite\",\n",
    "\"TGIF\": \"Thank God It’s Friday\",\n",
    "\"THX\": \"Thanks\",\n",
    "\"TIA\": \"Thanks in Advance\",\n",
    "\"TIL\": \"Today I Learned\",\n",
    "\"TIME\": \"Tears In My Eyes\",\n",
    "\"TL;DR\": \"Too Long; Didn’t Read\",\n",
    "\"TLDR\":\"Too long didn’t read\",\n",
    "\"TL DR\":\"Too long didn’t read\",\n",
    "\"TLC\": \"Tender Loving Care\",\n",
    "\"TMI\": \"Too Much Information\",\n",
    "\"TTYL\": \"Talk To You Later\",\n",
    "\"TTYS\": \"Talk To You Soon\",\n",
    "\"Txt\": \"Text\",\n",
    "\"TYVM\": \"Thank You Very Much\",\n",
    "\"U\": \"You\",\n",
    "\"U4F\": \"You Forever\",\n",
    "\"UR\": \"Your\",\n",
    "\"VBG\": \"Very Big Grin\",\n",
    "\"VSF\": \"Very Sad Face\",\n",
    "\"WB\": \"Welcome Back\",\n",
    "\"WBU\": \"What About You?\",\n",
    "\"WEG\": \"Wicked Evil Grin\",\n",
    "\"WKND\": \"Weekend\",\n",
    "\"WOM\": \"Word of Mouth\",\n",
    "\"WOTD\": \"Word Of The Day\",\n",
    "\"Wru\": \"Who Are You\",\n",
    "\"WTH\": \"What The Heck?\",\n",
    "\"WTPA\": \"Where The Party At?\",\n",
    "\"WU?\": \"What's Up\",\n",
    "\"WU\":\"What's Up\",\n",
    "\"WYCM\": \"Will You Call Me?\",\n",
    "\"WYWH\": \"Wish You Were Here\",\n",
    "\"XOXO\": \"Hugs and Kisses\",\n",
    "\"YGM\": \"You’ve Got Mail\",\n",
    "\"YNK\": \"You Never Know\",\n",
    "\"YOLO\": \"You Only Live Once\",\n",
    "\"YT\": \"YouTube\",\n",
    "\"YW\": \"You’re Welcome\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reemplazar_lenguaje_internet(texto):\n",
    "    texto=texto.upper()\n",
    "    palabras=texto.split()\n",
    "    palabras_procesadas=[]\n",
    "    for palabra in palabras:\n",
    "        traduccion=Diccionario_de_lenguaje_de_internet.get(palabra,'not internet slang')\n",
    "        if(traduccion!='not internet slang'):\n",
    "            lista_aux=traduccion.split()\n",
    "            for x in lista_aux:\n",
    "                palabras_procesadas.append(x.lower())\n",
    "        else:\n",
    "            palabras_procesadas.append(palabra.lower())\n",
    "    texto=' '.join([word for word in palabras_procesadas])\n",
    "    return texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "diccionario_contracciones = {  \"ain't\": \"is not\",\n",
    "                               \"aint\":\"is not\",\n",
    "                               \"aren't\": \"are not\",\n",
    "                               \"arent\":\"are not\",\n",
    "                               \"can't\": \"can not\",\n",
    "                               \"cant\":\"can not\",\n",
    "                               \"cause\": \"because\",\n",
    "                               \"could've\": \"could have\",\n",
    "                               \"couldve\":\"could have\",\n",
    "                               \"couldn't\": \"could not\",\n",
    "                               \"couldnt\":\"could not\",\n",
    "                               \"didn't\": \"did not\", \n",
    "                               \"didnt\":\"did not\",\n",
    "                               \"doesn't\": \"does not\",\n",
    "                               \"doesnt\":\"does not\",\n",
    "                               \"don't\": \"do not\",\n",
    "                               \"dont\":\"do not\",\n",
    "                               \"hadn't\": \"had not\",\n",
    "                               \"hadnt\":\"had not\",\n",
    "                               \"hasn't\": \"has not\",\n",
    "                               \"hasnt\":\"has not\",\n",
    "                               \"haven't\": \"have not\",\n",
    "                               \"havent\":\"have not\",\n",
    "                               \"he'd\": \"he would\",\n",
    "                               \"hed\":\"he would\",\n",
    "                               \"he'll\": \"he will\",\n",
    "                               \"he ll\":\"he will\",\n",
    "                               \"he's\": \"he is\",\n",
    "                               \"hes\":\"he is\",\n",
    "                               \"how'd\": \"how did\",\n",
    "                               \"howd\":\"how did\",\n",
    "                               \"how'd'y\": \"how do you\",\n",
    "                               \"howdy\":\"how do you\",\n",
    "                               \"how'll\": \"how will\",\n",
    "                               \"howll\":\"how will\",\n",
    "                               \"how's\": \"how is\",\n",
    "                               \"hows\":\"how is\",\n",
    "                               \"I'd\": \"I would\",\n",
    "                               \"id\":\"i would\",\n",
    "                               \"I'd've\": \"I would have\",\n",
    "                               \"idve\":\"i would have\",\n",
    "                               \"I'll\": \"I will\", \n",
    "                               \"Ill\":\"i will\", #duda: si algun tweet habla de un enfermo puede traer ruido\n",
    "                               \"I'll've\": \"I will have\",\n",
    "                               \"I'm\": \"I am\", \n",
    "                               \"im\":\"i am\",\n",
    "                               \"I've\": \"I have\",\n",
    "                               \"ive\":\"i have\",\n",
    "                               \"i'd\": \"i would\",\n",
    "                               \"i'd've\": \"i would have\",\n",
    "                               \"i'll\": \"i will\",\n",
    "                               \"i'll've\": \"i will have\",\n",
    "                               \"i'm\": \"i am\", \n",
    "                               \"im\":\"i am\",\n",
    "                               \"i've\": \"i have\",\n",
    "                               \"isn't\": \"is not\",\n",
    "                               \"isnt\":\"is not\",\n",
    "                               \"it'd\": \"it would\",\n",
    "                               \"itd\":\"it would\",\n",
    "                               \"it'd've\": \"it would have\",\n",
    "                               \"it'll\": \"it will\",\n",
    "                               \"itll\":\"it will\",\n",
    "                               \"it'll've\": \"it will have\",\n",
    "                               \"it's\": \"it is\",\n",
    "                               \"its\":\"it is\",\n",
    "                               \"let's\": \"let us\",\n",
    "                               \"lets\":\"let us\",\n",
    "                               \"ma'am\": \"madam\",\n",
    "                               \"maam\":\"madam\",\n",
    "                               \"mayn't\": \"may not\",\n",
    "                               \"maynt\":\"may not\",\n",
    "                               \"might've\": \"might have\",\n",
    "                               \"mightve\":\"might have\",\n",
    "                               \"mightn't\": \"might not\",\n",
    "                               \"mightnt\":\"might not\",\n",
    "                               \"mightn't've\": \"might not have\",\n",
    "                               \"must've\": \"must have\",\n",
    "                               \"mustve\":\"must have\",\n",
    "                               \"mustn't\": \"must not\",\n",
    "                               \"mustnt\":\"must not\",\n",
    "                               \"mustn't've\": \"must not have\",\n",
    "                               \"needn't\": \"need not\",\n",
    "                               \"neednt\":\"need not\",\n",
    "                               \"needn't've\": \"need not have\",\n",
    "                               \"o'clock\": \"of the clock\",\n",
    "                               \"oclock\":\"of the clock\",\n",
    "                               \"oughtn't\": \"ought not\",\n",
    "                               \"oughtnt\":\"ought not\",\n",
    "                               \"oughtn't've\": \"ought not have\",\n",
    "                               \"shan't\": \"shall not\",\n",
    "                               \"shant\":\"shall not\",\n",
    "                               \"sha'n't\": \"shall not\",\n",
    "                               \"shan't've\": \"shall not have\",\n",
    "                               \"she'd\": \"she would\",\n",
    "                               \"shed\":\"she would\",\n",
    "                               \"she'd've\": \"she would have\",\n",
    "                               \"she'll\": \"she will\",\n",
    "                               \"shell\":\"she will\",#nuevamente aca tengo ruido por dos palabras igual escritas\n",
    "                               \"she'll've\": \"she will have\", \n",
    "                               \"she's\": \"she is\",\n",
    "                               \"shes\":\"she is\",\n",
    "                               \"should've\": \"should have\",\n",
    "                               \"shouldve\":\"should have\",\n",
    "                               \"shouldn't\": \"should not\",\n",
    "                               \"shouldnt\": \"should not\",\n",
    "                               \"shouldn't've\": \"should not have\",\n",
    "                               \"so've\": \"so have\",\n",
    "                               \"so's\": \"so as\",\n",
    "                               \"this's\": \"this is\",\n",
    "                               \"that'd\": \"that would\",\n",
    "                               \"that'd've\": \"that would have\",\n",
    "                               \"that's\": \"that is\",\n",
    "                               \"thats\":\"that is\",\n",
    "                               \"there'd\": \"there would\",\n",
    "                               \"thered\":\"there would\",\n",
    "                               \"there'd've\": \"there would have\",\n",
    "                               \"there's\": \"there is\",\n",
    "                               \"theres\":\"there is\",\n",
    "                               \"here's\": \"here is\",\n",
    "                               \"heres\":\"here is\",\n",
    "                               \"they'd\": \"they would\",\n",
    "                               \"theyd\":\"they would\",\n",
    "                               \"they'd've\": \"they would have\",\n",
    "                               \"they'll\": \"they will\",\n",
    "                               \"theyll\":\"they will\",\n",
    "                               \"they'll've\": \"they will have\", \n",
    "                               \"they're\": \"they are\",\n",
    "                               \"theyre\":\"they are\",\n",
    "                               \"they've\": \"they have\", \n",
    "                               \"theyve\":\"they have\",\n",
    "                               \"to've\": \"to have\", \n",
    "                               \"tove\":\"to have\",\n",
    "                               \"wasn't\": \"was not\",\n",
    "                               \"wasnt\":\"was not\",\n",
    "                               \"we'd\": \"we would\",\n",
    "                               \"wed\":\"we would\",#aca nuevamente hay conflicto\n",
    "                               \"we'd've\": \"we would have\",\n",
    "                               \"we'll\": \"we will\",\n",
    "                               \"we'll've\": \"we will have\",\n",
    "                               \"we're\": \"we are\",\n",
    "                               \"we've\": \"we have\",\n",
    "                               \"weren't\": \"were not\",\n",
    "                               \"werent\":\"were not\",\n",
    "                               \"what'll\": \"what will\",\n",
    "                               \"whatll\":\"what will\",\n",
    "                               \"what'll've\": \"what will have\",\n",
    "                               \"what're\": \"what are\", \n",
    "                               \"whatre\":\"what are\",\n",
    "                               \"what's\": \"what is\",\n",
    "                               \"whats\":\"what is\",\n",
    "                               \"what've\": \"what have\",\n",
    "                               \"whatve\":\"what have\",\n",
    "                               \"when's\": \"when is\",\n",
    "                               \"whens\":\"when is\",\n",
    "                               \"when've\": \"when have\",\n",
    "                               \"whenve\":\"when have\",\n",
    "                               \"where'd\": \"where did\",\n",
    "                               \"whered\":\"where did\",\n",
    "                               \"where's\": \"where is\",\n",
    "                               \"wheres\":\"where is\",\n",
    "                               \"where've\": \"where have\",\n",
    "                               \"whereve\":\"where have\",\n",
    "                               \"who'll\": \"who will\", \n",
    "                               \"wholl\":\"who will\",\n",
    "                               \"who'll've\": \"who will have\",\n",
    "                               \"who's\": \"who is\",\n",
    "                               \"whos\":\"who is\",\n",
    "                               \"who've\": \"who have\",\n",
    "                               \"whove\":\"who have\",\n",
    "                               \"why's\": \"why is\",\n",
    "                               \"whys\":\"why is\",\n",
    "                               \"why've\": \"why have\",\n",
    "                               \"whyve\":\"why have\",\n",
    "                               \"will've\": \"will have\",\n",
    "                               \"willve\":\"will have\",\n",
    "                               \"won't\": \"will not\",\n",
    "                               \"wont\":\"will not\",\n",
    "                               \"won't've\": \"will not have\",\n",
    "                               \"would've\": \"would have\",\n",
    "                               \"wouldve\":\"would have\",\n",
    "                               \"wouldn't\": \"would not\",\n",
    "                               \"wouldnt\":\"would not\",\n",
    "                               \"wouldn't've\": \"would not have\",\n",
    "                               \"y'all\": \"you all\",\n",
    "                               \"yall\":\"you all\",\n",
    "                               \"y'all'd\": \"you all would\",\n",
    "                               \"yalld\":\"you all would\",\n",
    "                               \"y'all'd've\": \"you all would have\",\n",
    "                               \"y'all're\": \"you all are\",\n",
    "                               \"yallre\":\"you all are\",\n",
    "                               \"y'all've\": \"you all have\",\n",
    "                               \"you'd\": \"you would\",\n",
    "                               \"youd\":\"you would\",\n",
    "                               \"you'd've\": \"you would have\",\n",
    "                               \"you'll\": \"you will\",\n",
    "                               \"youll\":\"you will\",\n",
    "                               \"you'll've\": \"you will have\",\n",
    "                               \"you're\": \"you are\",\n",
    "                               \"youre\":\"you are\",\n",
    "                               \"you've\": \"you have\",\n",
    "                               \"youve\":\"you have\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reemplazar_contracciones(texto):\n",
    "    texto=texto.lower()\n",
    "    palabras=texto.split()\n",
    "    palabras_procesadas=[]\n",
    "    for palabra in palabras:\n",
    "        traduccion=diccionario_contracciones.get(palabra,'no es contraccion')\n",
    "        if(traduccion!='no es contraccion'):\n",
    "            lista_aux=traduccion.split()\n",
    "            for x in lista_aux:\n",
    "                palabras_procesadas.append(x.lower())\n",
    "        else:\n",
    "            palabras_procesadas.append(palabra.lower())\n",
    "    texto=' '.join([word for word in palabras_procesadas])\n",
    "    return texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#es necesario importar la libreria re (regular expression) para poder usar esta funcion\n",
    "#ya esta bajada en el comienzo del notebook pero si lo que quieren es analizar la documentación\n",
    "#lo pueden revisar en google\n",
    "#falta borrar los arroba\n",
    "def limpiar_texto(texto):\n",
    "    texto = reemplazar_lenguaje_internet(texto)\n",
    "    texto=reemplazar_contracciones(texto)\n",
    "    texto=texto.lower()\n",
    "    texto = re.sub('\\[.*?\\]', '', texto) # remove text in square brackets\n",
    "    texto = re.sub('https?://\\S+|www\\.\\S+', '', texto) # remove URLs\n",
    "    texto = re.sub('<.*?>+', '', texto) # remove html tags\n",
    "    texto = re.sub('[%s]' % re.escape(string.punctuation), '', texto) # remove punctuation\n",
    "    texto = re.sub('\\n', '', texto) # remove words conatinaing numbers\n",
    "    texto = re.sub('\\w*\\d\\w*', '', texto)\n",
    "    texto = re.sub('[‘’“”…]', '', texto)\n",
    "    texto = quitar_stopwords(texto)\n",
    "    texto = quitar_menciones(texto)\n",
    "\n",
    "    return texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set['text']=train_set['text'].apply(limpiar_texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizar_texto(texto):\n",
    "    lista_palabras=nltk.word_tokenize(texto)\n",
    "    texto=' '.join([lemmatizer.lemmatize(word) for word in lista_palabras])\n",
    "    return texto\n",
    "#nota: la función esta definida aca, pero para ver los resultados que produce hay\n",
    "#que ir al comienzo del notebook y ejecutar la celda de lemmatizar texto luego\n",
    "#de haber limpiado el texto original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(palabra):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([palabra])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizar_con_pos(texto):\n",
    "    lista_palabras=texto.split()\n",
    "    texto=' '.join(lemmatizer.lemmatize(palabra,get_wordnet_pos(palabra)) for palabra in lista_palabras)\n",
    "    return texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set['text']=train_set['text'].apply(lemmatizar_con_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases, Phraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent=[row.split() for row in train_set['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases=Phrases(sent,min_count=30,progress_per=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram=Phraser(phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences=bigram[sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14305"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freq = defaultdict(int)\n",
    "for sent in sentences:\n",
    "    for i in sent:\n",
    "        word_freq[i] += 1\n",
    "len(word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['im_sorry', 'get', 'fire', 'amp', 'like', 'go', 'new', 'via', 'people', 'one']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(word_freq, key=word_freq.get, reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "cores = multiprocessing.cpu_count()\n",
    "w2v_model = Word2Vec(min_count=20,\n",
    "                     window=2,\n",
    "                     size=300,\n",
    "                     sample=6e-5, \n",
    "                     alpha=0.03, \n",
    "                     min_alpha=0.0007, \n",
    "                     negative=20,\n",
    "                     workers=cores-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to build vocab: 0.01 mins\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "\n",
    "w2v_model.build_vocab(sentences, progress_per=10000)\n",
    "\n",
    "print('Time to build vocab: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hail', 0.17814017832279205),\n",
       " ('class', 0.16760270297527313),\n",
       " ('mean', 0.1640603244304657),\n",
       " ('learn', 0.1531810164451599),\n",
       " ('appear', 0.14799894392490387),\n",
       " ('thousand', 0.14281725883483887),\n",
       " ('movie', 0.14201197028160095),\n",
       " ('death', 0.13591092824935913),\n",
       " ('trouble', 0.13466258347034454),\n",
       " ('day', 0.1295892894268036)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"california\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.word2vec.Word2Vec at 0x7f4acd4d4e10>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Please install Keras to use this function",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomRotation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.keras.layers.experimental.preprocessing'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_keras_embedding\u001b[0;34m(self, train_embeddings, word_index)\u001b[0m\n\u001b[1;32m   1418\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1419\u001b[0;31m             \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1420\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     raise ImportError(\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0;34m'Keras requires TensorFlow 2.2 or higher. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         'Install TensorFlow via `pip install tensorflow`')\n",
      "\u001b[0;31mImportError\u001b[0m: Keras requires TensorFlow 2.2 or higher. Install TensorFlow via `pip install tensorflow`",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-2f5fdc893f2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mw2v_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_keras_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_keras_embedding\u001b[0;34m(self, train_embeddings, word_index)\u001b[0m\n\u001b[1;32m   1419\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1421\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Please install Keras to use this function\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1422\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mword_index\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1423\u001b[0m             \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: Please install Keras to use this function"
     ]
    }
   ],
   "source": [
    "w2v_model.wv.get_keras_embedding(train_embeddings=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
